{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dysl-ai/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-29 11:19:54.203832: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-29 11:19:54.204913: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-29 11:19:54.225065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-29 11:19:54.579861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read JSONL files\n",
    "def read_jsonl(file_path, nrows=None):\n",
    "    return pd.read_json(file_path, lines=True, nrows=nrows)\n",
    "\n",
    "# Read data\n",
    "train_data = read_jsonl('/home/dysl-ai/Desktop/indoml_datathon/datathon_phase_2_data/training_data/train.features')\n",
    "train_solution = read_jsonl('/home/dysl-ai/Desktop/indoml_datathon/datathon_phase_2_data/training_data/train.labels')\n",
    "test_data = read_jsonl('/home/dysl-ai/Desktop/indoml_datathon/final_test_data/final_test_data.features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, solution):\n",
    "    merged = pd.merge(data, solution, on='indoml_id')\n",
    "\n",
    "    merged['input_text'] = merged.apply(lambda row: f\"description: {row['description']} retailer: {row['retailer']} price: {row['price']}\", axis=1)\n",
    "    merged['target_text'] = merged.apply(lambda row: f\"supergroup: {row['supergroup']} group: {row['group']} module: {row['module']} brand: {row['brand']}\", axis=1)\n",
    "    \n",
    "    return merged[['input_text', 'target_text']]\n",
    "\n",
    "\n",
    "train_processed = preprocess_data(train_data, train_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset\n",
    "dataset = train\n",
    "\n",
    "# If your dataset is already split into train and test, you might need to select one\n",
    "# For example, if you want to split the training set:\n",
    "# dataset = dataset[\"train\"]\n",
    "\n",
    "# Split the dataset\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Now you have your splits\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']  # Note: This is actually our validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "model = T5ForConditionalGeneration.from_pretrained('/home/dysl-ai/Desktop/indoml_datathon/final_final_results/checkpoint-252828')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 449470/449470 [00:38<00:00, 11776.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112368/112368 [00:09<00:00, 11859.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding='max_length', truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dysl-ai/anaconda3/envs/summarize/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./final_final_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.0001,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    report_to='none'\n",
    ")\n",
    "# Custom optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=1000)\n",
    "\n",
    "# Custom callback\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step: {state.global_step}\")\n",
    "            for key, value in logs.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[CustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dysl-ai/anaconda3/envs/summarize/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "STAGE:2024-10-29 11:20:50 350654:350654 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-29 11:20:51 350654:350654 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-29 11:20:51 350654:350654 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./final_final_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.0001,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Custom optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=1000)\n",
    "\n",
    "# Custom callback\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step: {state.global_step}\")\n",
    "            for key, value in logs.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[CustomCallback()],\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "# Training with profiling for one sample and one epoch\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "    # Take the first batch (one sample)\n",
    "    for batch in trainer.get_train_dataloader():\n",
    "        inputs = batch\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        break  # Only one batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters that are used for training\n",
      "737668096\n"
     ]
    }
   ],
   "source": [
    "# Print number of model parameters\n",
    "print(\"Number of model parameters that are used for training\")\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         2.15%      11.548ms         7.13%      38.231ms      33.100us     198.654ms        40.05%     234.801ms     203.291us          1155   9062917.865  \n",
      "                                              aten::bmm         0.60%       3.243ms         1.74%       9.327ms      21.590us      11.034ms         2.22%      14.324ms      33.157us           432    231928.234  \n",
      "                                              aten::mul         1.03%       5.533ms         2.46%      13.205ms      10.127us       9.068ms         1.83%      10.784ms       8.270us          1304      2100.455  \n",
      "                                              aten::add         0.29%       1.579ms         0.43%       2.320ms       7.759us       3.820ms         0.77%       6.507ms      21.763us           299       388.420  \n",
      "                                            aten::empty         0.30%       1.583ms         1.33%       7.107ms       6.256us       0.000us         0.00%       1.614ms       1.421us          1136            --  \n",
      "                                          aten::random_         0.00%      19.000us         0.00%      19.000us      19.000us       0.000us         0.00%       0.000us       0.000us             1            --  \n",
      "                                             aten::item         0.00%      21.000us         0.00%      25.000us       0.025us       0.000us         0.00%       0.000us       0.000us          1020            --  \n",
      "                              aten::_local_scalar_dense         0.00%       4.000us         0.00%       4.000us       0.004us       0.000us         0.00%       0.000us       0.000us          1020            --  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         1.85%       9.937ms         2.77%      14.831ms       7.415ms       0.000us         0.00%      11.000us       5.500us             2            --  \n",
      "                                         aten::randperm         0.59%       3.164ms         1.18%       6.322ms       3.161ms       0.000us         0.00%       0.000us       0.000us             2            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 536.230ms\n",
      "Self CUDA time total: 496.007ms\n",
      "\n",
      "GFLOPs during training\n",
      "9297.334974464\n"
     ]
    }
   ],
   "source": [
    "# Print profile log for training\n",
    "print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "print(\"GFLOPs during training\")  # GigaFLOPs\n",
    "print(sum(k.flops for k in prof.key_averages()) / 1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data1(data):\n",
    "    # Create the input_text column\n",
    "    data['input_text'] = data.apply(lambda row: f\"description: {row['description']} retailer: {row['retailer']} price: {row['price']}\", axis=1)\n",
    "\n",
    "    # Return the dictionary format with only input_text\n",
    "    return {\n",
    "        'input_text': data['input_text'].tolist()\n",
    "    }\n",
    "\n",
    "# Process the test data\n",
    "test_processed = preprocess_data1(test_data)\n",
    "\n",
    "# Convert the processed dictionary to a Hugging Face Dataset\n",
    "test_dataset = Dataset.from_dict(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-29 11:20:55 350654:350654 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-29 11:20:56 350654:350654 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-29 11:20:56 350654:350654 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 5.8283538818359375 seconds\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm        11.01%      29.830ms        14.32%      38.813ms       7.184us      53.949ms        79.26%      54.490ms      10.085us          5403     33485.488  \n",
      "                                              aten::bmm         5.45%      14.773ms         7.30%      19.773ms       7.490us       2.904ms         4.27%       3.123ms       1.183us          2640       116.785  \n",
      "                                              aten::mul         5.84%      15.827ms         7.62%      20.650ms       4.878us       2.695ms         3.96%       3.164ms       0.747us          4233         5.026  \n",
      "                                              aten::add         6.62%      17.929ms         8.40%      22.762ms       5.449us     868.000us         1.28%       1.236ms       0.296us          4177         2.912  \n",
      "                                            aten::empty         0.22%     596.000us         0.22%     596.000us       1.540us       0.000us         0.00%       0.000us       0.000us           387            --  \n",
      "                                               aten::to         0.36%     976.000us         0.74%       2.002ms       0.290us       0.000us         0.00%     110.000us       0.016us          6912            --  \n",
      "                                       aten::lift_fresh         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             5            --  \n",
      "                                          aten::detach_         0.00%       4.000us         0.00%       6.000us       1.200us       0.000us         0.00%       0.000us       0.000us             5            --  \n",
      "                                                detach_         0.00%       2.000us         0.00%       2.000us       0.400us       0.000us         0.00%       0.000us       0.000us             5            --  \n",
      "                                         aten::_to_copy         0.13%     353.000us         0.43%       1.170ms       9.915us       0.000us         0.00%     123.000us       1.042us           118            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 270.990ms\n",
      "Self CUDA time total: 68.067ms\n",
      "\n",
      "GFLOPs during inference\n",
      "33.610211588\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('/home/dysl-ai/Desktop/indoml_datathon/fine_tuned_t5_large_4').to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained('/home/dysl-ai/Desktop/indoml_datathon/fine_tuned_t5_large_4')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Helper function to generate text\n",
    "def generate_text(inputs):\n",
    "    inputs = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=352)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return generated_texts\n",
    "\n",
    "# Inference with profiling for one sample\n",
    "start = time.time()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "    test_data = test_dataset['input_text']\n",
    "    # Take the first sample\n",
    "    sample_input = [test_data[0]]\n",
    "    generated_text = generate_text(sample_input)\n",
    "\n",
    "# Inference time\n",
    "inference_time = time.time() - start\n",
    "print(f\"Inference time: {inference_time} seconds\")\n",
    "\n",
    "# Print profile log for inference\n",
    "print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "print(\"GFLOPs during inference\")  # GigaFLOPs\n",
    "print(sum(k.flops for k in prof.key_averages()) / 1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
