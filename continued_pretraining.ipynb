{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dysl-ai/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-28 14:32:09.841714: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-28 14:32:09.842855: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-28 14:32:09.863081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-28 14:32:10.276665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path, nrows=None):\n",
    "    return pd.read_json(file_path, lines=True, nrows=nrows)\n",
    "\n",
    "\n",
    "train_data = read_jsonl('/home/dysl-ai/Desktop/indoml_datathon/datathon_phase_2_data/training_data/train.features')\n",
    "train_solution = read_jsonl('/home/dysl-ai/Desktop/indoml_datathon/datathon_phase_2_data/training_data/train.labels')\n",
    "test_data=read_jsonl('/home/dysl-ai/Desktop/indoml_datathon/final_test_data/final_test_data.features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, solution):\n",
    "    merged = pd.merge(data, solution, on='indoml_id')\n",
    "\n",
    "    merged['input_text'] = merged.apply(lambda row: f\"description: {row['description']} retailer: {row['retailer']} price: {row['price']}\", axis=1)\n",
    "    merged['target_text'] = merged.apply(lambda row: f\"supergroup: {row['supergroup']} group: {row['group']} module: {row['module']} brand: {row['brand']}\", axis=1)\n",
    "    \n",
    "    return merged[['input_text', 'target_text']]\n",
    "\n",
    "\n",
    "train_processed = preprocess_data(train_data, train_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset\n",
    "dataset = train\n",
    "\n",
    "# If your dataset is already split into train and test, you might need to select one\n",
    "# For example, if you want to split the training set:\n",
    "# dataset = dataset[\"train\"]\n",
    "\n",
    "# Split the dataset\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Now you have your splits\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']  # Note: This is actually our validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "model = T5ForConditionalGeneration.from_pretrained('/home/dysl-ai/Desktop/indoml_datathon/final_final_results/checkpoint-224736')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 449470/449470 [00:38<00:00, 11621.70 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112368/112368 [00:09<00:00, 11821.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding='max_length', truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 449470\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 112368\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dysl-ai/anaconda3/envs/summarize/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./final_final_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=9,\n",
    "    weight_decay=0.0001,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step: {state.global_step}\")\n",
    "            for key, value in logs.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252828' max='252828' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252828/252828 3:04:35, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 224740\n",
      "loss: 0.0009\n",
      "grad_norm: 0.012383080087602139\n",
      "learning_rate: 3.332858702358916e-05\n",
      "epoch: 8.000142389292325\n",
      "\n",
      "\n",
      "Step: 224760\n",
      "loss: 0.0008\n",
      "grad_norm: 0.02467884123325348\n",
      "learning_rate: 3.330485547486829e-05\n",
      "epoch: 8.000854335753951\n",
      "\n",
      "\n",
      "Step: 224780\n",
      "loss: 0.001\n",
      "grad_norm: 0.016249921172857285\n",
      "learning_rate: 3.3281123926147415e-05\n",
      "epoch: 8.001566282215578\n",
      "\n",
      "\n",
      "Step: 224800\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01644115336239338\n",
      "learning_rate: 3.325739237742655e-05\n",
      "epoch: 8.002278228677204\n",
      "\n",
      "\n",
      "Step: 224820\n",
      "loss: 0.0007\n",
      "grad_norm: 0.00980929285287857\n",
      "learning_rate: 3.323366082870568e-05\n",
      "epoch: 8.00299017513883\n",
      "\n",
      "\n",
      "Step: 224840\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01944376528263092\n",
      "learning_rate: 3.320992927998481e-05\n",
      "epoch: 8.003702121600456\n",
      "\n",
      "\n",
      "Step: 224860\n",
      "loss: 0.0009\n",
      "grad_norm: 0.016858242452144623\n",
      "learning_rate: 3.3186197731263936e-05\n",
      "epoch: 8.004414068062081\n",
      "\n",
      "\n",
      "Step: 224880\n",
      "loss: 0.0007\n",
      "grad_norm: 0.010275577194988728\n",
      "learning_rate: 3.316246618254307e-05\n",
      "epoch: 8.005126014523707\n",
      "\n",
      "\n",
      "Step: 224900\n",
      "loss: 0.0007\n",
      "grad_norm: 0.01272615883499384\n",
      "learning_rate: 3.31387346338222e-05\n",
      "epoch: 8.005837960985334\n",
      "\n",
      "\n",
      "Step: 224920\n",
      "loss: 0.0006\n",
      "grad_norm: 0.0016177454963326454\n",
      "learning_rate: 3.311500308510133e-05\n",
      "epoch: 8.00654990744696\n",
      "\n",
      "\n",
      "Step: 224940\n",
      "loss: 0.001\n",
      "grad_norm: 0.028149697929620743\n",
      "learning_rate: 3.309127153638046e-05\n",
      "epoch: 8.007261853908586\n",
      "\n",
      "\n",
      "Step: 224960\n",
      "loss: 0.001\n",
      "grad_norm: 0.010557659901678562\n",
      "learning_rate: 3.306753998765959e-05\n",
      "epoch: 8.007973800370213\n",
      "\n",
      "\n",
      "Step: 224980\n",
      "loss: 0.0013\n",
      "grad_norm: 0.017350373789668083\n",
      "learning_rate: 3.304380843893872e-05\n",
      "epoch: 8.008685746831839\n",
      "\n",
      "\n",
      "Step: 225000\n",
      "loss: 0.001\n",
      "grad_norm: 0.027149448171257973\n",
      "learning_rate: 3.302007689021785e-05\n",
      "epoch: 8.009397693293465\n",
      "\n",
      "\n",
      "Step: 225020\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012880373746156693\n",
      "learning_rate: 3.2996345341496983e-05\n",
      "epoch: 8.01010963975509\n",
      "\n",
      "\n",
      "Step: 225040\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02852795645594597\n",
      "learning_rate: 3.2972613792776115e-05\n",
      "epoch: 8.010821586216716\n",
      "\n",
      "\n",
      "Step: 225060\n",
      "loss: 0.0006\n",
      "grad_norm: 0.011615438386797905\n",
      "learning_rate: 3.294888224405524e-05\n",
      "epoch: 8.011533532678342\n",
      "\n",
      "\n",
      "Step: 225080\n",
      "loss: 0.0011\n",
      "grad_norm: 0.024185910820961\n",
      "learning_rate: 3.292515069533437e-05\n",
      "epoch: 8.012245479139969\n",
      "\n",
      "\n",
      "Step: 225100\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01586448960006237\n",
      "learning_rate: 3.2901419146613504e-05\n",
      "epoch: 8.012957425601595\n",
      "\n",
      "\n",
      "Step: 225120\n",
      "loss: 0.0008\n",
      "grad_norm: 0.014063713140785694\n",
      "learning_rate: 3.2877687597892636e-05\n",
      "epoch: 8.013669372063221\n",
      "\n",
      "\n",
      "Step: 225140\n",
      "loss: 0.0009\n",
      "grad_norm: 0.013797110877931118\n",
      "learning_rate: 3.285395604917177e-05\n",
      "epoch: 8.014381318524848\n",
      "\n",
      "\n",
      "Step: 225160\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004374044947326183\n",
      "learning_rate: 3.283022450045089e-05\n",
      "epoch: 8.015093264986474\n",
      "\n",
      "\n",
      "Step: 225180\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03499912470579147\n",
      "learning_rate: 3.2806492951730024e-05\n",
      "epoch: 8.015805211448098\n",
      "\n",
      "\n",
      "Step: 225200\n",
      "loss: 0.0007\n",
      "grad_norm: 0.019482970237731934\n",
      "learning_rate: 3.2782761403009156e-05\n",
      "epoch: 8.016517157909725\n",
      "\n",
      "\n",
      "Step: 225220\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011071554385125637\n",
      "learning_rate: 3.275902985428829e-05\n",
      "epoch: 8.017229104371351\n",
      "\n",
      "\n",
      "Step: 225240\n",
      "loss: 0.001\n",
      "grad_norm: 0.0030915287788957357\n",
      "learning_rate: 3.273529830556742e-05\n",
      "epoch: 8.017941050832977\n",
      "\n",
      "\n",
      "Step: 225260\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02826310135424137\n",
      "learning_rate: 3.271156675684655e-05\n",
      "epoch: 8.018652997294604\n",
      "\n",
      "\n",
      "Step: 225280\n",
      "loss: 0.001\n",
      "grad_norm: 0.01983390562236309\n",
      "learning_rate: 3.268783520812568e-05\n",
      "epoch: 8.01936494375623\n",
      "\n",
      "\n",
      "Step: 225300\n",
      "loss: 0.0012\n",
      "grad_norm: 0.003389572026208043\n",
      "learning_rate: 3.266410365940481e-05\n",
      "epoch: 8.020076890217856\n",
      "\n",
      "\n",
      "Step: 225320\n",
      "loss: 0.0014\n",
      "grad_norm: 0.01835276186466217\n",
      "learning_rate: 3.264037211068394e-05\n",
      "epoch: 8.02078883667948\n",
      "\n",
      "\n",
      "Step: 225340\n",
      "loss: 0.001\n",
      "grad_norm: 0.025859372690320015\n",
      "learning_rate: 3.261664056196307e-05\n",
      "epoch: 8.021500783141107\n",
      "\n",
      "\n",
      "Step: 225360\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0003432601224631071\n",
      "learning_rate: 3.2592909013242204e-05\n",
      "epoch: 8.022212729602733\n",
      "\n",
      "\n",
      "Step: 225380\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01358119212090969\n",
      "learning_rate: 3.2569177464521335e-05\n",
      "epoch: 8.02292467606436\n",
      "\n",
      "\n",
      "Step: 225400\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02058803290128708\n",
      "learning_rate: 3.254544591580046e-05\n",
      "epoch: 8.023636622525986\n",
      "\n",
      "\n",
      "Step: 225420\n",
      "loss: 0.0011\n",
      "grad_norm: 0.038293953984975815\n",
      "learning_rate: 3.252171436707959e-05\n",
      "epoch: 8.024348568987612\n",
      "\n",
      "\n",
      "Step: 225440\n",
      "loss: 0.0006\n",
      "grad_norm: 0.017266791313886642\n",
      "learning_rate: 3.2497982818358724e-05\n",
      "epoch: 8.025060515449239\n",
      "\n",
      "\n",
      "Step: 225460\n",
      "loss: 0.0013\n",
      "grad_norm: 0.03049571067094803\n",
      "learning_rate: 3.2474251269637856e-05\n",
      "epoch: 8.025772461910865\n",
      "\n",
      "\n",
      "Step: 225480\n",
      "loss: 0.0008\n",
      "grad_norm: 0.004574920050799847\n",
      "learning_rate: 3.245051972091699e-05\n",
      "epoch: 8.02648440837249\n",
      "\n",
      "\n",
      "Step: 225500\n",
      "loss: 0.0014\n",
      "grad_norm: 0.003210506634786725\n",
      "learning_rate: 3.242678817219611e-05\n",
      "epoch: 8.027196354834116\n",
      "\n",
      "\n",
      "Step: 225520\n",
      "loss: 0.0007\n",
      "grad_norm: 0.00040091582923196256\n",
      "learning_rate: 3.2403056623475244e-05\n",
      "epoch: 8.027908301295742\n",
      "\n",
      "\n",
      "Step: 225540\n",
      "loss: 0.0012\n",
      "grad_norm: 0.018329812213778496\n",
      "learning_rate: 3.2379325074754376e-05\n",
      "epoch: 8.028620247757368\n",
      "\n",
      "\n",
      "Step: 225560\n",
      "loss: 0.0008\n",
      "grad_norm: 0.015570334158837795\n",
      "learning_rate: 3.235559352603351e-05\n",
      "epoch: 8.029332194218995\n",
      "\n",
      "\n",
      "Step: 225580\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02385842055082321\n",
      "learning_rate: 3.233186197731264e-05\n",
      "epoch: 8.030044140680621\n",
      "\n",
      "\n",
      "Step: 225600\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0016006954247131944\n",
      "learning_rate: 3.2308130428591765e-05\n",
      "epoch: 8.030756087142247\n",
      "\n",
      "\n",
      "Step: 225620\n",
      "loss: 0.0008\n",
      "grad_norm: 0.001951277838088572\n",
      "learning_rate: 3.22843988798709e-05\n",
      "epoch: 8.031468033603874\n",
      "\n",
      "\n",
      "Step: 225640\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0196542888879776\n",
      "learning_rate: 3.226066733115003e-05\n",
      "epoch: 8.032179980065498\n",
      "\n",
      "\n",
      "Step: 225660\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00030149269150570035\n",
      "learning_rate: 3.223693578242916e-05\n",
      "epoch: 8.032891926527125\n",
      "\n",
      "\n",
      "Step: 225680\n",
      "loss: 0.0007\n",
      "grad_norm: 0.019970707595348358\n",
      "learning_rate: 3.221320423370829e-05\n",
      "epoch: 8.033603872988751\n",
      "\n",
      "\n",
      "Step: 225700\n",
      "loss: 0.0008\n",
      "grad_norm: 0.010476168245077133\n",
      "learning_rate: 3.218947268498742e-05\n",
      "epoch: 8.034315819450377\n",
      "\n",
      "\n",
      "Step: 225720\n",
      "loss: 0.0011\n",
      "grad_norm: 0.022997768595814705\n",
      "learning_rate: 3.216574113626655e-05\n",
      "epoch: 8.035027765912004\n",
      "\n",
      "\n",
      "Step: 225740\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010486061684787273\n",
      "learning_rate: 3.214200958754568e-05\n",
      "epoch: 8.03573971237363\n",
      "\n",
      "\n",
      "Step: 225760\n",
      "loss: 0.0011\n",
      "grad_norm: 0.05840502306818962\n",
      "learning_rate: 3.211827803882481e-05\n",
      "epoch: 8.036451658835256\n",
      "\n",
      "\n",
      "Step: 225780\n",
      "loss: 0.001\n",
      "grad_norm: 0.004264621064066887\n",
      "learning_rate: 3.209454649010394e-05\n",
      "epoch: 8.037163605296882\n",
      "\n",
      "\n",
      "Step: 225800\n",
      "loss: 0.0015\n",
      "grad_norm: 0.010170883499085903\n",
      "learning_rate: 3.207081494138307e-05\n",
      "epoch: 8.037875551758507\n",
      "\n",
      "\n",
      "Step: 225820\n",
      "loss: 0.0014\n",
      "grad_norm: 0.018409064039587975\n",
      "learning_rate: 3.20470833926622e-05\n",
      "epoch: 8.038587498220133\n",
      "\n",
      "\n",
      "Step: 225840\n",
      "loss: 0.0012\n",
      "grad_norm: 0.033238209784030914\n",
      "learning_rate: 3.202335184394133e-05\n",
      "epoch: 8.03929944468176\n",
      "\n",
      "\n",
      "Step: 225860\n",
      "loss: 0.0012\n",
      "grad_norm: 0.024993106722831726\n",
      "learning_rate: 3.1999620295220465e-05\n",
      "epoch: 8.040011391143386\n",
      "\n",
      "\n",
      "Step: 225880\n",
      "loss: 0.0014\n",
      "grad_norm: 0.034202225506305695\n",
      "learning_rate: 3.197588874649959e-05\n",
      "epoch: 8.040723337605012\n",
      "\n",
      "\n",
      "Step: 225900\n",
      "loss: 0.0015\n",
      "grad_norm: 0.004487285856157541\n",
      "learning_rate: 3.195215719777872e-05\n",
      "epoch: 8.041435284066639\n",
      "\n",
      "\n",
      "Step: 225920\n",
      "loss: 0.001\n",
      "grad_norm: 0.02220163121819496\n",
      "learning_rate: 3.192842564905785e-05\n",
      "epoch: 8.042147230528265\n",
      "\n",
      "\n",
      "Step: 225940\n",
      "loss: 0.001\n",
      "grad_norm: 0.021870872005820274\n",
      "learning_rate: 3.1904694100336985e-05\n",
      "epoch: 8.042859176989891\n",
      "\n",
      "\n",
      "Step: 225960\n",
      "loss: 0.001\n",
      "grad_norm: 0.0293190348893404\n",
      "learning_rate: 3.188096255161612e-05\n",
      "epoch: 8.043571123451516\n",
      "\n",
      "\n",
      "Step: 225980\n",
      "loss: 0.0007\n",
      "grad_norm: 0.012548045255243778\n",
      "learning_rate: 3.185723100289524e-05\n",
      "epoch: 8.044283069913142\n",
      "\n",
      "\n",
      "Step: 226000\n",
      "loss: 0.0013\n",
      "grad_norm: 0.019868912175297737\n",
      "learning_rate: 3.1833499454174374e-05\n",
      "epoch: 8.044995016374768\n",
      "\n",
      "\n",
      "Step: 226020\n",
      "loss: 0.0015\n",
      "grad_norm: 0.03311103209853172\n",
      "learning_rate: 3.1809767905453506e-05\n",
      "epoch: 8.045706962836395\n",
      "\n",
      "\n",
      "Step: 226040\n",
      "loss: 0.001\n",
      "grad_norm: 0.03452548012137413\n",
      "learning_rate: 3.178603635673264e-05\n",
      "epoch: 8.046418909298021\n",
      "\n",
      "\n",
      "Step: 226060\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03082161583006382\n",
      "learning_rate: 3.176230480801177e-05\n",
      "epoch: 8.047130855759647\n",
      "\n",
      "\n",
      "Step: 226080\n",
      "loss: 0.0016\n",
      "grad_norm: 0.023213203996419907\n",
      "learning_rate: 3.17385732592909e-05\n",
      "epoch: 8.047842802221274\n",
      "\n",
      "\n",
      "Step: 226100\n",
      "loss: 0.0008\n",
      "grad_norm: 0.025649335235357285\n",
      "learning_rate: 3.171484171057003e-05\n",
      "epoch: 8.0485547486829\n",
      "\n",
      "\n",
      "Step: 226120\n",
      "loss: 0.0011\n",
      "grad_norm: 0.011020771227777004\n",
      "learning_rate: 3.169111016184916e-05\n",
      "epoch: 8.049266695144524\n",
      "\n",
      "\n",
      "Step: 226140\n",
      "loss: 0.001\n",
      "grad_norm: 0.024568665772676468\n",
      "learning_rate: 3.166737861312829e-05\n",
      "epoch: 8.04997864160615\n",
      "\n",
      "\n",
      "Step: 226160\n",
      "loss: 0.0008\n",
      "grad_norm: 0.020929697901010513\n",
      "learning_rate: 3.164364706440742e-05\n",
      "epoch: 8.050690588067777\n",
      "\n",
      "\n",
      "Step: 226180\n",
      "loss: 0.0011\n",
      "grad_norm: 0.014618396759033203\n",
      "learning_rate: 3.161991551568655e-05\n",
      "epoch: 8.051402534529403\n",
      "\n",
      "\n",
      "Step: 226200\n",
      "loss: 0.0012\n",
      "grad_norm: 0.027385320514440536\n",
      "learning_rate: 3.1596183966965685e-05\n",
      "epoch: 8.05211448099103\n",
      "\n",
      "\n",
      "Step: 226220\n",
      "loss: 0.0008\n",
      "grad_norm: 0.008897705934941769\n",
      "learning_rate: 3.157245241824481e-05\n",
      "epoch: 8.052826427452656\n",
      "\n",
      "\n",
      "Step: 226240\n",
      "loss: 0.0011\n",
      "grad_norm: 0.011043106205761433\n",
      "learning_rate: 3.154872086952394e-05\n",
      "epoch: 8.053538373914282\n",
      "\n",
      "\n",
      "Step: 226260\n",
      "loss: 0.001\n",
      "grad_norm: 0.04059941694140434\n",
      "learning_rate: 3.1524989320803074e-05\n",
      "epoch: 8.054250320375909\n",
      "\n",
      "\n",
      "Step: 226280\n",
      "loss: 0.0012\n",
      "grad_norm: 0.006412955932319164\n",
      "learning_rate: 3.1501257772082205e-05\n",
      "epoch: 8.054962266837533\n",
      "\n",
      "\n",
      "Step: 226300\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03524428606033325\n",
      "learning_rate: 3.147752622336134e-05\n",
      "epoch: 8.05567421329916\n",
      "\n",
      "\n",
      "Step: 226320\n",
      "loss: 0.0017\n",
      "grad_norm: 0.008318712934851646\n",
      "learning_rate: 3.145379467464046e-05\n",
      "epoch: 8.056386159760786\n",
      "\n",
      "\n",
      "Step: 226340\n",
      "loss: 0.0011\n",
      "grad_norm: 0.035462476313114166\n",
      "learning_rate: 3.1430063125919594e-05\n",
      "epoch: 8.057098106222412\n",
      "\n",
      "\n",
      "Step: 226360\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01044449768960476\n",
      "learning_rate: 3.1406331577198726e-05\n",
      "epoch: 8.057810052684038\n",
      "\n",
      "\n",
      "Step: 226380\n",
      "loss: 0.001\n",
      "grad_norm: 0.021478042006492615\n",
      "learning_rate: 3.138260002847786e-05\n",
      "epoch: 8.058521999145665\n",
      "\n",
      "\n",
      "Step: 226400\n",
      "loss: 0.0008\n",
      "grad_norm: 0.008157256059348583\n",
      "learning_rate: 3.135886847975699e-05\n",
      "epoch: 8.059233945607291\n",
      "\n",
      "\n",
      "Step: 226420\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04219590872526169\n",
      "learning_rate: 3.1335136931036114e-05\n",
      "epoch: 8.059945892068916\n",
      "\n",
      "\n",
      "Step: 226440\n",
      "loss: 0.0007\n",
      "grad_norm: 0.007899538613855839\n",
      "learning_rate: 3.1311405382315246e-05\n",
      "epoch: 8.060657838530542\n",
      "\n",
      "\n",
      "Step: 226460\n",
      "loss: 0.0008\n",
      "grad_norm: 0.018601883202791214\n",
      "learning_rate: 3.128767383359438e-05\n",
      "epoch: 8.061369784992168\n",
      "\n",
      "\n",
      "Step: 226480\n",
      "loss: 0.0011\n",
      "grad_norm: 0.010335240513086319\n",
      "learning_rate: 3.126394228487351e-05\n",
      "epoch: 8.062081731453794\n",
      "\n",
      "\n",
      "Step: 226500\n",
      "loss: 0.0009\n",
      "grad_norm: 0.05719940364360809\n",
      "learning_rate: 3.124021073615264e-05\n",
      "epoch: 8.06279367791542\n",
      "\n",
      "\n",
      "Step: 226520\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01802004687488079\n",
      "learning_rate: 3.1216479187431767e-05\n",
      "epoch: 8.063505624377047\n",
      "\n",
      "\n",
      "Step: 226540\n",
      "loss: 0.0008\n",
      "grad_norm: 0.000685945269651711\n",
      "learning_rate: 3.11927476387109e-05\n",
      "epoch: 8.064217570838673\n",
      "\n",
      "\n",
      "Step: 226560\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01282962504774332\n",
      "learning_rate: 3.116901608999003e-05\n",
      "epoch: 8.0649295173003\n",
      "\n",
      "\n",
      "Step: 226580\n",
      "loss: 0.001\n",
      "grad_norm: 0.02854936383664608\n",
      "learning_rate: 3.114528454126916e-05\n",
      "epoch: 8.065641463761924\n",
      "\n",
      "\n",
      "Step: 226600\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011177433654665947\n",
      "learning_rate: 3.112155299254829e-05\n",
      "epoch: 8.06635341022355\n",
      "\n",
      "\n",
      "Step: 226620\n",
      "loss: 0.001\n",
      "grad_norm: 0.009874568320810795\n",
      "learning_rate: 3.109782144382742e-05\n",
      "epoch: 8.067065356685177\n",
      "\n",
      "\n",
      "Step: 226640\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04021826386451721\n",
      "learning_rate: 3.107408989510655e-05\n",
      "epoch: 8.067777303146803\n",
      "\n",
      "\n",
      "Step: 226660\n",
      "loss: 0.001\n",
      "grad_norm: 0.02175227738916874\n",
      "learning_rate: 3.105035834638568e-05\n",
      "epoch: 8.06848924960843\n",
      "\n",
      "\n",
      "Step: 226680\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02367105893790722\n",
      "learning_rate: 3.1026626797664814e-05\n",
      "epoch: 8.069201196070056\n",
      "\n",
      "\n",
      "Step: 226700\n",
      "loss: 0.0009\n",
      "grad_norm: 0.015271837823092937\n",
      "learning_rate: 3.100289524894394e-05\n",
      "epoch: 8.069913142531682\n",
      "\n",
      "\n",
      "Step: 226720\n",
      "loss: 0.0011\n",
      "grad_norm: 0.05566415190696716\n",
      "learning_rate: 3.097916370022307e-05\n",
      "epoch: 8.070625088993308\n",
      "\n",
      "\n",
      "Step: 226740\n",
      "loss: 0.001\n",
      "grad_norm: 0.0013018258614465594\n",
      "learning_rate: 3.09554321515022e-05\n",
      "epoch: 8.071337035454933\n",
      "\n",
      "\n",
      "Step: 226760\n",
      "loss: 0.0014\n",
      "grad_norm: 0.013221760280430317\n",
      "learning_rate: 3.0931700602781335e-05\n",
      "epoch: 8.07204898191656\n",
      "\n",
      "\n",
      "Step: 226780\n",
      "loss: 0.001\n",
      "grad_norm: 0.009507251903414726\n",
      "learning_rate: 3.0907969054060466e-05\n",
      "epoch: 8.072760928378186\n",
      "\n",
      "\n",
      "Step: 226800\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04308898374438286\n",
      "learning_rate: 3.088423750533959e-05\n",
      "epoch: 8.073472874839812\n",
      "\n",
      "\n",
      "Step: 226820\n",
      "loss: 0.0008\n",
      "grad_norm: 0.003300619777292013\n",
      "learning_rate: 3.086050595661872e-05\n",
      "epoch: 8.074184821301438\n",
      "\n",
      "\n",
      "Step: 226840\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004987224005162716\n",
      "learning_rate: 3.083677440789786e-05\n",
      "epoch: 8.074896767763065\n",
      "\n",
      "\n",
      "Step: 226860\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0010821085888892412\n",
      "learning_rate: 3.081304285917699e-05\n",
      "epoch: 8.07560871422469\n",
      "\n",
      "\n",
      "Step: 226880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0028085880912840366\n",
      "learning_rate: 3.078931131045612e-05\n",
      "epoch: 8.076320660686317\n",
      "\n",
      "\n",
      "Step: 226900\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03322729468345642\n",
      "learning_rate: 3.0765579761735244e-05\n",
      "epoch: 8.077032607147942\n",
      "\n",
      "\n",
      "Step: 226920\n",
      "loss: 0.0011\n",
      "grad_norm: 0.022520391270518303\n",
      "learning_rate: 3.074184821301438e-05\n",
      "epoch: 8.077744553609568\n",
      "\n",
      "\n",
      "Step: 226940\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02033989503979683\n",
      "learning_rate: 3.071811666429351e-05\n",
      "epoch: 8.078456500071194\n",
      "\n",
      "\n",
      "Step: 226960\n",
      "loss: 0.001\n",
      "grad_norm: 0.013847032561898232\n",
      "learning_rate: 3.069438511557264e-05\n",
      "epoch: 8.07916844653282\n",
      "\n",
      "\n",
      "Step: 226980\n",
      "loss: 0.0009\n",
      "grad_norm: 0.009163254871964455\n",
      "learning_rate: 3.067065356685177e-05\n",
      "epoch: 8.079880392994447\n",
      "\n",
      "\n",
      "Step: 227000\n",
      "loss: 0.001\n",
      "grad_norm: 0.04500795900821686\n",
      "learning_rate: 3.06469220181309e-05\n",
      "epoch: 8.080592339456073\n",
      "\n",
      "\n",
      "Step: 227020\n",
      "loss: 0.0011\n",
      "grad_norm: 0.027436407282948494\n",
      "learning_rate: 3.0623190469410034e-05\n",
      "epoch: 8.0813042859177\n",
      "\n",
      "\n",
      "Step: 227040\n",
      "loss: 0.0013\n",
      "grad_norm: 0.025467781350016594\n",
      "learning_rate: 3.059945892068916e-05\n",
      "epoch: 8.082016232379326\n",
      "\n",
      "\n",
      "Step: 227060\n",
      "loss: 0.0009\n",
      "grad_norm: 0.026099245995283127\n",
      "learning_rate: 3.057572737196829e-05\n",
      "epoch: 8.08272817884095\n",
      "\n",
      "\n",
      "Step: 227080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.020871112123131752\n",
      "learning_rate: 3.055199582324742e-05\n",
      "epoch: 8.083440125302577\n",
      "\n",
      "\n",
      "Step: 227100\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04171137139201164\n",
      "learning_rate: 3.0528264274526555e-05\n",
      "epoch: 8.084152071764203\n",
      "\n",
      "\n",
      "Step: 227120\n",
      "loss: 0.0008\n",
      "grad_norm: 0.014673757366836071\n",
      "learning_rate: 3.0504532725805683e-05\n",
      "epoch: 8.08486401822583\n",
      "\n",
      "\n",
      "Step: 227140\n",
      "loss: 0.0011\n",
      "grad_norm: 0.047980472445487976\n",
      "learning_rate: 3.048080117708481e-05\n",
      "epoch: 8.085575964687456\n",
      "\n",
      "\n",
      "Step: 227160\n",
      "loss: 0.0006\n",
      "grad_norm: 0.015620735473930836\n",
      "learning_rate: 3.0457069628363943e-05\n",
      "epoch: 8.086287911149082\n",
      "\n",
      "\n",
      "Step: 227180\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02849971130490303\n",
      "learning_rate: 3.0433338079643072e-05\n",
      "epoch: 8.086999857610708\n",
      "\n",
      "\n",
      "Step: 227200\n",
      "loss: 0.0016\n",
      "grad_norm: 0.020479563623666763\n",
      "learning_rate: 3.0409606530922204e-05\n",
      "epoch: 8.087711804072335\n",
      "\n",
      "\n",
      "Step: 227220\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0025009997189044952\n",
      "learning_rate: 3.038587498220134e-05\n",
      "epoch: 8.08842375053396\n",
      "\n",
      "\n",
      "Step: 227240\n",
      "loss: 0.0011\n",
      "grad_norm: 0.014038141816854477\n",
      "learning_rate: 3.0362143433480464e-05\n",
      "epoch: 8.089135696995585\n",
      "\n",
      "\n",
      "Step: 227260\n",
      "loss: 0.001\n",
      "grad_norm: 0.018168510869145393\n",
      "learning_rate: 3.03384118847596e-05\n",
      "epoch: 8.089847643457212\n",
      "\n",
      "\n",
      "Step: 227280\n",
      "loss: 0.0011\n",
      "grad_norm: 0.08155588805675507\n",
      "learning_rate: 3.0314680336038724e-05\n",
      "epoch: 8.090559589918838\n",
      "\n",
      "\n",
      "Step: 227300\n",
      "loss: 0.0015\n",
      "grad_norm: 0.030439933761954308\n",
      "learning_rate: 3.029094878731786e-05\n",
      "epoch: 8.091271536380464\n",
      "\n",
      "\n",
      "Step: 227320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.029606303200125694\n",
      "learning_rate: 3.026721723859699e-05\n",
      "epoch: 8.09198348284209\n",
      "\n",
      "\n",
      "Step: 227340\n",
      "loss: 0.0011\n",
      "grad_norm: 0.026929128915071487\n",
      "learning_rate: 3.024348568987612e-05\n",
      "epoch: 8.092695429303717\n",
      "\n",
      "\n",
      "Step: 227360\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02177850715816021\n",
      "learning_rate: 3.021975414115525e-05\n",
      "epoch: 8.093407375765343\n",
      "\n",
      "\n",
      "Step: 227380\n",
      "loss: 0.0009\n",
      "grad_norm: 0.027418112382292747\n",
      "learning_rate: 3.019602259243438e-05\n",
      "epoch: 8.094119322226968\n",
      "\n",
      "\n",
      "Step: 227400\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03526383265852928\n",
      "learning_rate: 3.017229104371351e-05\n",
      "epoch: 8.094831268688594\n",
      "\n",
      "\n",
      "Step: 227420\n",
      "loss: 0.0009\n",
      "grad_norm: 0.013309363275766373\n",
      "learning_rate: 3.0148559494992643e-05\n",
      "epoch: 8.09554321515022\n",
      "\n",
      "\n",
      "Step: 227440\n",
      "loss: 0.0009\n",
      "grad_norm: 0.034448135644197464\n",
      "learning_rate: 3.0124827946271772e-05\n",
      "epoch: 8.096255161611847\n",
      "\n",
      "\n",
      "Step: 227460\n",
      "loss: 0.0011\n",
      "grad_norm: 0.05011038854718208\n",
      "learning_rate: 3.0101096397550904e-05\n",
      "epoch: 8.096967108073473\n",
      "\n",
      "\n",
      "Step: 227480\n",
      "loss: 0.0011\n",
      "grad_norm: 0.026005461812019348\n",
      "learning_rate: 3.0077364848830032e-05\n",
      "epoch: 8.0976790545351\n",
      "\n",
      "\n",
      "Step: 227500\n",
      "loss: 0.0005\n",
      "grad_norm: 0.003836981486529112\n",
      "learning_rate: 3.0053633300109164e-05\n",
      "epoch: 8.098391000996726\n",
      "\n",
      "\n",
      "Step: 227520\n",
      "loss: 0.0008\n",
      "grad_norm: 0.005873613525182009\n",
      "learning_rate: 3.0029901751388292e-05\n",
      "epoch: 8.099102947458352\n",
      "\n",
      "\n",
      "Step: 227540\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009135079570114613\n",
      "learning_rate: 3.0006170202667424e-05\n",
      "epoch: 8.099814893919977\n",
      "\n",
      "\n",
      "Step: 227560\n",
      "loss: 0.0008\n",
      "grad_norm: 0.043393976986408234\n",
      "learning_rate: 2.9982438653946556e-05\n",
      "epoch: 8.100526840381603\n",
      "\n",
      "\n",
      "Step: 227580\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012782348319888115\n",
      "learning_rate: 2.9958707105225684e-05\n",
      "epoch: 8.10123878684323\n",
      "\n",
      "\n",
      "Step: 227600\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0018198337638750672\n",
      "learning_rate: 2.9934975556504816e-05\n",
      "epoch: 8.101950733304855\n",
      "\n",
      "\n",
      "Step: 227620\n",
      "loss: 0.0012\n",
      "grad_norm: 0.007931072264909744\n",
      "learning_rate: 2.9911244007783944e-05\n",
      "epoch: 8.102662679766482\n",
      "\n",
      "\n",
      "Step: 227640\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04133174195885658\n",
      "learning_rate: 2.9887512459063076e-05\n",
      "epoch: 8.103374626228108\n",
      "\n",
      "\n",
      "Step: 227660\n",
      "loss: 0.0013\n",
      "grad_norm: 0.010756304487586021\n",
      "learning_rate: 2.9863780910342208e-05\n",
      "epoch: 8.104086572689734\n",
      "\n",
      "\n",
      "Step: 227680\n",
      "loss: 0.0007\n",
      "grad_norm: 0.01489357091486454\n",
      "learning_rate: 2.9840049361621336e-05\n",
      "epoch: 8.104798519151359\n",
      "\n",
      "\n",
      "Step: 227700\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0784418061375618\n",
      "learning_rate: 2.9816317812900468e-05\n",
      "epoch: 8.105510465612985\n",
      "\n",
      "\n",
      "Step: 227720\n",
      "loss: 0.0009\n",
      "grad_norm: 0.052563030272722244\n",
      "learning_rate: 2.9792586264179597e-05\n",
      "epoch: 8.106222412074612\n",
      "\n",
      "\n",
      "Step: 227740\n",
      "loss: 0.0013\n",
      "grad_norm: 0.011430216953158379\n",
      "learning_rate: 2.976885471545873e-05\n",
      "epoch: 8.106934358536238\n",
      "\n",
      "\n",
      "Step: 227760\n",
      "loss: 0.0008\n",
      "grad_norm: 0.006179322488605976\n",
      "learning_rate: 2.9745123166737857e-05\n",
      "epoch: 8.107646304997864\n",
      "\n",
      "\n",
      "Step: 227780\n",
      "loss: 0.001\n",
      "grad_norm: 0.05578583478927612\n",
      "learning_rate: 2.972139161801699e-05\n",
      "epoch: 8.10835825145949\n",
      "\n",
      "\n",
      "Step: 227800\n",
      "loss: 0.0006\n",
      "grad_norm: 0.001656746375374496\n",
      "learning_rate: 2.969766006929612e-05\n",
      "epoch: 8.109070197921117\n",
      "\n",
      "\n",
      "Step: 227820\n",
      "loss: 0.0011\n",
      "grad_norm: 0.003315133508294821\n",
      "learning_rate: 2.967392852057525e-05\n",
      "epoch: 8.109782144382743\n",
      "\n",
      "\n",
      "Step: 227840\n",
      "loss: 0.0006\n",
      "grad_norm: 0.001587879378348589\n",
      "learning_rate: 2.965019697185438e-05\n",
      "epoch: 8.110494090844368\n",
      "\n",
      "\n",
      "Step: 227860\n",
      "loss: 0.0018\n",
      "grad_norm: 0.05044866353273392\n",
      "learning_rate: 2.962646542313351e-05\n",
      "epoch: 8.111206037305994\n",
      "\n",
      "\n",
      "Step: 227880\n",
      "loss: 0.001\n",
      "grad_norm: 0.0025961152277886868\n",
      "learning_rate: 2.960273387441264e-05\n",
      "epoch: 8.11191798376762\n",
      "\n",
      "\n",
      "Step: 227900\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01781597174704075\n",
      "learning_rate: 2.9579002325691773e-05\n",
      "epoch: 8.112629930229247\n",
      "\n",
      "\n",
      "Step: 227920\n",
      "loss: 0.0011\n",
      "grad_norm: 0.032291579991579056\n",
      "learning_rate: 2.95552707769709e-05\n",
      "epoch: 8.113341876690873\n",
      "\n",
      "\n",
      "Step: 227940\n",
      "loss: 0.0011\n",
      "grad_norm: 0.018673939630389214\n",
      "learning_rate: 2.9531539228250033e-05\n",
      "epoch: 8.1140538231525\n",
      "\n",
      "\n",
      "Step: 227960\n",
      "loss: 0.0018\n",
      "grad_norm: 0.0030186790972948074\n",
      "learning_rate: 2.950780767952916e-05\n",
      "epoch: 8.114765769614126\n",
      "\n",
      "\n",
      "Step: 227980\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010997865349054337\n",
      "learning_rate: 2.9484076130808293e-05\n",
      "epoch: 8.115477716075752\n",
      "\n",
      "\n",
      "Step: 228000\n",
      "loss: 0.0013\n",
      "grad_norm: 0.011935348622500896\n",
      "learning_rate: 2.9460344582087428e-05\n",
      "epoch: 8.116189662537376\n",
      "\n",
      "\n",
      "Step: 228020\n",
      "loss: 0.0017\n",
      "grad_norm: 0.0363919660449028\n",
      "learning_rate: 2.9436613033366553e-05\n",
      "epoch: 8.116901608999003\n",
      "\n",
      "\n",
      "Step: 228040\n",
      "loss: 0.001\n",
      "grad_norm: 0.015585634857416153\n",
      "learning_rate: 2.941288148464569e-05\n",
      "epoch: 8.117613555460629\n",
      "\n",
      "\n",
      "Step: 228060\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01074603945016861\n",
      "learning_rate: 2.9389149935924813e-05\n",
      "epoch: 8.118325501922255\n",
      "\n",
      "\n",
      "Step: 228080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.007248296868056059\n",
      "learning_rate: 2.936541838720395e-05\n",
      "epoch: 8.119037448383882\n",
      "\n",
      "\n",
      "Step: 228100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.016536520794034004\n",
      "learning_rate: 2.9341686838483074e-05\n",
      "epoch: 8.119749394845508\n",
      "\n",
      "\n",
      "Step: 228120\n",
      "loss: 0.0016\n",
      "grad_norm: 0.3809623420238495\n",
      "learning_rate: 2.931795528976221e-05\n",
      "epoch: 8.120461341307134\n",
      "\n",
      "\n",
      "Step: 228140\n",
      "loss: 0.0007\n",
      "grad_norm: 0.014993654564023018\n",
      "learning_rate: 2.929422374104134e-05\n",
      "epoch: 8.12117328776876\n",
      "\n",
      "\n",
      "Step: 228160\n",
      "loss: 0.0008\n",
      "grad_norm: 0.013903934508562088\n",
      "learning_rate: 2.927049219232047e-05\n",
      "epoch: 8.121885234230385\n",
      "\n",
      "\n",
      "Step: 228180\n",
      "loss: 0.0012\n",
      "grad_norm: 0.014680483378469944\n",
      "learning_rate: 2.92467606435996e-05\n",
      "epoch: 8.122597180692011\n",
      "\n",
      "\n",
      "Step: 228200\n",
      "loss: 0.001\n",
      "grad_norm: 0.011704926379024982\n",
      "learning_rate: 2.922302909487873e-05\n",
      "epoch: 8.123309127153638\n",
      "\n",
      "\n",
      "Step: 228220\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02226281724870205\n",
      "learning_rate: 2.919929754615786e-05\n",
      "epoch: 8.124021073615264\n",
      "\n",
      "\n",
      "Step: 228240\n",
      "loss: 0.0012\n",
      "grad_norm: 0.024888765066862106\n",
      "learning_rate: 2.9175565997436993e-05\n",
      "epoch: 8.12473302007689\n",
      "\n",
      "\n",
      "Step: 228260\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00540923373773694\n",
      "learning_rate: 2.915183444871612e-05\n",
      "epoch: 8.125444966538517\n",
      "\n",
      "\n",
      "Step: 228280\n",
      "loss: 0.001\n",
      "grad_norm: 0.03284050524234772\n",
      "learning_rate: 2.9128102899995253e-05\n",
      "epoch: 8.126156913000143\n",
      "\n",
      "\n",
      "Step: 228300\n",
      "loss: 0.0018\n",
      "grad_norm: 0.024445390328764915\n",
      "learning_rate: 2.910437135127438e-05\n",
      "epoch: 8.12686885946177\n",
      "\n",
      "\n",
      "Step: 228320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04425228014588356\n",
      "learning_rate: 2.9080639802553513e-05\n",
      "epoch: 8.127580805923394\n",
      "\n",
      "\n",
      "Step: 228340\n",
      "loss: 0.0008\n",
      "grad_norm: 0.007888119667768478\n",
      "learning_rate: 2.905690825383264e-05\n",
      "epoch: 8.12829275238502\n",
      "\n",
      "\n",
      "Step: 228360\n",
      "loss: 0.0008\n",
      "grad_norm: 0.020366927608847618\n",
      "learning_rate: 2.9033176705111773e-05\n",
      "epoch: 8.129004698846646\n",
      "\n",
      "\n",
      "Step: 228380\n",
      "loss: 0.001\n",
      "grad_norm: 0.01352022122591734\n",
      "learning_rate: 2.9009445156390905e-05\n",
      "epoch: 8.129716645308273\n",
      "\n",
      "\n",
      "Step: 228400\n",
      "loss: 0.0014\n",
      "grad_norm: 0.053649164736270905\n",
      "learning_rate: 2.8985713607670034e-05\n",
      "epoch: 8.130428591769899\n",
      "\n",
      "\n",
      "Step: 228420\n",
      "loss: 0.0008\n",
      "grad_norm: 0.02293800748884678\n",
      "learning_rate: 2.8961982058949165e-05\n",
      "epoch: 8.131140538231525\n",
      "\n",
      "\n",
      "Step: 228440\n",
      "loss: 0.0013\n",
      "grad_norm: 0.008532756008207798\n",
      "learning_rate: 2.8938250510228294e-05\n",
      "epoch: 8.131852484693152\n",
      "\n",
      "\n",
      "Step: 228460\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0012297830544412136\n",
      "learning_rate: 2.8914518961507426e-05\n",
      "epoch: 8.132564431154778\n",
      "\n",
      "\n",
      "Step: 228480\n",
      "loss: 0.0015\n",
      "grad_norm: 0.015134516172111034\n",
      "learning_rate: 2.8890787412786557e-05\n",
      "epoch: 8.133276377616403\n",
      "\n",
      "\n",
      "Step: 228500\n",
      "loss: 0.0011\n",
      "grad_norm: 0.007678032852709293\n",
      "learning_rate: 2.8867055864065686e-05\n",
      "epoch: 8.133988324078029\n",
      "\n",
      "\n",
      "Step: 228520\n",
      "loss: 0.0012\n",
      "grad_norm: 0.021597303450107574\n",
      "learning_rate: 2.8843324315344818e-05\n",
      "epoch: 8.134700270539655\n",
      "\n",
      "\n",
      "Step: 228540\n",
      "loss: 0.0014\n",
      "grad_norm: 0.026041243225336075\n",
      "learning_rate: 2.8819592766623946e-05\n",
      "epoch: 8.135412217001281\n",
      "\n",
      "\n",
      "Step: 228560\n",
      "loss: 0.001\n",
      "grad_norm: 0.014825715683400631\n",
      "learning_rate: 2.8795861217903078e-05\n",
      "epoch: 8.136124163462908\n",
      "\n",
      "\n",
      "Step: 228580\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03198089823126793\n",
      "learning_rate: 2.877212966918221e-05\n",
      "epoch: 8.136836109924534\n",
      "\n",
      "\n",
      "Step: 228600\n",
      "loss: 0.0012\n",
      "grad_norm: 0.059229616075754166\n",
      "learning_rate: 2.8748398120461338e-05\n",
      "epoch: 8.13754805638616\n",
      "\n",
      "\n",
      "Step: 228620\n",
      "loss: 0.0007\n",
      "grad_norm: 0.015697335824370384\n",
      "learning_rate: 2.872466657174047e-05\n",
      "epoch: 8.138260002847787\n",
      "\n",
      "\n",
      "Step: 228640\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0030516895931214094\n",
      "learning_rate: 2.8700935023019598e-05\n",
      "epoch: 8.138971949309411\n",
      "\n",
      "\n",
      "Step: 228660\n",
      "loss: 0.0015\n",
      "grad_norm: 0.041443902999162674\n",
      "learning_rate: 2.867720347429873e-05\n",
      "epoch: 8.139683895771038\n",
      "\n",
      "\n",
      "Step: 228680\n",
      "loss: 0.0014\n",
      "grad_norm: 0.000789785583037883\n",
      "learning_rate: 2.865347192557786e-05\n",
      "epoch: 8.140395842232664\n",
      "\n",
      "\n",
      "Step: 228700\n",
      "loss: 0.0013\n",
      "grad_norm: 0.06110216677188873\n",
      "learning_rate: 2.862974037685699e-05\n",
      "epoch: 8.14110778869429\n",
      "\n",
      "\n",
      "Step: 228720\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04389474168419838\n",
      "learning_rate: 2.8606008828136122e-05\n",
      "epoch: 8.141819735155917\n",
      "\n",
      "\n",
      "Step: 228740\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02692439965903759\n",
      "learning_rate: 2.858227727941525e-05\n",
      "epoch: 8.142531681617543\n",
      "\n",
      "\n",
      "Step: 228760\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03546850010752678\n",
      "learning_rate: 2.8558545730694382e-05\n",
      "epoch: 8.14324362807917\n",
      "\n",
      "\n",
      "Step: 228780\n",
      "loss: 0.0015\n",
      "grad_norm: 0.00918988510966301\n",
      "learning_rate: 2.853481418197351e-05\n",
      "epoch: 8.143955574540794\n",
      "\n",
      "\n",
      "Step: 228800\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0287188608199358\n",
      "learning_rate: 2.8511082633252642e-05\n",
      "epoch: 8.14466752100242\n",
      "\n",
      "\n",
      "Step: 228820\n",
      "loss: 0.0012\n",
      "grad_norm: 0.036257438361644745\n",
      "learning_rate: 2.8487351084531774e-05\n",
      "epoch: 8.145379467464046\n",
      "\n",
      "\n",
      "Step: 228840\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012502110563218594\n",
      "learning_rate: 2.8463619535810903e-05\n",
      "epoch: 8.146091413925673\n",
      "\n",
      "\n",
      "Step: 228860\n",
      "loss: 0.0008\n",
      "grad_norm: 0.023386340588331223\n",
      "learning_rate: 2.8439887987090038e-05\n",
      "epoch: 8.146803360387299\n",
      "\n",
      "\n",
      "Step: 228880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.015488863922655582\n",
      "learning_rate: 2.8416156438369163e-05\n",
      "epoch: 8.147515306848925\n",
      "\n",
      "\n",
      "Step: 228900\n",
      "loss: 0.0011\n",
      "grad_norm: 0.015375316143035889\n",
      "learning_rate: 2.8392424889648298e-05\n",
      "epoch: 8.148227253310552\n",
      "\n",
      "\n",
      "Step: 228920\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0253852941095829\n",
      "learning_rate: 2.8368693340927423e-05\n",
      "epoch: 8.148939199772178\n",
      "\n",
      "\n",
      "Step: 228940\n",
      "loss: 0.0011\n",
      "grad_norm: 0.024292249232530594\n",
      "learning_rate: 2.8344961792206558e-05\n",
      "epoch: 8.149651146233802\n",
      "\n",
      "\n",
      "Step: 228960\n",
      "loss: 0.0011\n",
      "grad_norm: 0.029758503660559654\n",
      "learning_rate: 2.832123024348569e-05\n",
      "epoch: 8.150363092695429\n",
      "\n",
      "\n",
      "Step: 228980\n",
      "loss: 0.0009\n",
      "grad_norm: 0.009256266057491302\n",
      "learning_rate: 2.829749869476482e-05\n",
      "epoch: 8.151075039157055\n",
      "\n",
      "\n",
      "Step: 229000\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0012054842663928866\n",
      "learning_rate: 2.827376714604395e-05\n",
      "epoch: 8.151786985618681\n",
      "\n",
      "\n",
      "Step: 229020\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0038499950896948576\n",
      "learning_rate: 2.825003559732308e-05\n",
      "epoch: 8.152498932080308\n",
      "\n",
      "\n",
      "Step: 229040\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0006356523372232914\n",
      "learning_rate: 2.822630404860221e-05\n",
      "epoch: 8.153210878541934\n",
      "\n",
      "\n",
      "Step: 229060\n",
      "loss: 0.0014\n",
      "grad_norm: 0.08171840757131577\n",
      "learning_rate: 2.8202572499881342e-05\n",
      "epoch: 8.15392282500356\n",
      "\n",
      "\n",
      "Step: 229080\n",
      "loss: 0.0017\n",
      "grad_norm: 0.019996583461761475\n",
      "learning_rate: 2.817884095116047e-05\n",
      "epoch: 8.154634771465187\n",
      "\n",
      "\n",
      "Step: 229100\n",
      "loss: 0.0011\n",
      "grad_norm: 0.039704032242298126\n",
      "learning_rate: 2.8155109402439602e-05\n",
      "epoch: 8.155346717926811\n",
      "\n",
      "\n",
      "Step: 229120\n",
      "loss: 0.0013\n",
      "grad_norm: 0.009888829663395882\n",
      "learning_rate: 2.813137785371873e-05\n",
      "epoch: 8.156058664388437\n",
      "\n",
      "\n",
      "Step: 229140\n",
      "loss: 0.0009\n",
      "grad_norm: 0.022574415430426598\n",
      "learning_rate: 2.8107646304997863e-05\n",
      "epoch: 8.156770610850064\n",
      "\n",
      "\n",
      "Step: 229160\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03564050793647766\n",
      "learning_rate: 2.8083914756276995e-05\n",
      "epoch: 8.15748255731169\n",
      "\n",
      "\n",
      "Step: 229180\n",
      "loss: 0.0009\n",
      "grad_norm: 0.020534293726086617\n",
      "learning_rate: 2.8060183207556123e-05\n",
      "epoch: 8.158194503773316\n",
      "\n",
      "\n",
      "Step: 229200\n",
      "loss: 0.0011\n",
      "grad_norm: 0.005644531920552254\n",
      "learning_rate: 2.8036451658835255e-05\n",
      "epoch: 8.158906450234943\n",
      "\n",
      "\n",
      "Step: 229220\n",
      "loss: 0.001\n",
      "grad_norm: 0.04314545542001724\n",
      "learning_rate: 2.8012720110114383e-05\n",
      "epoch: 8.159618396696569\n",
      "\n",
      "\n",
      "Step: 229240\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03083571046590805\n",
      "learning_rate: 2.7988988561393515e-05\n",
      "epoch: 8.160330343158195\n",
      "\n",
      "\n",
      "Step: 229260\n",
      "loss: 0.0011\n",
      "grad_norm: 0.018769340589642525\n",
      "learning_rate: 2.7965257012672643e-05\n",
      "epoch: 8.16104228961982\n",
      "\n",
      "\n",
      "Step: 229280\n",
      "loss: 0.0013\n",
      "grad_norm: 0.020852884277701378\n",
      "learning_rate: 2.7941525463951775e-05\n",
      "epoch: 8.161754236081446\n",
      "\n",
      "\n",
      "Step: 229300\n",
      "loss: 0.0014\n",
      "grad_norm: 0.018806589767336845\n",
      "learning_rate: 2.7917793915230907e-05\n",
      "epoch: 8.162466182543072\n",
      "\n",
      "\n",
      "Step: 229320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.05456859990954399\n",
      "learning_rate: 2.7894062366510035e-05\n",
      "epoch: 8.163178129004699\n",
      "\n",
      "\n",
      "Step: 229340\n",
      "loss: 0.0006\n",
      "grad_norm: 0.004114619921892881\n",
      "learning_rate: 2.7870330817789167e-05\n",
      "epoch: 8.163890075466325\n",
      "\n",
      "\n",
      "Step: 229360\n",
      "loss: 0.001\n",
      "grad_norm: 0.010322630405426025\n",
      "learning_rate: 2.7846599269068296e-05\n",
      "epoch: 8.164602021927951\n",
      "\n",
      "\n",
      "Step: 229380\n",
      "loss: 0.0008\n",
      "grad_norm: 0.03983987867832184\n",
      "learning_rate: 2.7822867720347427e-05\n",
      "epoch: 8.165313968389578\n",
      "\n",
      "\n",
      "Step: 229400\n",
      "loss: 0.0011\n",
      "grad_norm: 0.013641825877130032\n",
      "learning_rate: 2.779913617162656e-05\n",
      "epoch: 8.166025914851204\n",
      "\n",
      "\n",
      "Step: 229420\n",
      "loss: 0.0014\n",
      "grad_norm: 0.044512104243040085\n",
      "learning_rate: 2.7775404622905688e-05\n",
      "epoch: 8.166737861312829\n",
      "\n",
      "\n",
      "Step: 229440\n",
      "loss: 0.0012\n",
      "grad_norm: 0.05571293085813522\n",
      "learning_rate: 2.775167307418482e-05\n",
      "epoch: 8.167449807774455\n",
      "\n",
      "\n",
      "Step: 229460\n",
      "loss: 0.0013\n",
      "grad_norm: 0.025122791528701782\n",
      "learning_rate: 2.7727941525463948e-05\n",
      "epoch: 8.168161754236081\n",
      "\n",
      "\n",
      "Step: 229480\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04815029352903366\n",
      "learning_rate: 2.770420997674308e-05\n",
      "epoch: 8.168873700697707\n",
      "\n",
      "\n",
      "Step: 229500\n",
      "loss: 0.0018\n",
      "grad_norm: 0.025580504909157753\n",
      "learning_rate: 2.7680478428022208e-05\n",
      "epoch: 8.169585647159334\n",
      "\n",
      "\n",
      "Step: 229520\n",
      "loss: 0.001\n",
      "grad_norm: 0.01319263968616724\n",
      "learning_rate: 2.765674687930134e-05\n",
      "epoch: 8.17029759362096\n",
      "\n",
      "\n",
      "Step: 229540\n",
      "loss: 0.0009\n",
      "grad_norm: 0.012247029691934586\n",
      "learning_rate: 2.763301533058047e-05\n",
      "epoch: 8.171009540082586\n",
      "\n",
      "\n",
      "Step: 229560\n",
      "loss: 0.001\n",
      "grad_norm: 0.031097711995244026\n",
      "learning_rate: 2.76092837818596e-05\n",
      "epoch: 8.171721486544213\n",
      "\n",
      "\n",
      "Step: 229580\n",
      "loss: 0.0016\n",
      "grad_norm: 0.037475164979696274\n",
      "learning_rate: 2.7585552233138732e-05\n",
      "epoch: 8.172433433005837\n",
      "\n",
      "\n",
      "Step: 229600\n",
      "loss: 0.001\n",
      "grad_norm: 0.021283049136400223\n",
      "learning_rate: 2.756182068441786e-05\n",
      "epoch: 8.173145379467464\n",
      "\n",
      "\n",
      "Step: 229620\n",
      "loss: 0.0011\n",
      "grad_norm: 0.023985471576452255\n",
      "learning_rate: 2.7538089135696992e-05\n",
      "epoch: 8.17385732592909\n",
      "\n",
      "\n",
      "Step: 229640\n",
      "loss: 0.001\n",
      "grad_norm: 0.008182570338249207\n",
      "learning_rate: 2.7514357586976124e-05\n",
      "epoch: 8.174569272390716\n",
      "\n",
      "\n",
      "Step: 229660\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03859727829694748\n",
      "learning_rate: 2.7490626038255252e-05\n",
      "epoch: 8.175281218852342\n",
      "\n",
      "\n",
      "Step: 229680\n",
      "loss: 0.001\n",
      "grad_norm: 0.024180658161640167\n",
      "learning_rate: 2.7466894489534384e-05\n",
      "epoch: 8.175993165313969\n",
      "\n",
      "\n",
      "Step: 229700\n",
      "loss: 0.0012\n",
      "grad_norm: 0.009894768707454205\n",
      "learning_rate: 2.7443162940813512e-05\n",
      "epoch: 8.176705111775595\n",
      "\n",
      "\n",
      "Step: 229720\n",
      "loss: 0.0017\n",
      "grad_norm: 0.015095929615199566\n",
      "learning_rate: 2.7419431392092648e-05\n",
      "epoch: 8.177417058237221\n",
      "\n",
      "\n",
      "Step: 229740\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0027233141008764505\n",
      "learning_rate: 2.739569984337178e-05\n",
      "epoch: 8.178129004698846\n",
      "\n",
      "\n",
      "Step: 229760\n",
      "loss: 0.0011\n",
      "grad_norm: 0.001081183785572648\n",
      "learning_rate: 2.7371968294650908e-05\n",
      "epoch: 8.178840951160472\n",
      "\n",
      "\n",
      "Step: 229780\n",
      "loss: 0.0013\n",
      "grad_norm: 0.007553005125373602\n",
      "learning_rate: 2.734823674593004e-05\n",
      "epoch: 8.179552897622099\n",
      "\n",
      "\n",
      "Step: 229800\n",
      "loss: 0.001\n",
      "grad_norm: 0.0028868173249065876\n",
      "learning_rate: 2.7324505197209168e-05\n",
      "epoch: 8.180264844083725\n",
      "\n",
      "\n",
      "Step: 229820\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0017836261540651321\n",
      "learning_rate: 2.73007736484883e-05\n",
      "epoch: 8.180976790545351\n",
      "\n",
      "\n",
      "Step: 229840\n",
      "loss: 0.0007\n",
      "grad_norm: 0.014335515908896923\n",
      "learning_rate: 2.7277042099767428e-05\n",
      "epoch: 8.181688737006978\n",
      "\n",
      "\n",
      "Step: 229860\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0287819541990757\n",
      "learning_rate: 2.725331055104656e-05\n",
      "epoch: 8.182400683468604\n",
      "\n",
      "\n",
      "Step: 229880\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02764340117573738\n",
      "learning_rate: 2.7229579002325692e-05\n",
      "epoch: 8.183112629930228\n",
      "\n",
      "\n",
      "Step: 229900\n",
      "loss: 0.0013\n",
      "grad_norm: 0.011135407723486423\n",
      "learning_rate: 2.720584745360482e-05\n",
      "epoch: 8.183824576391855\n",
      "\n",
      "\n",
      "Step: 229920\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01550258882343769\n",
      "learning_rate: 2.7182115904883952e-05\n",
      "epoch: 8.184536522853481\n",
      "\n",
      "\n",
      "Step: 229940\n",
      "loss: 0.0011\n",
      "grad_norm: 0.06982208788394928\n",
      "learning_rate: 2.715838435616308e-05\n",
      "epoch: 8.185248469315107\n",
      "\n",
      "\n",
      "Step: 229960\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0021183073986321688\n",
      "learning_rate: 2.7134652807442212e-05\n",
      "epoch: 8.185960415776734\n",
      "\n",
      "\n",
      "Step: 229980\n",
      "loss: 0.0007\n",
      "grad_norm: 0.03943488001823425\n",
      "learning_rate: 2.7110921258721344e-05\n",
      "epoch: 8.18667236223836\n",
      "\n",
      "\n",
      "Step: 230000\n",
      "loss: 0.0016\n",
      "grad_norm: 0.03950582072138786\n",
      "learning_rate: 2.7087189710000472e-05\n",
      "epoch: 8.187384308699986\n",
      "\n",
      "\n",
      "Step: 230020\n",
      "loss: 0.0011\n",
      "grad_norm: 0.027982285246253014\n",
      "learning_rate: 2.7063458161279604e-05\n",
      "epoch: 8.188096255161613\n",
      "\n",
      "\n",
      "Step: 230040\n",
      "loss: 0.0009\n",
      "grad_norm: 0.012239427305758\n",
      "learning_rate: 2.7039726612558733e-05\n",
      "epoch: 8.188808201623237\n",
      "\n",
      "\n",
      "Step: 230060\n",
      "loss: 0.0013\n",
      "grad_norm: 0.010024397633969784\n",
      "learning_rate: 2.7015995063837864e-05\n",
      "epoch: 8.189520148084863\n",
      "\n",
      "\n",
      "Step: 230080\n",
      "loss: 0.0012\n",
      "grad_norm: 0.010342387482523918\n",
      "learning_rate: 2.6992263515116993e-05\n",
      "epoch: 8.19023209454649\n",
      "\n",
      "\n",
      "Step: 230100\n",
      "loss: 0.0016\n",
      "grad_norm: 0.018529942259192467\n",
      "learning_rate: 2.6968531966396125e-05\n",
      "epoch: 8.190944041008116\n",
      "\n",
      "\n",
      "Step: 230120\n",
      "loss: 0.001\n",
      "grad_norm: 0.012008937075734138\n",
      "learning_rate: 2.6944800417675256e-05\n",
      "epoch: 8.191655987469742\n",
      "\n",
      "\n",
      "Step: 230140\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0027788556180894375\n",
      "learning_rate: 2.6921068868954385e-05\n",
      "epoch: 8.192367933931369\n",
      "\n",
      "\n",
      "Step: 230160\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0020810950081795454\n",
      "learning_rate: 2.6897337320233517e-05\n",
      "epoch: 8.193079880392995\n",
      "\n",
      "\n",
      "Step: 230180\n",
      "loss: 0.001\n",
      "grad_norm: 0.030176591128110886\n",
      "learning_rate: 2.6873605771512645e-05\n",
      "epoch: 8.193791826854621\n",
      "\n",
      "\n",
      "Step: 230200\n",
      "loss: 0.0011\n",
      "grad_norm: 0.002423790981993079\n",
      "learning_rate: 2.6849874222791777e-05\n",
      "epoch: 8.194503773316246\n",
      "\n",
      "\n",
      "Step: 230220\n",
      "loss: 0.0009\n",
      "grad_norm: 0.002434436697512865\n",
      "learning_rate: 2.682614267407091e-05\n",
      "epoch: 8.195215719777872\n",
      "\n",
      "\n",
      "Step: 230240\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04756873473525047\n",
      "learning_rate: 2.6802411125350037e-05\n",
      "epoch: 8.195927666239498\n",
      "\n",
      "\n",
      "Step: 230260\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0021284259855747223\n",
      "learning_rate: 2.677867957662917e-05\n",
      "epoch: 8.196639612701125\n",
      "\n",
      "\n",
      "Step: 230280\n",
      "loss: 0.0013\n",
      "grad_norm: 0.006599016487598419\n",
      "learning_rate: 2.6754948027908297e-05\n",
      "epoch: 8.197351559162751\n",
      "\n",
      "\n",
      "Step: 230300\n",
      "loss: 0.001\n",
      "grad_norm: 0.020273691043257713\n",
      "learning_rate: 2.673121647918743e-05\n",
      "epoch: 8.198063505624377\n",
      "\n",
      "\n",
      "Step: 230320\n",
      "loss: 0.0013\n",
      "grad_norm: 0.040551237761974335\n",
      "learning_rate: 2.670748493046656e-05\n",
      "epoch: 8.198775452086004\n",
      "\n",
      "\n",
      "Step: 230340\n",
      "loss: 0.001\n",
      "grad_norm: 0.006832740269601345\n",
      "learning_rate: 2.668375338174569e-05\n",
      "epoch: 8.19948739854763\n",
      "\n",
      "\n",
      "Step: 230360\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04544518142938614\n",
      "learning_rate: 2.666002183302482e-05\n",
      "epoch: 8.200199345009255\n",
      "\n",
      "\n",
      "Step: 230380\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0020995887462049723\n",
      "learning_rate: 2.663629028430395e-05\n",
      "epoch: 8.20091129147088\n",
      "\n",
      "\n",
      "Step: 230400\n",
      "loss: 0.0013\n",
      "grad_norm: 0.003664835821837187\n",
      "learning_rate: 2.661255873558308e-05\n",
      "epoch: 8.201623237932507\n",
      "\n",
      "\n",
      "Step: 230420\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0006511952960863709\n",
      "learning_rate: 2.658882718686221e-05\n",
      "epoch: 8.202335184394133\n",
      "\n",
      "\n",
      "Step: 230440\n",
      "loss: 0.0009\n",
      "grad_norm: 0.007982694543898106\n",
      "learning_rate: 2.656509563814134e-05\n",
      "epoch: 8.20304713085576\n",
      "\n",
      "\n",
      "Step: 230460\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0140443230047822\n",
      "learning_rate: 2.6541364089420473e-05\n",
      "epoch: 8.203759077317386\n",
      "\n",
      "\n",
      "Step: 230480\n",
      "loss: 0.001\n",
      "grad_norm: 0.00808750744909048\n",
      "learning_rate: 2.65176325406996e-05\n",
      "epoch: 8.204471023779012\n",
      "\n",
      "\n",
      "Step: 230500\n",
      "loss: 0.0009\n",
      "grad_norm: 0.00919938925653696\n",
      "learning_rate: 2.6493900991978733e-05\n",
      "epoch: 8.205182970240639\n",
      "\n",
      "\n",
      "Step: 230520\n",
      "loss: 0.0016\n",
      "grad_norm: 0.017859015613794327\n",
      "learning_rate: 2.6470169443257862e-05\n",
      "epoch: 8.205894916702263\n",
      "\n",
      "\n",
      "Step: 230540\n",
      "loss: 0.001\n",
      "grad_norm: 0.006001634988933802\n",
      "learning_rate: 2.6446437894536994e-05\n",
      "epoch: 8.20660686316389\n",
      "\n",
      "\n",
      "Step: 230560\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01701973006129265\n",
      "learning_rate: 2.642270634581613e-05\n",
      "epoch: 8.207318809625516\n",
      "\n",
      "\n",
      "Step: 230580\n",
      "loss: 0.0016\n",
      "grad_norm: 0.022598618641495705\n",
      "learning_rate: 2.6398974797095257e-05\n",
      "epoch: 8.208030756087142\n",
      "\n",
      "\n",
      "Step: 230600\n",
      "loss: 0.0013\n",
      "grad_norm: 0.037983238697052\n",
      "learning_rate: 2.637524324837439e-05\n",
      "epoch: 8.208742702548768\n",
      "\n",
      "\n",
      "Step: 230620\n",
      "loss: 0.0022\n",
      "grad_norm: 0.0124506251886487\n",
      "learning_rate: 2.6351511699653517e-05\n",
      "epoch: 8.209454649010395\n",
      "\n",
      "\n",
      "Step: 230640\n",
      "loss: 0.001\n",
      "grad_norm: 0.0307293813675642\n",
      "learning_rate: 2.632778015093265e-05\n",
      "epoch: 8.210166595472021\n",
      "\n",
      "\n",
      "Step: 230660\n",
      "loss: 0.0009\n",
      "grad_norm: 0.012718533165752888\n",
      "learning_rate: 2.6304048602211778e-05\n",
      "epoch: 8.210878541933647\n",
      "\n",
      "\n",
      "Step: 230680\n",
      "loss: 0.0013\n",
      "grad_norm: 0.014976239763200283\n",
      "learning_rate: 2.628031705349091e-05\n",
      "epoch: 8.211590488395272\n",
      "\n",
      "\n",
      "Step: 230700\n",
      "loss: 0.0011\n",
      "grad_norm: 0.011529178358614445\n",
      "learning_rate: 2.625658550477004e-05\n",
      "epoch: 8.212302434856898\n",
      "\n",
      "\n",
      "Step: 230720\n",
      "loss: 0.0012\n",
      "grad_norm: 0.022285109385848045\n",
      "learning_rate: 2.623285395604917e-05\n",
      "epoch: 8.213014381318525\n",
      "\n",
      "\n",
      "Step: 230740\n",
      "loss: 0.0014\n",
      "grad_norm: 0.008767758496105671\n",
      "learning_rate: 2.62091224073283e-05\n",
      "epoch: 8.21372632778015\n",
      "\n",
      "\n",
      "Step: 230760\n",
      "loss: 0.0011\n",
      "grad_norm: 0.023437121883034706\n",
      "learning_rate: 2.618539085860743e-05\n",
      "epoch: 8.214438274241777\n",
      "\n",
      "\n",
      "Step: 230780\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0223257876932621\n",
      "learning_rate: 2.6161659309886562e-05\n",
      "epoch: 8.215150220703404\n",
      "\n",
      "\n",
      "Step: 230800\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04232397302985191\n",
      "learning_rate: 2.6137927761165693e-05\n",
      "epoch: 8.21586216716503\n",
      "\n",
      "\n",
      "Step: 230820\n",
      "loss: 0.001\n",
      "grad_norm: 0.03309674561023712\n",
      "learning_rate: 2.6114196212444822e-05\n",
      "epoch: 8.216574113626656\n",
      "\n",
      "\n",
      "Step: 230840\n",
      "loss: 0.001\n",
      "grad_norm: 0.014997598715126514\n",
      "learning_rate: 2.6090464663723954e-05\n",
      "epoch: 8.21728606008828\n",
      "\n",
      "\n",
      "Step: 230860\n",
      "loss: 0.0008\n",
      "grad_norm: 0.03677242621779442\n",
      "learning_rate: 2.6066733115003082e-05\n",
      "epoch: 8.217998006549907\n",
      "\n",
      "\n",
      "Step: 230880\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02133128046989441\n",
      "learning_rate: 2.6043001566282214e-05\n",
      "epoch: 8.218709953011533\n",
      "\n",
      "\n",
      "Step: 230900\n",
      "loss: 0.0015\n",
      "grad_norm: 0.03170321136713028\n",
      "learning_rate: 2.6019270017561346e-05\n",
      "epoch: 8.21942189947316\n",
      "\n",
      "\n",
      "Step: 230920\n",
      "loss: 0.002\n",
      "grad_norm: 0.041043467819690704\n",
      "learning_rate: 2.5995538468840474e-05\n",
      "epoch: 8.220133845934786\n",
      "\n",
      "\n",
      "Step: 230940\n",
      "loss: 0.0011\n",
      "grad_norm: 0.034733790904283524\n",
      "learning_rate: 2.5971806920119606e-05\n",
      "epoch: 8.220845792396412\n",
      "\n",
      "\n",
      "Step: 230960\n",
      "loss: 0.0013\n",
      "grad_norm: 0.025270484387874603\n",
      "learning_rate: 2.5948075371398734e-05\n",
      "epoch: 8.221557738858039\n",
      "\n",
      "\n",
      "Step: 230980\n",
      "loss: 0.0019\n",
      "grad_norm: 0.019589409232139587\n",
      "learning_rate: 2.5924343822677866e-05\n",
      "epoch: 8.222269685319663\n",
      "\n",
      "\n",
      "Step: 231000\n",
      "loss: 0.0011\n",
      "grad_norm: 0.06632804870605469\n",
      "learning_rate: 2.5900612273956995e-05\n",
      "epoch: 8.22298163178129\n",
      "\n",
      "\n",
      "Step: 231020\n",
      "loss: 0.0011\n",
      "grad_norm: 0.060918521136045456\n",
      "learning_rate: 2.5876880725236126e-05\n",
      "epoch: 8.223693578242916\n",
      "\n",
      "\n",
      "Step: 231040\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0136862238869071\n",
      "learning_rate: 2.5853149176515258e-05\n",
      "epoch: 8.224405524704542\n",
      "\n",
      "\n",
      "Step: 231060\n",
      "loss: 0.0011\n",
      "grad_norm: 0.016220327466726303\n",
      "learning_rate: 2.5829417627794387e-05\n",
      "epoch: 8.225117471166168\n",
      "\n",
      "\n",
      "Step: 231080\n",
      "loss: 0.0006\n",
      "grad_norm: 0.027383795008063316\n",
      "learning_rate: 2.580568607907352e-05\n",
      "epoch: 8.225829417627795\n",
      "\n",
      "\n",
      "Step: 231100\n",
      "loss: 0.0013\n",
      "grad_norm: 0.007781376596540213\n",
      "learning_rate: 2.5781954530352647e-05\n",
      "epoch: 8.226541364089421\n",
      "\n",
      "\n",
      "Step: 231120\n",
      "loss: 0.0013\n",
      "grad_norm: 0.006498420611023903\n",
      "learning_rate: 2.575822298163178e-05\n",
      "epoch: 8.227253310551047\n",
      "\n",
      "\n",
      "Step: 231140\n",
      "loss: 0.0011\n",
      "grad_norm: 0.023939236998558044\n",
      "learning_rate: 2.573449143291091e-05\n",
      "epoch: 8.227965257012672\n",
      "\n",
      "\n",
      "Step: 231160\n",
      "loss: 0.0009\n",
      "grad_norm: 0.017824191600084305\n",
      "learning_rate: 2.571075988419004e-05\n",
      "epoch: 8.228677203474298\n",
      "\n",
      "\n",
      "Step: 231180\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0564732700586319\n",
      "learning_rate: 2.568702833546917e-05\n",
      "epoch: 8.229389149935924\n",
      "\n",
      "\n",
      "Step: 231200\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03255893290042877\n",
      "learning_rate: 2.56632967867483e-05\n",
      "epoch: 8.23010109639755\n",
      "\n",
      "\n",
      "Step: 231220\n",
      "loss: 0.0008\n",
      "grad_norm: 0.025757664814591408\n",
      "learning_rate: 2.563956523802743e-05\n",
      "epoch: 8.230813042859177\n",
      "\n",
      "\n",
      "Step: 231240\n",
      "loss: 0.0008\n",
      "grad_norm: 0.02885124646127224\n",
      "learning_rate: 2.561583368930656e-05\n",
      "epoch: 8.231524989320803\n",
      "\n",
      "\n",
      "Step: 231260\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0061271702870726585\n",
      "learning_rate: 2.559210214058569e-05\n",
      "epoch: 8.23223693578243\n",
      "\n",
      "\n",
      "Step: 231280\n",
      "loss: 0.0012\n",
      "grad_norm: 0.006456337869167328\n",
      "learning_rate: 2.5568370591864823e-05\n",
      "epoch: 8.232948882244056\n",
      "\n",
      "\n",
      "Step: 231300\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008057082071900368\n",
      "learning_rate: 2.554463904314395e-05\n",
      "epoch: 8.23366082870568\n",
      "\n",
      "\n",
      "Step: 231320\n",
      "loss: 0.001\n",
      "grad_norm: 0.026772456243634224\n",
      "learning_rate: 2.5520907494423083e-05\n",
      "epoch: 8.234372775167307\n",
      "\n",
      "\n",
      "Step: 231340\n",
      "loss: 0.001\n",
      "grad_norm: 0.013812413439154625\n",
      "learning_rate: 2.549717594570221e-05\n",
      "epoch: 8.235084721628933\n",
      "\n",
      "\n",
      "Step: 231360\n",
      "loss: 0.0014\n",
      "grad_norm: 0.018883736804127693\n",
      "learning_rate: 2.5473444396981343e-05\n",
      "epoch: 8.23579666809056\n",
      "\n",
      "\n",
      "Step: 231380\n",
      "loss: 0.001\n",
      "grad_norm: 0.02996269054710865\n",
      "learning_rate: 2.544971284826048e-05\n",
      "epoch: 8.236508614552186\n",
      "\n",
      "\n",
      "Step: 231400\n",
      "loss: 0.0015\n",
      "grad_norm: 0.014830074273049831\n",
      "learning_rate: 2.5425981299539603e-05\n",
      "epoch: 8.237220561013812\n",
      "\n",
      "\n",
      "Step: 231420\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03545248135924339\n",
      "learning_rate: 2.540224975081874e-05\n",
      "epoch: 8.237932507475438\n",
      "\n",
      "\n",
      "Step: 231440\n",
      "loss: 0.001\n",
      "grad_norm: 0.011252446100115776\n",
      "learning_rate: 2.5378518202097867e-05\n",
      "epoch: 8.238644453937065\n",
      "\n",
      "\n",
      "Step: 231460\n",
      "loss: 0.0012\n",
      "grad_norm: 0.009595073759555817\n",
      "learning_rate: 2.5354786653377e-05\n",
      "epoch: 8.23935640039869\n",
      "\n",
      "\n",
      "Step: 231480\n",
      "loss: 0.0007\n",
      "grad_norm: 0.001884419354610145\n",
      "learning_rate: 2.533105510465613e-05\n",
      "epoch: 8.240068346860316\n",
      "\n",
      "\n",
      "Step: 231500\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03806406259536743\n",
      "learning_rate: 2.530732355593526e-05\n",
      "epoch: 8.240780293321942\n",
      "\n",
      "\n",
      "Step: 231520\n",
      "loss: 0.0017\n",
      "grad_norm: 0.013224571011960506\n",
      "learning_rate: 2.528359200721439e-05\n",
      "epoch: 8.241492239783568\n",
      "\n",
      "\n",
      "Step: 231540\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0375370979309082\n",
      "learning_rate: 2.525986045849352e-05\n",
      "epoch: 8.242204186245194\n",
      "\n",
      "\n",
      "Step: 231560\n",
      "loss: 0.0012\n",
      "grad_norm: 0.015413405373692513\n",
      "learning_rate: 2.523612890977265e-05\n",
      "epoch: 8.24291613270682\n",
      "\n",
      "\n",
      "Step: 231580\n",
      "loss: 0.0011\n",
      "grad_norm: 0.007999232970178127\n",
      "learning_rate: 2.521239736105178e-05\n",
      "epoch: 8.243628079168447\n",
      "\n",
      "\n",
      "Step: 231600\n",
      "loss: 0.0015\n",
      "grad_norm: 0.028860369697213173\n",
      "learning_rate: 2.518866581233091e-05\n",
      "epoch: 8.244340025630073\n",
      "\n",
      "\n",
      "Step: 231620\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02294095605611801\n",
      "learning_rate: 2.5164934263610043e-05\n",
      "epoch: 8.245051972091698\n",
      "\n",
      "\n",
      "Step: 231640\n",
      "loss: 0.0012\n",
      "grad_norm: 0.025997499004006386\n",
      "learning_rate: 2.514120271488917e-05\n",
      "epoch: 8.245763918553324\n",
      "\n",
      "\n",
      "Step: 231660\n",
      "loss: 0.0006\n",
      "grad_norm: 0.016659416258335114\n",
      "learning_rate: 2.5117471166168303e-05\n",
      "epoch: 8.24647586501495\n",
      "\n",
      "\n",
      "Step: 231680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.033896397799253464\n",
      "learning_rate: 2.509373961744743e-05\n",
      "epoch: 8.247187811476577\n",
      "\n",
      "\n",
      "Step: 231700\n",
      "loss: 0.001\n",
      "grad_norm: 0.03538905084133148\n",
      "learning_rate: 2.5070008068726563e-05\n",
      "epoch: 8.247899757938203\n",
      "\n",
      "\n",
      "Step: 231720\n",
      "loss: 0.0014\n",
      "grad_norm: 0.01398052554577589\n",
      "learning_rate: 2.5046276520005695e-05\n",
      "epoch: 8.24861170439983\n",
      "\n",
      "\n",
      "Step: 231740\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01292654499411583\n",
      "learning_rate: 2.5022544971284824e-05\n",
      "epoch: 8.249323650861456\n",
      "\n",
      "\n",
      "Step: 231760\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0035462642554193735\n",
      "learning_rate: 2.4998813422563955e-05\n",
      "epoch: 8.250035597323082\n",
      "\n",
      "\n",
      "Step: 231780\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01608608290553093\n",
      "learning_rate: 2.4975081873843084e-05\n",
      "epoch: 8.250747543784707\n",
      "\n",
      "\n",
      "Step: 231800\n",
      "loss: 0.0007\n",
      "grad_norm: 0.046512097120285034\n",
      "learning_rate: 2.4951350325122216e-05\n",
      "epoch: 8.251459490246333\n",
      "\n",
      "\n",
      "Step: 231820\n",
      "loss: 0.0009\n",
      "grad_norm: 0.002920905826613307\n",
      "learning_rate: 2.4927618776401344e-05\n",
      "epoch: 8.25217143670796\n",
      "\n",
      "\n",
      "Step: 231840\n",
      "loss: 0.0011\n",
      "grad_norm: 0.011486601084470749\n",
      "learning_rate: 2.4903887227680476e-05\n",
      "epoch: 8.252883383169586\n",
      "\n",
      "\n",
      "Step: 231860\n",
      "loss: 0.0011\n",
      "grad_norm: 0.026362484320998192\n",
      "learning_rate: 2.4880155678959608e-05\n",
      "epoch: 8.253595329631212\n",
      "\n",
      "\n",
      "Step: 231880\n",
      "loss: 0.0012\n",
      "grad_norm: 0.022774582728743553\n",
      "learning_rate: 2.4856424130238736e-05\n",
      "epoch: 8.254307276092838\n",
      "\n",
      "\n",
      "Step: 231900\n",
      "loss: 0.0016\n",
      "grad_norm: 0.06898901611566544\n",
      "learning_rate: 2.4832692581517868e-05\n",
      "epoch: 8.255019222554465\n",
      "\n",
      "\n",
      "Step: 231920\n",
      "loss: 0.0009\n",
      "grad_norm: 0.016960399225354195\n",
      "learning_rate: 2.4808961032796996e-05\n",
      "epoch: 8.25573116901609\n",
      "\n",
      "\n",
      "Step: 231940\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0033839778043329716\n",
      "learning_rate: 2.4785229484076128e-05\n",
      "epoch: 8.256443115477715\n",
      "\n",
      "\n",
      "Step: 231960\n",
      "loss: 0.0014\n",
      "grad_norm: 0.01569642685353756\n",
      "learning_rate: 2.476149793535526e-05\n",
      "epoch: 8.257155061939342\n",
      "\n",
      "\n",
      "Step: 231980\n",
      "loss: 0.0018\n",
      "grad_norm: 0.057879190891981125\n",
      "learning_rate: 2.4737766386634388e-05\n",
      "epoch: 8.257867008400968\n",
      "\n",
      "\n",
      "Step: 232000\n",
      "loss: 0.0017\n",
      "grad_norm: 0.02194276824593544\n",
      "learning_rate: 2.471403483791352e-05\n",
      "epoch: 8.258578954862594\n",
      "\n",
      "\n",
      "Step: 232020\n",
      "loss: 0.0015\n",
      "grad_norm: 0.005646278150379658\n",
      "learning_rate: 2.469030328919265e-05\n",
      "epoch: 8.25929090132422\n",
      "\n",
      "\n",
      "Step: 232040\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01766986958682537\n",
      "learning_rate: 2.466657174047178e-05\n",
      "epoch: 8.260002847785847\n",
      "\n",
      "\n",
      "Step: 232060\n",
      "loss: 0.0014\n",
      "grad_norm: 0.008830045349895954\n",
      "learning_rate: 2.4642840191750912e-05\n",
      "epoch: 8.260714794247473\n",
      "\n",
      "\n",
      "Step: 232080\n",
      "loss: 0.0013\n",
      "grad_norm: 0.019907236099243164\n",
      "learning_rate: 2.461910864303004e-05\n",
      "epoch: 8.261426740709098\n",
      "\n",
      "\n",
      "Step: 232100\n",
      "loss: 0.001\n",
      "grad_norm: 0.0011459048837423325\n",
      "learning_rate: 2.4595377094309172e-05\n",
      "epoch: 8.262138687170724\n",
      "\n",
      "\n",
      "Step: 232120\n",
      "loss: 0.0013\n",
      "grad_norm: 0.006239227019250393\n",
      "learning_rate: 2.45716455455883e-05\n",
      "epoch: 8.26285063363235\n",
      "\n",
      "\n",
      "Step: 232140\n",
      "loss: 0.0013\n",
      "grad_norm: 0.024386540055274963\n",
      "learning_rate: 2.4547913996867432e-05\n",
      "epoch: 8.263562580093977\n",
      "\n",
      "\n",
      "Step: 232160\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0006007293122820556\n",
      "learning_rate: 2.452418244814656e-05\n",
      "epoch: 8.264274526555603\n",
      "\n",
      "\n",
      "Step: 232180\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01084293145686388\n",
      "learning_rate: 2.4500450899425693e-05\n",
      "epoch: 8.26498647301723\n",
      "\n",
      "\n",
      "Step: 232200\n",
      "loss: 0.0011\n",
      "grad_norm: 0.021517056971788406\n",
      "learning_rate: 2.4476719350704828e-05\n",
      "epoch: 8.265698419478856\n",
      "\n",
      "\n",
      "Step: 232220\n",
      "loss: 0.0013\n",
      "grad_norm: 0.013949276879429817\n",
      "learning_rate: 2.4452987801983953e-05\n",
      "epoch: 8.266410365940482\n",
      "\n",
      "\n",
      "Step: 232240\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00978885032236576\n",
      "learning_rate: 2.4429256253263088e-05\n",
      "epoch: 8.267122312402108\n",
      "\n",
      "\n",
      "Step: 232260\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010206913575530052\n",
      "learning_rate: 2.4405524704542213e-05\n",
      "epoch: 8.267834258863733\n",
      "\n",
      "\n",
      "Step: 232280\n",
      "loss: 0.0012\n",
      "grad_norm: 0.014982616528868675\n",
      "learning_rate: 2.4381793155821348e-05\n",
      "epoch: 8.268546205325359\n",
      "\n",
      "\n",
      "Step: 232300\n",
      "loss: 0.0012\n",
      "grad_norm: 0.019178522750735283\n",
      "learning_rate: 2.435806160710048e-05\n",
      "epoch: 8.269258151786985\n",
      "\n",
      "\n",
      "Step: 232320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.017969543114304543\n",
      "learning_rate: 2.433433005837961e-05\n",
      "epoch: 8.269970098248612\n",
      "\n",
      "\n",
      "Step: 232340\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0035445925313979387\n",
      "learning_rate: 2.431059850965874e-05\n",
      "epoch: 8.270682044710238\n",
      "\n",
      "\n",
      "Step: 232360\n",
      "loss: 0.0016\n",
      "grad_norm: 0.0215079877525568\n",
      "learning_rate: 2.428686696093787e-05\n",
      "epoch: 8.271393991171864\n",
      "\n",
      "\n",
      "Step: 232380\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008719808422029018\n",
      "learning_rate: 2.4263135412217e-05\n",
      "epoch: 8.27210593763349\n",
      "\n",
      "\n",
      "Step: 232400\n",
      "loss: 0.0011\n",
      "grad_norm: 0.027755042538046837\n",
      "learning_rate: 2.423940386349613e-05\n",
      "epoch: 8.272817884095115\n",
      "\n",
      "\n",
      "Step: 232420\n",
      "loss: 0.0014\n",
      "grad_norm: 0.006178488023579121\n",
      "learning_rate: 2.421567231477526e-05\n",
      "epoch: 8.273529830556742\n",
      "\n",
      "\n",
      "Step: 232440\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010975551791489124\n",
      "learning_rate: 2.4191940766054392e-05\n",
      "epoch: 8.274241777018368\n",
      "\n",
      "\n",
      "Step: 232460\n",
      "loss: 0.0007\n",
      "grad_norm: 0.021038634702563286\n",
      "learning_rate: 2.416820921733352e-05\n",
      "epoch: 8.274953723479994\n",
      "\n",
      "\n",
      "Step: 232480\n",
      "loss: 0.0012\n",
      "grad_norm: 0.010846605524420738\n",
      "learning_rate: 2.4144477668612653e-05\n",
      "epoch: 8.27566566994162\n",
      "\n",
      "\n",
      "Step: 232500\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011649991385638714\n",
      "learning_rate: 2.412074611989178e-05\n",
      "epoch: 8.276377616403247\n",
      "\n",
      "\n",
      "Step: 232520\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02596301957964897\n",
      "learning_rate: 2.4097014571170913e-05\n",
      "epoch: 8.277089562864873\n",
      "\n",
      "\n",
      "Step: 232540\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04903928562998772\n",
      "learning_rate: 2.4073283022450045e-05\n",
      "epoch: 8.2778015093265\n",
      "\n",
      "\n",
      "Step: 232560\n",
      "loss: 0.0008\n",
      "grad_norm: 0.013514523394405842\n",
      "learning_rate: 2.4049551473729173e-05\n",
      "epoch: 8.278513455788124\n",
      "\n",
      "\n",
      "Step: 232580\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02040974050760269\n",
      "learning_rate: 2.4025819925008305e-05\n",
      "epoch: 8.27922540224975\n",
      "\n",
      "\n",
      "Step: 232600\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03298104181885719\n",
      "learning_rate: 2.4002088376287433e-05\n",
      "epoch: 8.279937348711377\n",
      "\n",
      "\n",
      "Step: 232620\n",
      "loss: 0.0011\n",
      "grad_norm: 0.020368775352835655\n",
      "learning_rate: 2.3978356827566565e-05\n",
      "epoch: 8.280649295173003\n",
      "\n",
      "\n",
      "Step: 232640\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012969649396836758\n",
      "learning_rate: 2.3954625278845697e-05\n",
      "epoch: 8.28136124163463\n",
      "\n",
      "\n",
      "Step: 232660\n",
      "loss: 0.001\n",
      "grad_norm: 0.02491404302418232\n",
      "learning_rate: 2.3930893730124825e-05\n",
      "epoch: 8.282073188096255\n",
      "\n",
      "\n",
      "Step: 232680\n",
      "loss: 0.001\n",
      "grad_norm: 0.0643640011548996\n",
      "learning_rate: 2.3907162181403957e-05\n",
      "epoch: 8.282785134557882\n",
      "\n",
      "\n",
      "Step: 232700\n",
      "loss: 0.001\n",
      "grad_norm: 0.0028934029396623373\n",
      "learning_rate: 2.3883430632683086e-05\n",
      "epoch: 8.283497081019508\n",
      "\n",
      "\n",
      "Step: 232720\n",
      "loss: 0.0008\n",
      "grad_norm: 0.005669783800840378\n",
      "learning_rate: 2.3859699083962217e-05\n",
      "epoch: 8.284209027481133\n",
      "\n",
      "\n",
      "Step: 232740\n",
      "loss: 0.001\n",
      "grad_norm: 0.013103079050779343\n",
      "learning_rate: 2.3835967535241346e-05\n",
      "epoch: 8.284920973942759\n",
      "\n",
      "\n",
      "Step: 232760\n",
      "loss: 0.0007\n",
      "grad_norm: 0.013498382642865181\n",
      "learning_rate: 2.3812235986520478e-05\n",
      "epoch: 8.285632920404385\n",
      "\n",
      "\n",
      "Step: 232780\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04559621214866638\n",
      "learning_rate: 2.378850443779961e-05\n",
      "epoch: 8.286344866866012\n",
      "\n",
      "\n",
      "Step: 232800\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02282373048365116\n",
      "learning_rate: 2.3764772889078738e-05\n",
      "epoch: 8.287056813327638\n",
      "\n",
      "\n",
      "Step: 232820\n",
      "loss: 0.0014\n",
      "grad_norm: 0.029841464012861252\n",
      "learning_rate: 2.374104134035787e-05\n",
      "epoch: 8.287768759789264\n",
      "\n",
      "\n",
      "Step: 232840\n",
      "loss: 0.0013\n",
      "grad_norm: 0.004304499365389347\n",
      "learning_rate: 2.3717309791636998e-05\n",
      "epoch: 8.28848070625089\n",
      "\n",
      "\n",
      "Step: 232860\n",
      "loss: 0.0009\n",
      "grad_norm: 0.026329530403017998\n",
      "learning_rate: 2.369357824291613e-05\n",
      "epoch: 8.289192652712517\n",
      "\n",
      "\n",
      "Step: 232880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.003097021719440818\n",
      "learning_rate: 2.366984669419526e-05\n",
      "epoch: 8.289904599174141\n",
      "\n",
      "\n",
      "Step: 232900\n",
      "loss: 0.0013\n",
      "grad_norm: 0.045370761305093765\n",
      "learning_rate: 2.364611514547439e-05\n",
      "epoch: 8.290616545635768\n",
      "\n",
      "\n",
      "Step: 232920\n",
      "loss: 0.0013\n",
      "grad_norm: 0.010922709479928017\n",
      "learning_rate: 2.3622383596753522e-05\n",
      "epoch: 8.291328492097394\n",
      "\n",
      "\n",
      "Step: 232940\n",
      "loss: 0.001\n",
      "grad_norm: 0.01662447303533554\n",
      "learning_rate: 2.359865204803265e-05\n",
      "epoch: 8.29204043855902\n",
      "\n",
      "\n",
      "Step: 232960\n",
      "loss: 0.0012\n",
      "grad_norm: 0.005972868762910366\n",
      "learning_rate: 2.3574920499311782e-05\n",
      "epoch: 8.292752385020647\n",
      "\n",
      "\n",
      "Step: 232980\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0030559555161744356\n",
      "learning_rate: 2.355118895059091e-05\n",
      "epoch: 8.293464331482273\n",
      "\n",
      "\n",
      "Step: 233000\n",
      "loss: 0.0007\n",
      "grad_norm: 0.006834081839770079\n",
      "learning_rate: 2.3527457401870042e-05\n",
      "epoch: 8.2941762779439\n",
      "\n",
      "\n",
      "Step: 233020\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04388727620244026\n",
      "learning_rate: 2.3503725853149177e-05\n",
      "epoch: 8.294888224405526\n",
      "\n",
      "\n",
      "Step: 233040\n",
      "loss: 0.0012\n",
      "grad_norm: 0.030146770179271698\n",
      "learning_rate: 2.3479994304428302e-05\n",
      "epoch: 8.29560017086715\n",
      "\n",
      "\n",
      "Step: 233060\n",
      "loss: 0.0011\n",
      "grad_norm: 0.027352292090654373\n",
      "learning_rate: 2.3456262755707438e-05\n",
      "epoch: 8.296312117328776\n",
      "\n",
      "\n",
      "Step: 233080\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0312742181122303\n",
      "learning_rate: 2.3432531206986563e-05\n",
      "epoch: 8.297024063790403\n",
      "\n",
      "\n",
      "Step: 233100\n",
      "loss: 0.001\n",
      "grad_norm: 0.06780717521905899\n",
      "learning_rate: 2.3408799658265698e-05\n",
      "epoch: 8.297736010252029\n",
      "\n",
      "\n",
      "Step: 233120\n",
      "loss: 0.0015\n",
      "grad_norm: 0.018406841903924942\n",
      "learning_rate: 2.338506810954483e-05\n",
      "epoch: 8.298447956713655\n",
      "\n",
      "\n",
      "Step: 233140\n",
      "loss: 0.001\n",
      "grad_norm: 0.052026160061359406\n",
      "learning_rate: 2.3361336560823958e-05\n",
      "epoch: 8.299159903175282\n",
      "\n",
      "\n",
      "Step: 233160\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01712423376739025\n",
      "learning_rate: 2.333760501210309e-05\n",
      "epoch: 8.299871849636908\n",
      "\n",
      "\n",
      "Step: 233180\n",
      "loss: 0.001\n",
      "grad_norm: 0.0129509586840868\n",
      "learning_rate: 2.3313873463382218e-05\n",
      "epoch: 8.300583796098534\n",
      "\n",
      "\n",
      "Step: 233200\n",
      "loss: 0.001\n",
      "grad_norm: 0.01841212622821331\n",
      "learning_rate: 2.329014191466135e-05\n",
      "epoch: 8.301295742560159\n",
      "\n",
      "\n",
      "Step: 233220\n",
      "loss: 0.0016\n",
      "grad_norm: 0.007760148495435715\n",
      "learning_rate: 2.3266410365940482e-05\n",
      "epoch: 8.302007689021785\n",
      "\n",
      "\n",
      "Step: 233240\n",
      "loss: 0.0018\n",
      "grad_norm: 0.026612471789121628\n",
      "learning_rate: 2.324267881721961e-05\n",
      "epoch: 8.302719635483411\n",
      "\n",
      "\n",
      "Step: 233260\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03297153115272522\n",
      "learning_rate: 2.3218947268498742e-05\n",
      "epoch: 8.303431581945038\n",
      "\n",
      "\n",
      "Step: 233280\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02406137064099312\n",
      "learning_rate: 2.319521571977787e-05\n",
      "epoch: 8.304143528406664\n",
      "\n",
      "\n",
      "Step: 233300\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0036074183881282806\n",
      "learning_rate: 2.3171484171057002e-05\n",
      "epoch: 8.30485547486829\n",
      "\n",
      "\n",
      "Step: 233320\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02772093564271927\n",
      "learning_rate: 2.314775262233613e-05\n",
      "epoch: 8.305567421329917\n",
      "\n",
      "\n",
      "Step: 233340\n",
      "loss: 0.0011\n",
      "grad_norm: 0.013938906602561474\n",
      "learning_rate: 2.3124021073615262e-05\n",
      "epoch: 8.306279367791543\n",
      "\n",
      "\n",
      "Step: 233360\n",
      "loss: 0.0017\n",
      "grad_norm: 0.002735608024522662\n",
      "learning_rate: 2.3100289524894394e-05\n",
      "epoch: 8.306991314253168\n",
      "\n",
      "\n",
      "Step: 233380\n",
      "loss: 0.001\n",
      "grad_norm: 0.02940632961690426\n",
      "learning_rate: 2.3076557976173523e-05\n",
      "epoch: 8.307703260714794\n",
      "\n",
      "\n",
      "Step: 233400\n",
      "loss: 0.001\n",
      "grad_norm: 0.016864348202943802\n",
      "learning_rate: 2.3052826427452654e-05\n",
      "epoch: 8.30841520717642\n",
      "\n",
      "\n",
      "Step: 233420\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02949489653110504\n",
      "learning_rate: 2.3029094878731783e-05\n",
      "epoch: 8.309127153638046\n",
      "\n",
      "\n",
      "Step: 233440\n",
      "loss: 0.001\n",
      "grad_norm: 0.023028399795293808\n",
      "learning_rate: 2.3005363330010915e-05\n",
      "epoch: 8.309839100099673\n",
      "\n",
      "\n",
      "Step: 233460\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0323234386742115\n",
      "learning_rate: 2.2981631781290046e-05\n",
      "epoch: 8.310551046561299\n",
      "\n",
      "\n",
      "Step: 233480\n",
      "loss: 0.0014\n",
      "grad_norm: 0.033408019691705704\n",
      "learning_rate: 2.2957900232569175e-05\n",
      "epoch: 8.311262993022925\n",
      "\n",
      "\n",
      "Step: 233500\n",
      "loss: 0.0012\n",
      "grad_norm: 0.008702434599399567\n",
      "learning_rate: 2.2934168683848307e-05\n",
      "epoch: 8.31197493948455\n",
      "\n",
      "\n",
      "Step: 233520\n",
      "loss: 0.0015\n",
      "grad_norm: 0.021814590319991112\n",
      "learning_rate: 2.2910437135127435e-05\n",
      "epoch: 8.312686885946176\n",
      "\n",
      "\n",
      "Step: 233540\n",
      "loss: 0.001\n",
      "grad_norm: 0.024819299578666687\n",
      "learning_rate: 2.2886705586406567e-05\n",
      "epoch: 8.313398832407803\n",
      "\n",
      "\n",
      "Step: 233560\n",
      "loss: 0.0013\n",
      "grad_norm: 0.046787407249212265\n",
      "learning_rate: 2.2862974037685695e-05\n",
      "epoch: 8.314110778869429\n",
      "\n",
      "\n",
      "Step: 233580\n",
      "loss: 0.0012\n",
      "grad_norm: 0.09833459556102753\n",
      "learning_rate: 2.2839242488964827e-05\n",
      "epoch: 8.314822725331055\n",
      "\n",
      "\n",
      "Step: 233600\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03056373819708824\n",
      "learning_rate: 2.281551094024396e-05\n",
      "epoch: 8.315534671792681\n",
      "\n",
      "\n",
      "Step: 233620\n",
      "loss: 0.0007\n",
      "grad_norm: 0.006652443669736385\n",
      "learning_rate: 2.2791779391523087e-05\n",
      "epoch: 8.316246618254308\n",
      "\n",
      "\n",
      "Step: 233640\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03591708466410637\n",
      "learning_rate: 2.276804784280222e-05\n",
      "epoch: 8.316958564715934\n",
      "\n",
      "\n",
      "Step: 233660\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03234844654798508\n",
      "learning_rate: 2.2744316294081347e-05\n",
      "epoch: 8.317670511177559\n",
      "\n",
      "\n",
      "Step: 233680\n",
      "loss: 0.0012\n",
      "grad_norm: 0.025902407243847847\n",
      "learning_rate: 2.272058474536048e-05\n",
      "epoch: 8.318382457639185\n",
      "\n",
      "\n",
      "Step: 233700\n",
      "loss: 0.0015\n",
      "grad_norm: 0.02348373271524906\n",
      "learning_rate: 2.269685319663961e-05\n",
      "epoch: 8.319094404100811\n",
      "\n",
      "\n",
      "Step: 233720\n",
      "loss: 0.0012\n",
      "grad_norm: 0.009375215508043766\n",
      "learning_rate: 2.267312164791874e-05\n",
      "epoch: 8.319806350562438\n",
      "\n",
      "\n",
      "Step: 233740\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0081259710714221\n",
      "learning_rate: 2.264939009919787e-05\n",
      "epoch: 8.320518297024064\n",
      "\n",
      "\n",
      "Step: 233760\n",
      "loss: 0.0014\n",
      "grad_norm: 0.006868068594485521\n",
      "learning_rate: 2.2625658550477e-05\n",
      "epoch: 8.32123024348569\n",
      "\n",
      "\n",
      "Step: 233780\n",
      "loss: 0.001\n",
      "grad_norm: 0.04218984767794609\n",
      "learning_rate: 2.260192700175613e-05\n",
      "epoch: 8.321942189947316\n",
      "\n",
      "\n",
      "Step: 233800\n",
      "loss: 0.0009\n",
      "grad_norm: 0.006634673569351435\n",
      "learning_rate: 2.2578195453035267e-05\n",
      "epoch: 8.322654136408943\n",
      "\n",
      "\n",
      "Step: 233820\n",
      "loss: 0.0012\n",
      "grad_norm: 0.028748249635100365\n",
      "learning_rate: 2.255446390431439e-05\n",
      "epoch: 8.323366082870567\n",
      "\n",
      "\n",
      "Step: 233840\n",
      "loss: 0.0008\n",
      "grad_norm: 0.020212331786751747\n",
      "learning_rate: 2.2530732355593527e-05\n",
      "epoch: 8.324078029332194\n",
      "\n",
      "\n",
      "Step: 233860\n",
      "loss: 0.0011\n",
      "grad_norm: 0.022506708279252052\n",
      "learning_rate: 2.2507000806872652e-05\n",
      "epoch: 8.32478997579382\n",
      "\n",
      "\n",
      "Step: 233880\n",
      "loss: 0.0015\n",
      "grad_norm: 0.007100007962435484\n",
      "learning_rate: 2.2483269258151787e-05\n",
      "epoch: 8.325501922255446\n",
      "\n",
      "\n",
      "Step: 233900\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04854492470622063\n",
      "learning_rate: 2.2459537709430912e-05\n",
      "epoch: 8.326213868717073\n",
      "\n",
      "\n",
      "Step: 233920\n",
      "loss: 0.0013\n",
      "grad_norm: 0.014864921569824219\n",
      "learning_rate: 2.2435806160710047e-05\n",
      "epoch: 8.326925815178699\n",
      "\n",
      "\n",
      "Step: 233940\n",
      "loss: 0.0011\n",
      "grad_norm: 0.050997041165828705\n",
      "learning_rate: 2.241207461198918e-05\n",
      "epoch: 8.327637761640325\n",
      "\n",
      "\n",
      "Step: 233960\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0028163748793303967\n",
      "learning_rate: 2.2388343063268307e-05\n",
      "epoch: 8.328349708101952\n",
      "\n",
      "\n",
      "Step: 233980\n",
      "loss: 0.0013\n",
      "grad_norm: 0.05072628706693649\n",
      "learning_rate: 2.236461151454744e-05\n",
      "epoch: 8.329061654563576\n",
      "\n",
      "\n",
      "Step: 234000\n",
      "loss: 0.0015\n",
      "grad_norm: 0.006919137202203274\n",
      "learning_rate: 2.2340879965826568e-05\n",
      "epoch: 8.329773601025202\n",
      "\n",
      "\n",
      "Step: 234020\n",
      "loss: 0.001\n",
      "grad_norm: 0.05095208063721657\n",
      "learning_rate: 2.23171484171057e-05\n",
      "epoch: 8.330485547486829\n",
      "\n",
      "\n",
      "Step: 234040\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01761734113097191\n",
      "learning_rate: 2.229341686838483e-05\n",
      "epoch: 8.331197493948455\n",
      "\n",
      "\n",
      "Step: 234060\n",
      "loss: 0.001\n",
      "grad_norm: 0.017680196091532707\n",
      "learning_rate: 2.226968531966396e-05\n",
      "epoch: 8.331909440410081\n",
      "\n",
      "\n",
      "Step: 234080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.026077615097165108\n",
      "learning_rate: 2.224595377094309e-05\n",
      "epoch: 8.332621386871708\n",
      "\n",
      "\n",
      "Step: 234100\n",
      "loss: 0.0013\n",
      "grad_norm: 0.013389783911406994\n",
      "learning_rate: 2.222222222222222e-05\n",
      "epoch: 8.333333333333334\n",
      "\n",
      "\n",
      "Step: 234120\n",
      "loss: 0.0015\n",
      "grad_norm: 0.024827515706419945\n",
      "learning_rate: 2.2198490673501352e-05\n",
      "epoch: 8.33404527979496\n",
      "\n",
      "\n",
      "Step: 234140\n",
      "loss: 0.0013\n",
      "grad_norm: 0.010079958476126194\n",
      "learning_rate: 2.217475912478048e-05\n",
      "epoch: 8.334757226256585\n",
      "\n",
      "\n",
      "Step: 234160\n",
      "loss: 0.001\n",
      "grad_norm: 0.07274274528026581\n",
      "learning_rate: 2.2151027576059612e-05\n",
      "epoch: 8.335469172718211\n",
      "\n",
      "\n",
      "Step: 234180\n",
      "loss: 0.0009\n",
      "grad_norm: 0.00357993645593524\n",
      "learning_rate: 2.2127296027338744e-05\n",
      "epoch: 8.336181119179837\n",
      "\n",
      "\n",
      "Step: 234200\n",
      "loss: 0.0008\n",
      "grad_norm: 0.019266899675130844\n",
      "learning_rate: 2.2103564478617872e-05\n",
      "epoch: 8.336893065641464\n",
      "\n",
      "\n",
      "Step: 234220\n",
      "loss: 0.0014\n",
      "grad_norm: 0.026660947129130363\n",
      "learning_rate: 2.2079832929897004e-05\n",
      "epoch: 8.33760501210309\n",
      "\n",
      "\n",
      "Step: 234240\n",
      "loss: 0.001\n",
      "grad_norm: 0.023796293884515762\n",
      "learning_rate: 2.2056101381176132e-05\n",
      "epoch: 8.338316958564716\n",
      "\n",
      "\n",
      "Step: 234260\n",
      "loss: 0.0011\n",
      "grad_norm: 0.030604925006628036\n",
      "learning_rate: 2.2032369832455264e-05\n",
      "epoch: 8.339028905026343\n",
      "\n",
      "\n",
      "Step: 234280\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02543601766228676\n",
      "learning_rate: 2.2008638283734396e-05\n",
      "epoch: 8.339740851487969\n",
      "\n",
      "\n",
      "Step: 234300\n",
      "loss: 0.0011\n",
      "grad_norm: 0.015017610974609852\n",
      "learning_rate: 2.1984906735013524e-05\n",
      "epoch: 8.340452797949593\n",
      "\n",
      "\n",
      "Step: 234320\n",
      "loss: 0.0017\n",
      "grad_norm: 0.02357352338731289\n",
      "learning_rate: 2.1961175186292656e-05\n",
      "epoch: 8.34116474441122\n",
      "\n",
      "\n",
      "Step: 234340\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01638142205774784\n",
      "learning_rate: 2.1937443637571785e-05\n",
      "epoch: 8.341876690872846\n",
      "\n",
      "\n",
      "Step: 234360\n",
      "loss: 0.0007\n",
      "grad_norm: 0.027069421485066414\n",
      "learning_rate: 2.1913712088850916e-05\n",
      "epoch: 8.342588637334472\n",
      "\n",
      "\n",
      "Step: 234380\n",
      "loss: 0.0009\n",
      "grad_norm: 0.017216285690665245\n",
      "learning_rate: 2.1889980540130048e-05\n",
      "epoch: 8.343300583796099\n",
      "\n",
      "\n",
      "Step: 234400\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02065446227788925\n",
      "learning_rate: 2.1866248991409177e-05\n",
      "epoch: 8.344012530257725\n",
      "\n",
      "\n",
      "Step: 234420\n",
      "loss: 0.0012\n",
      "grad_norm: 0.013991801999509335\n",
      "learning_rate: 2.184251744268831e-05\n",
      "epoch: 8.344724476719351\n",
      "\n",
      "\n",
      "Step: 234440\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0343073233962059\n",
      "learning_rate: 2.1818785893967437e-05\n",
      "epoch: 8.345436423180978\n",
      "\n",
      "\n",
      "Step: 234460\n",
      "loss: 0.0011\n",
      "grad_norm: 0.023107478395104408\n",
      "learning_rate: 2.179505434524657e-05\n",
      "epoch: 8.346148369642602\n",
      "\n",
      "\n",
      "Step: 234480\n",
      "loss: 0.0011\n",
      "grad_norm: 0.06365221738815308\n",
      "learning_rate: 2.1771322796525697e-05\n",
      "epoch: 8.346860316104229\n",
      "\n",
      "\n",
      "Step: 234500\n",
      "loss: 0.0015\n",
      "grad_norm: 0.04549882933497429\n",
      "learning_rate: 2.174759124780483e-05\n",
      "epoch: 8.347572262565855\n",
      "\n",
      "\n",
      "Step: 234520\n",
      "loss: 0.0012\n",
      "grad_norm: 0.023250313475728035\n",
      "learning_rate: 2.172385969908396e-05\n",
      "epoch: 8.348284209027481\n",
      "\n",
      "\n",
      "Step: 234540\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01074956450611353\n",
      "learning_rate: 2.170012815036309e-05\n",
      "epoch: 8.348996155489107\n",
      "\n",
      "\n",
      "Step: 234560\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008823519572615623\n",
      "learning_rate: 2.167639660164222e-05\n",
      "epoch: 8.349708101950734\n",
      "\n",
      "\n",
      "Step: 234580\n",
      "loss: 0.0009\n",
      "grad_norm: 0.006519224029034376\n",
      "learning_rate: 2.165266505292135e-05\n",
      "epoch: 8.35042004841236\n",
      "\n",
      "\n",
      "Step: 234600\n",
      "loss: 0.0015\n",
      "grad_norm: 0.012325537391006947\n",
      "learning_rate: 2.162893350420048e-05\n",
      "epoch: 8.351131994873985\n",
      "\n",
      "\n",
      "Step: 234620\n",
      "loss: 0.0015\n",
      "grad_norm: 0.007189920172095299\n",
      "learning_rate: 2.1605201955479616e-05\n",
      "epoch: 8.351843941335611\n",
      "\n",
      "\n",
      "Step: 234640\n",
      "loss: 0.001\n",
      "grad_norm: 0.023494547232985497\n",
      "learning_rate: 2.158147040675874e-05\n",
      "epoch: 8.352555887797237\n",
      "\n",
      "\n",
      "Step: 234660\n",
      "loss: 0.0016\n",
      "grad_norm: 0.01609608717262745\n",
      "learning_rate: 2.1557738858037876e-05\n",
      "epoch: 8.353267834258864\n",
      "\n",
      "\n",
      "Step: 234680\n",
      "loss: 0.0012\n",
      "grad_norm: 0.05030551925301552\n",
      "learning_rate: 2.1534007309317e-05\n",
      "epoch: 8.35397978072049\n",
      "\n",
      "\n",
      "Step: 234700\n",
      "loss: 0.0012\n",
      "grad_norm: 0.005127370357513428\n",
      "learning_rate: 2.1510275760596137e-05\n",
      "epoch: 8.354691727182116\n",
      "\n",
      "\n",
      "Step: 234720\n",
      "loss: 0.0011\n",
      "grad_norm: 0.002560132183134556\n",
      "learning_rate: 2.148654421187526e-05\n",
      "epoch: 8.355403673643742\n",
      "\n",
      "\n",
      "Step: 234740\n",
      "loss: 0.001\n",
      "grad_norm: 0.022679027169942856\n",
      "learning_rate: 2.1462812663154397e-05\n",
      "epoch: 8.356115620105369\n",
      "\n",
      "\n",
      "Step: 234760\n",
      "loss: 0.001\n",
      "grad_norm: 0.009320481680333614\n",
      "learning_rate: 2.143908111443353e-05\n",
      "epoch: 8.356827566566993\n",
      "\n",
      "\n",
      "Step: 234780\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03202257677912712\n",
      "learning_rate: 2.1415349565712657e-05\n",
      "epoch: 8.35753951302862\n",
      "\n",
      "\n",
      "Step: 234800\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02544129267334938\n",
      "learning_rate: 2.139161801699179e-05\n",
      "epoch: 8.358251459490246\n",
      "\n",
      "\n",
      "Step: 234820\n",
      "loss: 0.0011\n",
      "grad_norm: 0.025400429964065552\n",
      "learning_rate: 2.1367886468270917e-05\n",
      "epoch: 8.358963405951872\n",
      "\n",
      "\n",
      "Step: 234840\n",
      "loss: 0.0008\n",
      "grad_norm: 0.018191922456026077\n",
      "learning_rate: 2.134415491955005e-05\n",
      "epoch: 8.359675352413499\n",
      "\n",
      "\n",
      "Step: 234860\n",
      "loss: 0.0017\n",
      "grad_norm: 0.02644583024084568\n",
      "learning_rate: 2.132042337082918e-05\n",
      "epoch: 8.360387298875125\n",
      "\n",
      "\n",
      "Step: 234880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.021534865722060204\n",
      "learning_rate: 2.129669182210831e-05\n",
      "epoch: 8.361099245336751\n",
      "\n",
      "\n",
      "Step: 234900\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04902775213122368\n",
      "learning_rate: 2.127296027338744e-05\n",
      "epoch: 8.361811191798378\n",
      "\n",
      "\n",
      "Step: 234920\n",
      "loss: 0.0013\n",
      "grad_norm: 0.029410654678940773\n",
      "learning_rate: 2.124922872466657e-05\n",
      "epoch: 8.362523138260002\n",
      "\n",
      "\n",
      "Step: 234940\n",
      "loss: 0.0013\n",
      "grad_norm: 0.022057892754673958\n",
      "learning_rate: 2.12254971759457e-05\n",
      "epoch: 8.363235084721628\n",
      "\n",
      "\n",
      "Step: 234960\n",
      "loss: 0.0016\n",
      "grad_norm: 0.002956898184493184\n",
      "learning_rate: 2.1201765627224833e-05\n",
      "epoch: 8.363947031183255\n",
      "\n",
      "\n",
      "Step: 234980\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01316057424992323\n",
      "learning_rate: 2.117803407850396e-05\n",
      "epoch: 8.364658977644881\n",
      "\n",
      "\n",
      "Step: 235000\n",
      "loss: 0.0009\n",
      "grad_norm: 0.008194822818040848\n",
      "learning_rate: 2.1154302529783093e-05\n",
      "epoch: 8.365370924106507\n",
      "\n",
      "\n",
      "Step: 235020\n",
      "loss: 0.001\n",
      "grad_norm: 0.014376613311469555\n",
      "learning_rate: 2.113057098106222e-05\n",
      "epoch: 8.366082870568134\n",
      "\n",
      "\n",
      "Step: 235040\n",
      "loss: 0.0014\n",
      "grad_norm: 0.030930321663618088\n",
      "learning_rate: 2.1106839432341353e-05\n",
      "epoch: 8.36679481702976\n",
      "\n",
      "\n",
      "Step: 235060\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03221934661269188\n",
      "learning_rate: 2.1083107883620482e-05\n",
      "epoch: 8.367506763491386\n",
      "\n",
      "\n",
      "Step: 235080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.00028752879006788135\n",
      "learning_rate: 2.1059376334899614e-05\n",
      "epoch: 8.36821870995301\n",
      "\n",
      "\n",
      "Step: 235100\n",
      "loss: 0.0014\n",
      "grad_norm: 0.035810016095638275\n",
      "learning_rate: 2.1035644786178745e-05\n",
      "epoch: 8.368930656414637\n",
      "\n",
      "\n",
      "Step: 235120\n",
      "loss: 0.0015\n",
      "grad_norm: 0.04251975193619728\n",
      "learning_rate: 2.1011913237457874e-05\n",
      "epoch: 8.369642602876263\n",
      "\n",
      "\n",
      "Step: 235140\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02161504700779915\n",
      "learning_rate: 2.0988181688737006e-05\n",
      "epoch: 8.37035454933789\n",
      "\n",
      "\n",
      "Step: 235160\n",
      "loss: 0.001\n",
      "grad_norm: 0.02406216412782669\n",
      "learning_rate: 2.0964450140016134e-05\n",
      "epoch: 8.371066495799516\n",
      "\n",
      "\n",
      "Step: 235180\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02530112862586975\n",
      "learning_rate: 2.0940718591295266e-05\n",
      "epoch: 8.371778442261142\n",
      "\n",
      "\n",
      "Step: 235200\n",
      "loss: 0.001\n",
      "grad_norm: 0.03703976422548294\n",
      "learning_rate: 2.0916987042574398e-05\n",
      "epoch: 8.372490388722769\n",
      "\n",
      "\n",
      "Step: 235220\n",
      "loss: 0.0012\n",
      "grad_norm: 0.036939769983291626\n",
      "learning_rate: 2.0893255493853526e-05\n",
      "epoch: 8.373202335184395\n",
      "\n",
      "\n",
      "Step: 235240\n",
      "loss: 0.0011\n",
      "grad_norm: 0.002386611420661211\n",
      "learning_rate: 2.0869523945132658e-05\n",
      "epoch: 8.37391428164602\n",
      "\n",
      "\n",
      "Step: 235260\n",
      "loss: 0.0011\n",
      "grad_norm: 0.005688696168363094\n",
      "learning_rate: 2.0845792396411786e-05\n",
      "epoch: 8.374626228107646\n",
      "\n",
      "\n",
      "Step: 235280\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01636931672692299\n",
      "learning_rate: 2.0822060847690918e-05\n",
      "epoch: 8.375338174569272\n",
      "\n",
      "\n",
      "Step: 235300\n",
      "loss: 0.0021\n",
      "grad_norm: 0.015975190326571465\n",
      "learning_rate: 2.0798329298970046e-05\n",
      "epoch: 8.376050121030898\n",
      "\n",
      "\n",
      "Step: 235320\n",
      "loss: 0.002\n",
      "grad_norm: 0.05016080662608147\n",
      "learning_rate: 2.0774597750249178e-05\n",
      "epoch: 8.376762067492525\n",
      "\n",
      "\n",
      "Step: 235340\n",
      "loss: 0.0015\n",
      "grad_norm: 0.018806956708431244\n",
      "learning_rate: 2.075086620152831e-05\n",
      "epoch: 8.377474013954151\n",
      "\n",
      "\n",
      "Step: 235360\n",
      "loss: 0.001\n",
      "grad_norm: 0.022646814584732056\n",
      "learning_rate: 2.072713465280744e-05\n",
      "epoch: 8.378185960415777\n",
      "\n",
      "\n",
      "Step: 235380\n",
      "loss: 0.0015\n",
      "grad_norm: 0.04485509544610977\n",
      "learning_rate: 2.070340310408657e-05\n",
      "epoch: 8.378897906877404\n",
      "\n",
      "\n",
      "Step: 235400\n",
      "loss: 0.0014\n",
      "grad_norm: 0.018837690353393555\n",
      "learning_rate: 2.06796715553657e-05\n",
      "epoch: 8.379609853339028\n",
      "\n",
      "\n",
      "Step: 235420\n",
      "loss: 0.0011\n",
      "grad_norm: 0.020053710788488388\n",
      "learning_rate: 2.065594000664483e-05\n",
      "epoch: 8.380321799800655\n",
      "\n",
      "\n",
      "Step: 235440\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02567972242832184\n",
      "learning_rate: 2.0632208457923966e-05\n",
      "epoch: 8.38103374626228\n",
      "\n",
      "\n",
      "Step: 235460\n",
      "loss: 0.001\n",
      "grad_norm: 0.02323627844452858\n",
      "learning_rate: 2.060847690920309e-05\n",
      "epoch: 8.381745692723907\n",
      "\n",
      "\n",
      "Step: 235480\n",
      "loss: 0.0011\n",
      "grad_norm: 0.013707849197089672\n",
      "learning_rate: 2.0584745360482226e-05\n",
      "epoch: 8.382457639185533\n",
      "\n",
      "\n",
      "Step: 235500\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0281937588006258\n",
      "learning_rate: 2.056101381176135e-05\n",
      "epoch: 8.38316958564716\n",
      "\n",
      "\n",
      "Step: 235520\n",
      "loss: 0.0008\n",
      "grad_norm: 0.05385357141494751\n",
      "learning_rate: 2.0537282263040486e-05\n",
      "epoch: 8.383881532108786\n",
      "\n",
      "\n",
      "Step: 235540\n",
      "loss: 0.0011\n",
      "grad_norm: 0.00394026143476367\n",
      "learning_rate: 2.0513550714319618e-05\n",
      "epoch: 8.384593478570412\n",
      "\n",
      "\n",
      "Step: 235560\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02694586291909218\n",
      "learning_rate: 2.0489819165598746e-05\n",
      "epoch: 8.385305425032037\n",
      "\n",
      "\n",
      "Step: 235580\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0029481693636626005\n",
      "learning_rate: 2.0466087616877878e-05\n",
      "epoch: 8.386017371493663\n",
      "\n",
      "\n",
      "Step: 235600\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0030857548117637634\n",
      "learning_rate: 2.0442356068157006e-05\n",
      "epoch: 8.38672931795529\n",
      "\n",
      "\n",
      "Step: 235620\n",
      "loss: 0.001\n",
      "grad_norm: 0.01690659113228321\n",
      "learning_rate: 2.0418624519436138e-05\n",
      "epoch: 8.387441264416916\n",
      "\n",
      "\n",
      "Step: 235640\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0018601948395371437\n",
      "learning_rate: 2.0394892970715267e-05\n",
      "epoch: 8.388153210878542\n",
      "\n",
      "\n",
      "Step: 235660\n",
      "loss: 0.001\n",
      "grad_norm: 0.01750768907368183\n",
      "learning_rate: 2.03711614219944e-05\n",
      "epoch: 8.388865157340168\n",
      "\n",
      "\n",
      "Step: 235680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.020826948806643486\n",
      "learning_rate: 2.034742987327353e-05\n",
      "epoch: 8.389577103801795\n",
      "\n",
      "\n",
      "Step: 235700\n",
      "loss: 0.001\n",
      "grad_norm: 0.02081230655312538\n",
      "learning_rate: 2.032369832455266e-05\n",
      "epoch: 8.39028905026342\n",
      "\n",
      "\n",
      "Step: 235720\n",
      "loss: 0.0014\n",
      "grad_norm: 0.019430359825491905\n",
      "learning_rate: 2.029996677583179e-05\n",
      "epoch: 8.391000996725046\n",
      "\n",
      "\n",
      "Step: 235740\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012126225046813488\n",
      "learning_rate: 2.027623522711092e-05\n",
      "epoch: 8.391712943186672\n",
      "\n",
      "\n",
      "Step: 235760\n",
      "loss: 0.0015\n",
      "grad_norm: 0.01227541919797659\n",
      "learning_rate: 2.025250367839005e-05\n",
      "epoch: 8.392424889648298\n",
      "\n",
      "\n",
      "Step: 235780\n",
      "loss: 0.0012\n",
      "grad_norm: 0.027041826397180557\n",
      "learning_rate: 2.0228772129669182e-05\n",
      "epoch: 8.393136836109925\n",
      "\n",
      "\n",
      "Step: 235800\n",
      "loss: 0.0012\n",
      "grad_norm: 0.057749003171920776\n",
      "learning_rate: 2.020504058094831e-05\n",
      "epoch: 8.39384878257155\n",
      "\n",
      "\n",
      "Step: 235820\n",
      "loss: 0.001\n",
      "grad_norm: 0.010165623389184475\n",
      "learning_rate: 2.0181309032227443e-05\n",
      "epoch: 8.394560729033177\n",
      "\n",
      "\n",
      "Step: 235840\n",
      "loss: 0.0015\n",
      "grad_norm: 0.023843882605433464\n",
      "learning_rate: 2.015757748350657e-05\n",
      "epoch: 8.395272675494803\n",
      "\n",
      "\n",
      "Step: 235860\n",
      "loss: 0.001\n",
      "grad_norm: 0.0030930261127650738\n",
      "learning_rate: 2.0133845934785703e-05\n",
      "epoch: 8.395984621956428\n",
      "\n",
      "\n",
      "Step: 235880\n",
      "loss: 0.001\n",
      "grad_norm: 0.03170505538582802\n",
      "learning_rate: 2.011011438606483e-05\n",
      "epoch: 8.396696568418054\n",
      "\n",
      "\n",
      "Step: 235900\n",
      "loss: 0.0012\n",
      "grad_norm: 0.009717118926346302\n",
      "learning_rate: 2.0086382837343963e-05\n",
      "epoch: 8.39740851487968\n",
      "\n",
      "\n",
      "Step: 235920\n",
      "loss: 0.0017\n",
      "grad_norm: 0.0024841369595378637\n",
      "learning_rate: 2.0062651288623095e-05\n",
      "epoch: 8.398120461341307\n",
      "\n",
      "\n",
      "Step: 235940\n",
      "loss: 0.0008\n",
      "grad_norm: 0.002642565406858921\n",
      "learning_rate: 2.0038919739902223e-05\n",
      "epoch: 8.398832407802933\n",
      "\n",
      "\n",
      "Step: 235960\n",
      "loss: 0.001\n",
      "grad_norm: 0.02588171884417534\n",
      "learning_rate: 2.0015188191181355e-05\n",
      "epoch: 8.39954435426456\n",
      "\n",
      "\n",
      "Step: 235980\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0011078240349888802\n",
      "learning_rate: 1.9991456642460484e-05\n",
      "epoch: 8.400256300726186\n",
      "\n",
      "\n",
      "Step: 236000\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0024317228235304356\n",
      "learning_rate: 1.9967725093739615e-05\n",
      "epoch: 8.400968247187812\n",
      "\n",
      "\n",
      "Step: 236020\n",
      "loss: 0.0009\n",
      "grad_norm: 0.008283056318759918\n",
      "learning_rate: 1.9943993545018747e-05\n",
      "epoch: 8.401680193649437\n",
      "\n",
      "\n",
      "Step: 236040\n",
      "loss: 0.0013\n",
      "grad_norm: 0.005736593622714281\n",
      "learning_rate: 1.9920261996297876e-05\n",
      "epoch: 8.402392140111063\n",
      "\n",
      "\n",
      "Step: 236060\n",
      "loss: 0.0017\n",
      "grad_norm: 0.011721564456820488\n",
      "learning_rate: 1.9896530447577007e-05\n",
      "epoch: 8.40310408657269\n",
      "\n",
      "\n",
      "Step: 236080\n",
      "loss: 0.0013\n",
      "grad_norm: 0.06346093863248825\n",
      "learning_rate: 1.9872798898856136e-05\n",
      "epoch: 8.403816033034316\n",
      "\n",
      "\n",
      "Step: 236100\n",
      "loss: 0.0012\n",
      "grad_norm: 0.014445512555539608\n",
      "learning_rate: 1.9849067350135268e-05\n",
      "epoch: 8.404527979495942\n",
      "\n",
      "\n",
      "Step: 236120\n",
      "loss: 0.0012\n",
      "grad_norm: 0.027381636202335358\n",
      "learning_rate: 1.98253358014144e-05\n",
      "epoch: 8.405239925957568\n",
      "\n",
      "\n",
      "Step: 236140\n",
      "loss: 0.0011\n",
      "grad_norm: 0.005381105933338404\n",
      "learning_rate: 1.9801604252693528e-05\n",
      "epoch: 8.405951872419195\n",
      "\n",
      "\n",
      "Step: 236160\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0058785355649888515\n",
      "learning_rate: 1.977787270397266e-05\n",
      "epoch: 8.406663818880821\n",
      "\n",
      "\n",
      "Step: 236180\n",
      "loss: 0.0018\n",
      "grad_norm: 0.04922078177332878\n",
      "learning_rate: 1.9754141155251788e-05\n",
      "epoch: 8.407375765342445\n",
      "\n",
      "\n",
      "Step: 236200\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0034570314455777407\n",
      "learning_rate: 1.973040960653092e-05\n",
      "epoch: 8.408087711804072\n",
      "\n",
      "\n",
      "Step: 236220\n",
      "loss: 0.0015\n",
      "grad_norm: 0.033931147307157516\n",
      "learning_rate: 1.9706678057810048e-05\n",
      "epoch: 8.408799658265698\n",
      "\n",
      "\n",
      "Step: 236240\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03166572377085686\n",
      "learning_rate: 1.968294650908918e-05\n",
      "epoch: 8.409511604727324\n",
      "\n",
      "\n",
      "Step: 236260\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02158726006746292\n",
      "learning_rate: 1.9659214960368315e-05\n",
      "epoch: 8.41022355118895\n",
      "\n",
      "\n",
      "Step: 236280\n",
      "loss: 0.001\n",
      "grad_norm: 0.020085109397768974\n",
      "learning_rate: 1.963548341164744e-05\n",
      "epoch: 8.410935497650577\n",
      "\n",
      "\n",
      "Step: 236300\n",
      "loss: 0.0014\n",
      "grad_norm: 0.025130081921815872\n",
      "learning_rate: 1.9611751862926575e-05\n",
      "epoch: 8.411647444112203\n",
      "\n",
      "\n",
      "Step: 236320\n",
      "loss: 0.0013\n",
      "grad_norm: 0.005776101257652044\n",
      "learning_rate: 1.95880203142057e-05\n",
      "epoch: 8.41235939057383\n",
      "\n",
      "\n",
      "Step: 236340\n",
      "loss: 0.0009\n",
      "grad_norm: 0.024060580879449844\n",
      "learning_rate: 1.9564288765484836e-05\n",
      "epoch: 8.413071337035454\n",
      "\n",
      "\n",
      "Step: 236360\n",
      "loss: 0.0007\n",
      "grad_norm: 0.006038876716047525\n",
      "learning_rate: 1.9540557216763967e-05\n",
      "epoch: 8.41378328349708\n",
      "\n",
      "\n",
      "Step: 236380\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0024367826990783215\n",
      "learning_rate: 1.9516825668043096e-05\n",
      "epoch: 8.414495229958707\n",
      "\n",
      "\n",
      "Step: 236400\n",
      "loss: 0.0013\n",
      "grad_norm: 0.021261047571897507\n",
      "learning_rate: 1.9493094119322228e-05\n",
      "epoch: 8.415207176420333\n",
      "\n",
      "\n",
      "Step: 236420\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02271130308508873\n",
      "learning_rate: 1.9469362570601356e-05\n",
      "epoch: 8.41591912288196\n",
      "\n",
      "\n",
      "Step: 236440\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03304703161120415\n",
      "learning_rate: 1.9445631021880488e-05\n",
      "epoch: 8.416631069343586\n",
      "\n",
      "\n",
      "Step: 236460\n",
      "loss: 0.0009\n",
      "grad_norm: 0.022523202002048492\n",
      "learning_rate: 1.9421899473159616e-05\n",
      "epoch: 8.417343015805212\n",
      "\n",
      "\n",
      "Step: 236480\n",
      "loss: 0.0013\n",
      "grad_norm: 0.015743017196655273\n",
      "learning_rate: 1.9398167924438748e-05\n",
      "epoch: 8.418054962266838\n",
      "\n",
      "\n",
      "Step: 236500\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03305944800376892\n",
      "learning_rate: 1.937443637571788e-05\n",
      "epoch: 8.418766908728463\n",
      "\n",
      "\n",
      "Step: 236520\n",
      "loss: 0.0015\n",
      "grad_norm: 0.015812348574399948\n",
      "learning_rate: 1.9350704826997008e-05\n",
      "epoch: 8.41947885519009\n",
      "\n",
      "\n",
      "Step: 236540\n",
      "loss: 0.001\n",
      "grad_norm: 0.056400660425424576\n",
      "learning_rate: 1.932697327827614e-05\n",
      "epoch: 8.420190801651716\n",
      "\n",
      "\n",
      "Step: 236560\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0008557601249776781\n",
      "learning_rate: 1.930324172955527e-05\n",
      "epoch: 8.420902748113342\n",
      "\n",
      "\n",
      "Step: 236580\n",
      "loss: 0.001\n",
      "grad_norm: 0.006746802479028702\n",
      "learning_rate: 1.92795101808344e-05\n",
      "epoch: 8.421614694574968\n",
      "\n",
      "\n",
      "Step: 236600\n",
      "loss: 0.0013\n",
      "grad_norm: 0.008569955825805664\n",
      "learning_rate: 1.9255778632113532e-05\n",
      "epoch: 8.422326641036594\n",
      "\n",
      "\n",
      "Step: 236620\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0064454758539795876\n",
      "learning_rate: 1.923204708339266e-05\n",
      "epoch: 8.42303858749822\n",
      "\n",
      "\n",
      "Step: 236640\n",
      "loss: 0.0009\n",
      "grad_norm: 0.021430429071187973\n",
      "learning_rate: 1.9208315534671792e-05\n",
      "epoch: 8.423750533959847\n",
      "\n",
      "\n",
      "Step: 236660\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011696059256792068\n",
      "learning_rate: 1.918458398595092e-05\n",
      "epoch: 8.424462480421472\n",
      "\n",
      "\n",
      "Step: 236680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.009117289446294308\n",
      "learning_rate: 1.9160852437230052e-05\n",
      "epoch: 8.425174426883098\n",
      "\n",
      "\n",
      "Step: 236700\n",
      "loss: 0.0019\n",
      "grad_norm: 0.03893081098794937\n",
      "learning_rate: 1.9137120888509184e-05\n",
      "epoch: 8.425886373344724\n",
      "\n",
      "\n",
      "Step: 236720\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04773877188563347\n",
      "learning_rate: 1.9113389339788313e-05\n",
      "epoch: 8.42659831980635\n",
      "\n",
      "\n",
      "Step: 236740\n",
      "loss: 0.0018\n",
      "grad_norm: 0.036018941551446915\n",
      "learning_rate: 1.9089657791067444e-05\n",
      "epoch: 8.427310266267977\n",
      "\n",
      "\n",
      "Step: 236760\n",
      "loss: 0.0011\n",
      "grad_norm: 0.011446318589150906\n",
      "learning_rate: 1.9065926242346573e-05\n",
      "epoch: 8.428022212729603\n",
      "\n",
      "\n",
      "Step: 236780\n",
      "loss: 0.0015\n",
      "grad_norm: 0.018093407154083252\n",
      "learning_rate: 1.9042194693625705e-05\n",
      "epoch: 8.42873415919123\n",
      "\n",
      "\n",
      "Step: 236800\n",
      "loss: 0.0014\n",
      "grad_norm: 0.024097828194499016\n",
      "learning_rate: 1.9018463144904833e-05\n",
      "epoch: 8.429446105652854\n",
      "\n",
      "\n",
      "Step: 236820\n",
      "loss: 0.0013\n",
      "grad_norm: 0.019582215696573257\n",
      "learning_rate: 1.8994731596183965e-05\n",
      "epoch: 8.43015805211448\n",
      "\n",
      "\n",
      "Step: 236840\n",
      "loss: 0.0007\n",
      "grad_norm: 0.01221385970711708\n",
      "learning_rate: 1.8971000047463097e-05\n",
      "epoch: 8.430869998576107\n",
      "\n",
      "\n",
      "Step: 236860\n",
      "loss: 0.0011\n",
      "grad_norm: 0.026214350014925003\n",
      "learning_rate: 1.8947268498742225e-05\n",
      "epoch: 8.431581945037733\n",
      "\n",
      "\n",
      "Step: 236880\n",
      "loss: 0.0008\n",
      "grad_norm: 0.038496412336826324\n",
      "learning_rate: 1.8923536950021357e-05\n",
      "epoch: 8.43229389149936\n",
      "\n",
      "\n",
      "Step: 236900\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009267253801226616\n",
      "learning_rate: 1.8899805401300485e-05\n",
      "epoch: 8.433005837960986\n",
      "\n",
      "\n",
      "Step: 236920\n",
      "loss: 0.0011\n",
      "grad_norm: 0.017751282081007957\n",
      "learning_rate: 1.8876073852579617e-05\n",
      "epoch: 8.433717784422612\n",
      "\n",
      "\n",
      "Step: 236940\n",
      "loss: 0.0014\n",
      "grad_norm: 0.024440376088023186\n",
      "learning_rate: 1.885234230385875e-05\n",
      "epoch: 8.434429730884238\n",
      "\n",
      "\n",
      "Step: 236960\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03857164457440376\n",
      "learning_rate: 1.8828610755137877e-05\n",
      "epoch: 8.435141677345863\n",
      "\n",
      "\n",
      "Step: 236980\n",
      "loss: 0.0017\n",
      "grad_norm: 0.04695012792944908\n",
      "learning_rate: 1.880487920641701e-05\n",
      "epoch: 8.435853623807489\n",
      "\n",
      "\n",
      "Step: 237000\n",
      "loss: 0.001\n",
      "grad_norm: 0.038723211735486984\n",
      "learning_rate: 1.8781147657696137e-05\n",
      "epoch: 8.436565570269115\n",
      "\n",
      "\n",
      "Step: 237020\n",
      "loss: 0.0013\n",
      "grad_norm: 0.028736663982272148\n",
      "learning_rate: 1.875741610897527e-05\n",
      "epoch: 8.437277516730742\n",
      "\n",
      "\n",
      "Step: 237040\n",
      "loss: 0.0015\n",
      "grad_norm: 0.04430381953716278\n",
      "learning_rate: 1.87336845602544e-05\n",
      "epoch: 8.437989463192368\n",
      "\n",
      "\n",
      "Step: 237060\n",
      "loss: 0.0015\n",
      "grad_norm: 0.023452069610357285\n",
      "learning_rate: 1.870995301153353e-05\n",
      "epoch: 8.438701409653994\n",
      "\n",
      "\n",
      "Step: 237080\n",
      "loss: 0.001\n",
      "grad_norm: 0.007165215909481049\n",
      "learning_rate: 1.868622146281266e-05\n",
      "epoch: 8.43941335611562\n",
      "\n",
      "\n",
      "Step: 237100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.1101200133562088\n",
      "learning_rate: 1.866248991409179e-05\n",
      "epoch: 8.440125302577247\n",
      "\n",
      "\n",
      "Step: 237120\n",
      "loss: 0.0007\n",
      "grad_norm: 0.02404613420367241\n",
      "learning_rate: 1.8638758365370925e-05\n",
      "epoch: 8.440837249038871\n",
      "\n",
      "\n",
      "Step: 237140\n",
      "loss: 0.0007\n",
      "grad_norm: 0.012934896163642406\n",
      "learning_rate: 1.8615026816650053e-05\n",
      "epoch: 8.441549195500498\n",
      "\n",
      "\n",
      "Step: 237160\n",
      "loss: 0.0012\n",
      "grad_norm: 0.006644363049417734\n",
      "learning_rate: 1.8591295267929185e-05\n",
      "epoch: 8.442261141962124\n",
      "\n",
      "\n",
      "Step: 237180\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02050011418759823\n",
      "learning_rate: 1.8567563719208313e-05\n",
      "epoch: 8.44297308842375\n",
      "\n",
      "\n",
      "Step: 237200\n",
      "loss: 0.0016\n",
      "grad_norm: 0.028123287484049797\n",
      "learning_rate: 1.8543832170487445e-05\n",
      "epoch: 8.443685034885377\n",
      "\n",
      "\n",
      "Step: 237220\n",
      "loss: 0.0026\n",
      "grad_norm: 0.0037014735862612724\n",
      "learning_rate: 1.8520100621766574e-05\n",
      "epoch: 8.444396981347003\n",
      "\n",
      "\n",
      "Step: 237240\n",
      "loss: 0.0014\n",
      "grad_norm: 0.05591703578829765\n",
      "learning_rate: 1.8496369073045705e-05\n",
      "epoch: 8.44510892780863\n",
      "\n",
      "\n",
      "Step: 237260\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0042821080423891544\n",
      "learning_rate: 1.8472637524324837e-05\n",
      "epoch: 8.445820874270256\n",
      "\n",
      "\n",
      "Step: 237280\n",
      "loss: 0.0016\n",
      "grad_norm: 0.059714045375585556\n",
      "learning_rate: 1.8448905975603966e-05\n",
      "epoch: 8.44653282073188\n",
      "\n",
      "\n",
      "Step: 237300\n",
      "loss: 0.0015\n",
      "grad_norm: 0.029543066397309303\n",
      "learning_rate: 1.8425174426883097e-05\n",
      "epoch: 8.447244767193506\n",
      "\n",
      "\n",
      "Step: 237320\n",
      "loss: 0.0011\n",
      "grad_norm: 0.029351020231842995\n",
      "learning_rate: 1.8401442878162226e-05\n",
      "epoch: 8.447956713655133\n",
      "\n",
      "\n",
      "Step: 237340\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0024342683609575033\n",
      "learning_rate: 1.8377711329441358e-05\n",
      "epoch: 8.448668660116759\n",
      "\n",
      "\n",
      "Step: 237360\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03139862045645714\n",
      "learning_rate: 1.835397978072049e-05\n",
      "epoch: 8.449380606578385\n",
      "\n",
      "\n",
      "Step: 237380\n",
      "loss: 0.0016\n",
      "grad_norm: 0.03486016392707825\n",
      "learning_rate: 1.8330248231999618e-05\n",
      "epoch: 8.450092553040012\n",
      "\n",
      "\n",
      "Step: 237400\n",
      "loss: 0.0014\n",
      "grad_norm: 0.032097794115543365\n",
      "learning_rate: 1.830651668327875e-05\n",
      "epoch: 8.450804499501638\n",
      "\n",
      "\n",
      "Step: 237420\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02406427450478077\n",
      "learning_rate: 1.8282785134557878e-05\n",
      "epoch: 8.451516445963264\n",
      "\n",
      "\n",
      "Step: 237440\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0030972843524068594\n",
      "learning_rate: 1.825905358583701e-05\n",
      "epoch: 8.452228392424889\n",
      "\n",
      "\n",
      "Step: 237460\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01655711606144905\n",
      "learning_rate: 1.823532203711614e-05\n",
      "epoch: 8.452940338886515\n",
      "\n",
      "\n",
      "Step: 237480\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0042058490216732025\n",
      "learning_rate: 1.821159048839527e-05\n",
      "epoch: 8.453652285348142\n",
      "\n",
      "\n",
      "Step: 237500\n",
      "loss: 0.001\n",
      "grad_norm: 0.03437764570116997\n",
      "learning_rate: 1.8187858939674402e-05\n",
      "epoch: 8.454364231809768\n",
      "\n",
      "\n",
      "Step: 237520\n",
      "loss: 0.0007\n",
      "grad_norm: 0.016780098900198936\n",
      "learning_rate: 1.816412739095353e-05\n",
      "epoch: 8.455076178271394\n",
      "\n",
      "\n",
      "Step: 237540\n",
      "loss: 0.0017\n",
      "grad_norm: 0.04063871130347252\n",
      "learning_rate: 1.8140395842232662e-05\n",
      "epoch: 8.45578812473302\n",
      "\n",
      "\n",
      "Step: 237560\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0020474374759942293\n",
      "learning_rate: 1.811666429351179e-05\n",
      "epoch: 8.456500071194647\n",
      "\n",
      "\n",
      "Step: 237580\n",
      "loss: 0.0012\n",
      "grad_norm: 0.027958642691373825\n",
      "learning_rate: 1.8092932744790926e-05\n",
      "epoch: 8.457212017656273\n",
      "\n",
      "\n",
      "Step: 237600\n",
      "loss: 0.0016\n",
      "grad_norm: 0.026044176891446114\n",
      "learning_rate: 1.8069201196070054e-05\n",
      "epoch: 8.457923964117898\n",
      "\n",
      "\n",
      "Step: 237620\n",
      "loss: 0.0014\n",
      "grad_norm: 0.04562242329120636\n",
      "learning_rate: 1.8045469647349186e-05\n",
      "epoch: 8.458635910579524\n",
      "\n",
      "\n",
      "Step: 237640\n",
      "loss: 0.0014\n",
      "grad_norm: 0.003504533553496003\n",
      "learning_rate: 1.8021738098628314e-05\n",
      "epoch: 8.45934785704115\n",
      "\n",
      "\n",
      "Step: 237660\n",
      "loss: 0.0007\n",
      "grad_norm: 0.020393533632159233\n",
      "learning_rate: 1.7998006549907446e-05\n",
      "epoch: 8.460059803502777\n",
      "\n",
      "\n",
      "Step: 237680\n",
      "loss: 0.0016\n",
      "grad_norm: 0.02630615234375\n",
      "learning_rate: 1.7974275001186575e-05\n",
      "epoch: 8.460771749964403\n",
      "\n",
      "\n",
      "Step: 237700\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00866027269512415\n",
      "learning_rate: 1.7950543452465706e-05\n",
      "epoch: 8.46148369642603\n",
      "\n",
      "\n",
      "Step: 237720\n",
      "loss: 0.0015\n",
      "grad_norm: 0.03381611406803131\n",
      "learning_rate: 1.7926811903744838e-05\n",
      "epoch: 8.462195642887655\n",
      "\n",
      "\n",
      "Step: 237740\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004345573019236326\n",
      "learning_rate: 1.7903080355023967e-05\n",
      "epoch: 8.462907589349282\n",
      "\n",
      "\n",
      "Step: 237760\n",
      "loss: 0.0008\n",
      "grad_norm: 0.015444339253008366\n",
      "learning_rate: 1.78793488063031e-05\n",
      "epoch: 8.463619535810906\n",
      "\n",
      "\n",
      "Step: 237780\n",
      "loss: 0.0013\n",
      "grad_norm: 0.015612385235726833\n",
      "learning_rate: 1.7855617257582227e-05\n",
      "epoch: 8.464331482272533\n",
      "\n",
      "\n",
      "Step: 237800\n",
      "loss: 0.001\n",
      "grad_norm: 0.022621000185608864\n",
      "learning_rate: 1.783188570886136e-05\n",
      "epoch: 8.465043428734159\n",
      "\n",
      "\n",
      "Step: 237820\n",
      "loss: 0.0015\n",
      "grad_norm: 0.030784524977207184\n",
      "learning_rate: 1.780815416014049e-05\n",
      "epoch: 8.465755375195785\n",
      "\n",
      "\n",
      "Step: 237840\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02613125927746296\n",
      "learning_rate: 1.778442261141962e-05\n",
      "epoch: 8.466467321657412\n",
      "\n",
      "\n",
      "Step: 237860\n",
      "loss: 0.0008\n",
      "grad_norm: 0.020338881760835648\n",
      "learning_rate: 1.776069106269875e-05\n",
      "epoch: 8.467179268119038\n",
      "\n",
      "\n",
      "Step: 237880\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0022445351351052523\n",
      "learning_rate: 1.773695951397788e-05\n",
      "epoch: 8.467891214580664\n",
      "\n",
      "\n",
      "Step: 237900\n",
      "loss: 0.001\n",
      "grad_norm: 0.01787412352859974\n",
      "learning_rate: 1.771322796525701e-05\n",
      "epoch: 8.468603161042289\n",
      "\n",
      "\n",
      "Step: 237920\n",
      "loss: 0.0007\n",
      "grad_norm: 0.010219882242381573\n",
      "learning_rate: 1.7689496416536143e-05\n",
      "epoch: 8.469315107503915\n",
      "\n",
      "\n",
      "Step: 237940\n",
      "loss: 0.0012\n",
      "grad_norm: 0.013561160303652287\n",
      "learning_rate: 1.7665764867815274e-05\n",
      "epoch: 8.470027053965541\n",
      "\n",
      "\n",
      "Step: 237960\n",
      "loss: 0.0008\n",
      "grad_norm: 0.005889114458113909\n",
      "learning_rate: 1.7642033319094403e-05\n",
      "epoch: 8.470739000427168\n",
      "\n",
      "\n",
      "Step: 237980\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03187764063477516\n",
      "learning_rate: 1.7618301770373535e-05\n",
      "epoch: 8.471450946888794\n",
      "\n",
      "\n",
      "Step: 238000\n",
      "loss: 0.0011\n",
      "grad_norm: 0.014486950822174549\n",
      "learning_rate: 1.7594570221652663e-05\n",
      "epoch: 8.47216289335042\n",
      "\n",
      "\n",
      "Step: 238020\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01292297150939703\n",
      "learning_rate: 1.7570838672931795e-05\n",
      "epoch: 8.472874839812047\n",
      "\n",
      "\n",
      "Step: 238040\n",
      "loss: 0.0011\n",
      "grad_norm: 0.025389190763235092\n",
      "learning_rate: 1.7547107124210927e-05\n",
      "epoch: 8.473586786273673\n",
      "\n",
      "\n",
      "Step: 238060\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03339696675539017\n",
      "learning_rate: 1.7523375575490055e-05\n",
      "epoch: 8.4742987327353\n",
      "\n",
      "\n",
      "Step: 238080\n",
      "loss: 0.0008\n",
      "grad_norm: 0.019351841881871223\n",
      "learning_rate: 1.7499644026769187e-05\n",
      "epoch: 8.475010679196924\n",
      "\n",
      "\n",
      "Step: 238100\n",
      "loss: 0.0012\n",
      "grad_norm: 0.006012365687638521\n",
      "learning_rate: 1.7475912478048315e-05\n",
      "epoch: 8.47572262565855\n",
      "\n",
      "\n",
      "Step: 238120\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0421966016292572\n",
      "learning_rate: 1.7452180929327447e-05\n",
      "epoch: 8.476434572120176\n",
      "\n",
      "\n",
      "Step: 238140\n",
      "loss: 0.0007\n",
      "grad_norm: 0.004440908785909414\n",
      "learning_rate: 1.7428449380606575e-05\n",
      "epoch: 8.477146518581803\n",
      "\n",
      "\n",
      "Step: 238160\n",
      "loss: 0.0013\n",
      "grad_norm: 0.03778191655874252\n",
      "learning_rate: 1.7404717831885707e-05\n",
      "epoch: 8.477858465043429\n",
      "\n",
      "\n",
      "Step: 238180\n",
      "loss: 0.0013\n",
      "grad_norm: 0.039323147386312485\n",
      "learning_rate: 1.738098628316484e-05\n",
      "epoch: 8.478570411505055\n",
      "\n",
      "\n",
      "Step: 238200\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011929204687476158\n",
      "learning_rate: 1.7357254734443967e-05\n",
      "epoch: 8.479282357966682\n",
      "\n",
      "\n",
      "Step: 238220\n",
      "loss: 0.001\n",
      "grad_norm: 0.013139130547642708\n",
      "learning_rate: 1.73335231857231e-05\n",
      "epoch: 8.479994304428306\n",
      "\n",
      "\n",
      "Step: 238240\n",
      "loss: 0.0008\n",
      "grad_norm: 0.026453647762537003\n",
      "learning_rate: 1.7309791637002228e-05\n",
      "epoch: 8.480706250889932\n",
      "\n",
      "\n",
      "Step: 238260\n",
      "loss: 0.0014\n",
      "grad_norm: 0.015223119407892227\n",
      "learning_rate: 1.728606008828136e-05\n",
      "epoch: 8.481418197351559\n",
      "\n",
      "\n",
      "Step: 238280\n",
      "loss: 0.0016\n",
      "grad_norm: 0.06716877222061157\n",
      "learning_rate: 1.726232853956049e-05\n",
      "epoch: 8.482130143813185\n",
      "\n",
      "\n",
      "Step: 238300\n",
      "loss: 0.0013\n",
      "grad_norm: 0.022829731926321983\n",
      "learning_rate: 1.723859699083962e-05\n",
      "epoch: 8.482842090274811\n",
      "\n",
      "\n",
      "Step: 238320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.011697842739522457\n",
      "learning_rate: 1.721486544211875e-05\n",
      "epoch: 8.483554036736438\n",
      "\n",
      "\n",
      "Step: 238340\n",
      "loss: 0.0012\n",
      "grad_norm: 0.031982727348804474\n",
      "learning_rate: 1.719113389339788e-05\n",
      "epoch: 8.484265983198064\n",
      "\n",
      "\n",
      "Step: 238360\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0015130164101719856\n",
      "learning_rate: 1.716740234467701e-05\n",
      "epoch: 8.48497792965969\n",
      "\n",
      "\n",
      "Step: 238380\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04796022176742554\n",
      "learning_rate: 1.714367079595614e-05\n",
      "epoch: 8.485689876121315\n",
      "\n",
      "\n",
      "Step: 238400\n",
      "loss: 0.0008\n",
      "grad_norm: 0.008008590899407864\n",
      "learning_rate: 1.7119939247235275e-05\n",
      "epoch: 8.486401822582941\n",
      "\n",
      "\n",
      "Step: 238420\n",
      "loss: 0.0009\n",
      "grad_norm: 0.030372608453035355\n",
      "learning_rate: 1.7096207698514404e-05\n",
      "epoch: 8.487113769044567\n",
      "\n",
      "\n",
      "Step: 238440\n",
      "loss: 0.0013\n",
      "grad_norm: 0.027070458978414536\n",
      "learning_rate: 1.7072476149793535e-05\n",
      "epoch: 8.487825715506194\n",
      "\n",
      "\n",
      "Step: 238460\n",
      "loss: 0.0015\n",
      "grad_norm: 0.010879558511078358\n",
      "learning_rate: 1.7048744601072664e-05\n",
      "epoch: 8.48853766196782\n",
      "\n",
      "\n",
      "Step: 238480\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01648622564971447\n",
      "learning_rate: 1.7025013052351796e-05\n",
      "epoch: 8.489249608429446\n",
      "\n",
      "\n",
      "Step: 238500\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01612904854118824\n",
      "learning_rate: 1.7001281503630927e-05\n",
      "epoch: 8.489961554891073\n",
      "\n",
      "\n",
      "Step: 238520\n",
      "loss: 0.0013\n",
      "grad_norm: 0.027666930109262466\n",
      "learning_rate: 1.6977549954910056e-05\n",
      "epoch: 8.490673501352699\n",
      "\n",
      "\n",
      "Step: 238540\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03134842589497566\n",
      "learning_rate: 1.6953818406189188e-05\n",
      "epoch: 8.491385447814324\n",
      "\n",
      "\n",
      "Step: 238560\n",
      "loss: 0.0012\n",
      "grad_norm: 0.024899359792470932\n",
      "learning_rate: 1.6930086857468316e-05\n",
      "epoch: 8.49209739427595\n",
      "\n",
      "\n",
      "Step: 238580\n",
      "loss: 0.0009\n",
      "grad_norm: 0.016319574788212776\n",
      "learning_rate: 1.6906355308747448e-05\n",
      "epoch: 8.492809340737576\n",
      "\n",
      "\n",
      "Step: 238600\n",
      "loss: 0.0007\n",
      "grad_norm: 0.016901016235351562\n",
      "learning_rate: 1.6882623760026576e-05\n",
      "epoch: 8.493521287199203\n",
      "\n",
      "\n",
      "Step: 238620\n",
      "loss: 0.001\n",
      "grad_norm: 0.0017528892494738102\n",
      "learning_rate: 1.6858892211305708e-05\n",
      "epoch: 8.494233233660829\n",
      "\n",
      "\n",
      "Step: 238640\n",
      "loss: 0.0013\n",
      "grad_norm: 0.06207391247153282\n",
      "learning_rate: 1.683516066258484e-05\n",
      "epoch: 8.494945180122455\n",
      "\n",
      "\n",
      "Step: 238660\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0029553670901805162\n",
      "learning_rate: 1.6811429113863968e-05\n",
      "epoch: 8.495657126584081\n",
      "\n",
      "\n",
      "Step: 238680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02865811251103878\n",
      "learning_rate: 1.67876975651431e-05\n",
      "epoch: 8.496369073045708\n",
      "\n",
      "\n",
      "Step: 238700\n",
      "loss: 0.0013\n",
      "grad_norm: 0.025689460337162018\n",
      "learning_rate: 1.676396601642223e-05\n",
      "epoch: 8.497081019507332\n",
      "\n",
      "\n",
      "Step: 238720\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03399590030312538\n",
      "learning_rate: 1.674023446770136e-05\n",
      "epoch: 8.497792965968959\n",
      "\n",
      "\n",
      "Step: 238740\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0036084086168557405\n",
      "learning_rate: 1.6716502918980492e-05\n",
      "epoch: 8.498504912430585\n",
      "\n",
      "\n",
      "Step: 238760\n",
      "loss: 0.0017\n",
      "grad_norm: 0.047471169382333755\n",
      "learning_rate: 1.6692771370259624e-05\n",
      "epoch: 8.499216858892211\n",
      "\n",
      "\n",
      "Step: 238780\n",
      "loss: 0.0011\n",
      "grad_norm: 0.044691309332847595\n",
      "learning_rate: 1.6669039821538752e-05\n",
      "epoch: 8.499928805353838\n",
      "\n",
      "\n",
      "Step: 238800\n",
      "loss: 0.001\n",
      "grad_norm: 0.02855549193918705\n",
      "learning_rate: 1.6645308272817884e-05\n",
      "epoch: 8.500640751815464\n",
      "\n",
      "\n",
      "Step: 238820\n",
      "loss: 0.0012\n",
      "grad_norm: 0.033263638615608215\n",
      "learning_rate: 1.6621576724097012e-05\n",
      "epoch: 8.50135269827709\n",
      "\n",
      "\n",
      "Step: 238840\n",
      "loss: 0.0012\n",
      "grad_norm: 0.015123923309147358\n",
      "learning_rate: 1.6597845175376144e-05\n",
      "epoch: 8.502064644738716\n",
      "\n",
      "\n",
      "Step: 238860\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009820382110774517\n",
      "learning_rate: 1.6574113626655276e-05\n",
      "epoch: 8.502776591200341\n",
      "\n",
      "\n",
      "Step: 238880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012211778201162815\n",
      "learning_rate: 1.6550382077934404e-05\n",
      "epoch: 8.503488537661967\n",
      "\n",
      "\n",
      "Step: 238900\n",
      "loss: 0.0015\n",
      "grad_norm: 0.00603192625567317\n",
      "learning_rate: 1.6526650529213536e-05\n",
      "epoch: 8.504200484123594\n",
      "\n",
      "\n",
      "Step: 238920\n",
      "loss: 0.0012\n",
      "grad_norm: 0.05251958966255188\n",
      "learning_rate: 1.6502918980492665e-05\n",
      "epoch: 8.50491243058522\n",
      "\n",
      "\n",
      "Step: 238940\n",
      "loss: 0.0008\n",
      "grad_norm: 0.00545575050637126\n",
      "learning_rate: 1.6479187431771796e-05\n",
      "epoch: 8.505624377046846\n",
      "\n",
      "\n",
      "Step: 238960\n",
      "loss: 0.0011\n",
      "grad_norm: 0.021919753402471542\n",
      "learning_rate: 1.6455455883050925e-05\n",
      "epoch: 8.506336323508473\n",
      "\n",
      "\n",
      "Step: 238980\n",
      "loss: 0.0008\n",
      "grad_norm: 0.03127370402216911\n",
      "learning_rate: 1.6431724334330057e-05\n",
      "epoch: 8.507048269970099\n",
      "\n",
      "\n",
      "Step: 239000\n",
      "loss: 0.0013\n",
      "grad_norm: 0.03020617738366127\n",
      "learning_rate: 1.640799278560919e-05\n",
      "epoch: 8.507760216431723\n",
      "\n",
      "\n",
      "Step: 239020\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03520889952778816\n",
      "learning_rate: 1.6384261236888317e-05\n",
      "epoch: 8.50847216289335\n",
      "\n",
      "\n",
      "Step: 239040\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0043981135822832584\n",
      "learning_rate: 1.636052968816745e-05\n",
      "epoch: 8.509184109354976\n",
      "\n",
      "\n",
      "Step: 239060\n",
      "loss: 0.0012\n",
      "grad_norm: 0.043418146669864655\n",
      "learning_rate: 1.6336798139446577e-05\n",
      "epoch: 8.509896055816602\n",
      "\n",
      "\n",
      "Step: 239080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.028839925304055214\n",
      "learning_rate: 1.631306659072571e-05\n",
      "epoch: 8.510608002278229\n",
      "\n",
      "\n",
      "Step: 239100\n",
      "loss: 0.0013\n",
      "grad_norm: 0.030079862102866173\n",
      "learning_rate: 1.628933504200484e-05\n",
      "epoch: 8.511319948739855\n",
      "\n",
      "\n",
      "Step: 239120\n",
      "loss: 0.0009\n",
      "grad_norm: 0.024494854733347893\n",
      "learning_rate: 1.626560349328397e-05\n",
      "epoch: 8.512031895201481\n",
      "\n",
      "\n",
      "Step: 239140\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01588442176580429\n",
      "learning_rate: 1.62418719445631e-05\n",
      "epoch: 8.512743841663108\n",
      "\n",
      "\n",
      "Step: 239160\n",
      "loss: 0.0006\n",
      "grad_norm: 0.00433860719203949\n",
      "learning_rate: 1.621814039584223e-05\n",
      "epoch: 8.513455788124734\n",
      "\n",
      "\n",
      "Step: 239180\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01820194721221924\n",
      "learning_rate: 1.619440884712136e-05\n",
      "epoch: 8.514167734586358\n",
      "\n",
      "\n",
      "Step: 239200\n",
      "loss: 0.0013\n",
      "grad_norm: 0.021486464887857437\n",
      "learning_rate: 1.6170677298400493e-05\n",
      "epoch: 8.514879681047985\n",
      "\n",
      "\n",
      "Step: 239220\n",
      "loss: 0.0011\n",
      "grad_norm: 0.022569619119167328\n",
      "learning_rate: 1.6146945749679625e-05\n",
      "epoch: 8.515591627509611\n",
      "\n",
      "\n",
      "Step: 239240\n",
      "loss: 0.0012\n",
      "grad_norm: 0.044001348316669464\n",
      "learning_rate: 1.6123214200958753e-05\n",
      "epoch: 8.516303573971237\n",
      "\n",
      "\n",
      "Step: 239260\n",
      "loss: 0.0015\n",
      "grad_norm: 0.022039376199245453\n",
      "learning_rate: 1.6099482652237885e-05\n",
      "epoch: 8.517015520432864\n",
      "\n",
      "\n",
      "Step: 239280\n",
      "loss: 0.0012\n",
      "grad_norm: 0.030404292047023773\n",
      "learning_rate: 1.6075751103517013e-05\n",
      "epoch: 8.51772746689449\n",
      "\n",
      "\n",
      "Step: 239300\n",
      "loss: 0.0017\n",
      "grad_norm: 0.059046801179647446\n",
      "learning_rate: 1.6052019554796145e-05\n",
      "epoch: 8.518439413356116\n",
      "\n",
      "\n",
      "Step: 239320\n",
      "loss: 0.0014\n",
      "grad_norm: 0.051860351115465164\n",
      "learning_rate: 1.6028288006075277e-05\n",
      "epoch: 8.51915135981774\n",
      "\n",
      "\n",
      "Step: 239340\n",
      "loss: 0.0008\n",
      "grad_norm: 0.01614466682076454\n",
      "learning_rate: 1.6004556457354405e-05\n",
      "epoch: 8.519863306279367\n",
      "\n",
      "\n",
      "Step: 239360\n",
      "loss: 0.0013\n",
      "grad_norm: 0.008860688656568527\n",
      "learning_rate: 1.5980824908633537e-05\n",
      "epoch: 8.520575252740993\n",
      "\n",
      "\n",
      "Step: 239380\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0010826234938576818\n",
      "learning_rate: 1.5957093359912666e-05\n",
      "epoch: 8.52128719920262\n",
      "\n",
      "\n",
      "Step: 239400\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0017341322964057326\n",
      "learning_rate: 1.5933361811191797e-05\n",
      "epoch: 8.521999145664246\n",
      "\n",
      "\n",
      "Step: 239420\n",
      "loss: 0.001\n",
      "grad_norm: 0.016977695748209953\n",
      "learning_rate: 1.5909630262470926e-05\n",
      "epoch: 8.522711092125872\n",
      "\n",
      "\n",
      "Step: 239440\n",
      "loss: 0.0011\n",
      "grad_norm: 0.031804248690605164\n",
      "learning_rate: 1.5885898713750058e-05\n",
      "epoch: 8.523423038587499\n",
      "\n",
      "\n",
      "Step: 239460\n",
      "loss: 0.0007\n",
      "grad_norm: 0.018919767811894417\n",
      "learning_rate: 1.586216716502919e-05\n",
      "epoch: 8.524134985049125\n",
      "\n",
      "\n",
      "Step: 239480\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02825547568500042\n",
      "learning_rate: 1.5838435616308318e-05\n",
      "epoch: 8.52484693151075\n",
      "\n",
      "\n",
      "Step: 239500\n",
      "loss: 0.0017\n",
      "grad_norm: 0.011408052407205105\n",
      "learning_rate: 1.581470406758745e-05\n",
      "epoch: 8.525558877972376\n",
      "\n",
      "\n",
      "Step: 239520\n",
      "loss: 0.0014\n",
      "grad_norm: 0.07691836357116699\n",
      "learning_rate: 1.5790972518866578e-05\n",
      "epoch: 8.526270824434002\n",
      "\n",
      "\n",
      "Step: 239540\n",
      "loss: 0.0013\n",
      "grad_norm: 0.023125097155570984\n",
      "learning_rate: 1.576724097014571e-05\n",
      "epoch: 8.526982770895629\n",
      "\n",
      "\n",
      "Step: 239560\n",
      "loss: 0.0014\n",
      "grad_norm: 0.044650617986917496\n",
      "learning_rate: 1.574350942142484e-05\n",
      "epoch: 8.527694717357255\n",
      "\n",
      "\n",
      "Step: 239580\n",
      "loss: 0.0009\n",
      "grad_norm: 0.016414012759923935\n",
      "learning_rate: 1.5719777872703973e-05\n",
      "epoch: 8.528406663818881\n",
      "\n",
      "\n",
      "Step: 239600\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0171338077634573\n",
      "learning_rate: 1.5696046323983102e-05\n",
      "epoch: 8.529118610280507\n",
      "\n",
      "\n",
      "Step: 239620\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03238092362880707\n",
      "learning_rate: 1.5672314775262234e-05\n",
      "epoch: 8.529830556742134\n",
      "\n",
      "\n",
      "Step: 239640\n",
      "loss: 0.0016\n",
      "grad_norm: 0.050342146307229996\n",
      "learning_rate: 1.5648583226541362e-05\n",
      "epoch: 8.530542503203758\n",
      "\n",
      "\n",
      "Step: 239660\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04768551141023636\n",
      "learning_rate: 1.5624851677820494e-05\n",
      "epoch: 8.531254449665385\n",
      "\n",
      "\n",
      "Step: 239680\n",
      "loss: 0.001\n",
      "grad_norm: 0.029585862532258034\n",
      "learning_rate: 1.5601120129099626e-05\n",
      "epoch: 8.531966396127011\n",
      "\n",
      "\n",
      "Step: 239700\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03702358528971672\n",
      "learning_rate: 1.5577388580378754e-05\n",
      "epoch: 8.532678342588637\n",
      "\n",
      "\n",
      "Step: 239720\n",
      "loss: 0.0016\n",
      "grad_norm: 0.016747940331697464\n",
      "learning_rate: 1.5553657031657886e-05\n",
      "epoch: 8.533390289050264\n",
      "\n",
      "\n",
      "Step: 239740\n",
      "loss: 0.0013\n",
      "grad_norm: 0.029395828023552895\n",
      "learning_rate: 1.5529925482937014e-05\n",
      "epoch: 8.53410223551189\n",
      "\n",
      "\n",
      "Step: 239760\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03521081060171127\n",
      "learning_rate: 1.5506193934216146e-05\n",
      "epoch: 8.534814181973516\n",
      "\n",
      "\n",
      "Step: 239780\n",
      "loss: 0.001\n",
      "grad_norm: 0.026336461305618286\n",
      "learning_rate: 1.5482462385495278e-05\n",
      "epoch: 8.535526128435142\n",
      "\n",
      "\n",
      "Step: 239800\n",
      "loss: 0.001\n",
      "grad_norm: 0.024370796978473663\n",
      "learning_rate: 1.5458730836774406e-05\n",
      "epoch: 8.536238074896767\n",
      "\n",
      "\n",
      "Step: 239820\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0433388277888298\n",
      "learning_rate: 1.5434999288053538e-05\n",
      "epoch: 8.536950021358393\n",
      "\n",
      "\n",
      "Step: 239840\n",
      "loss: 0.0014\n",
      "grad_norm: 0.009903926402330399\n",
      "learning_rate: 1.5411267739332666e-05\n",
      "epoch: 8.53766196782002\n",
      "\n",
      "\n",
      "Step: 239860\n",
      "loss: 0.0017\n",
      "grad_norm: 0.005012647248804569\n",
      "learning_rate: 1.5387536190611798e-05\n",
      "epoch: 8.538373914281646\n",
      "\n",
      "\n",
      "Step: 239880\n",
      "loss: 0.0008\n",
      "grad_norm: 0.015614128671586514\n",
      "learning_rate: 1.5363804641890927e-05\n",
      "epoch: 8.539085860743272\n",
      "\n",
      "\n",
      "Step: 239900\n",
      "loss: 0.0014\n",
      "grad_norm: 0.023097220808267593\n",
      "learning_rate: 1.534007309317006e-05\n",
      "epoch: 8.539797807204899\n",
      "\n",
      "\n",
      "Step: 239920\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0020895188208669424\n",
      "learning_rate: 1.531634154444919e-05\n",
      "epoch: 8.540509753666525\n",
      "\n",
      "\n",
      "Step: 239940\n",
      "loss: 0.0013\n",
      "grad_norm: 0.009247556328773499\n",
      "learning_rate: 1.529260999572832e-05\n",
      "epoch: 8.541221700128151\n",
      "\n",
      "\n",
      "Step: 239960\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010867360979318619\n",
      "learning_rate: 1.526887844700745e-05\n",
      "epoch: 8.541933646589776\n",
      "\n",
      "\n",
      "Step: 239980\n",
      "loss: 0.0012\n",
      "grad_norm: 0.009512258693575859\n",
      "learning_rate: 1.524514689828658e-05\n",
      "epoch: 8.542645593051402\n",
      "\n",
      "\n",
      "Step: 240000\n",
      "loss: 0.001\n",
      "grad_norm: 0.005653361324220896\n",
      "learning_rate: 1.522141534956571e-05\n",
      "epoch: 8.543357539513028\n",
      "\n",
      "\n",
      "Step: 240020\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01453979779034853\n",
      "learning_rate: 1.5197683800844842e-05\n",
      "epoch: 8.544069485974655\n",
      "\n",
      "\n",
      "Step: 240040\n",
      "loss: 0.001\n",
      "grad_norm: 0.01403031125664711\n",
      "learning_rate: 1.5173952252123973e-05\n",
      "epoch: 8.544781432436281\n",
      "\n",
      "\n",
      "Step: 240060\n",
      "loss: 0.0012\n",
      "grad_norm: 0.008640752173960209\n",
      "learning_rate: 1.5150220703403103e-05\n",
      "epoch: 8.545493378897907\n",
      "\n",
      "\n",
      "Step: 240080\n",
      "loss: 0.001\n",
      "grad_norm: 0.037274472415447235\n",
      "learning_rate: 1.5126489154682233e-05\n",
      "epoch: 8.546205325359534\n",
      "\n",
      "\n",
      "Step: 240100\n",
      "loss: 0.0015\n",
      "grad_norm: 0.04496004059910774\n",
      "learning_rate: 1.5102757605961363e-05\n",
      "epoch: 8.546917271821158\n",
      "\n",
      "\n",
      "Step: 240120\n",
      "loss: 0.001\n",
      "grad_norm: 0.015092681162059307\n",
      "learning_rate: 1.5079026057240493e-05\n",
      "epoch: 8.547629218282784\n",
      "\n",
      "\n",
      "Step: 240140\n",
      "loss: 0.0007\n",
      "grad_norm: 0.00684466352686286\n",
      "learning_rate: 1.5055294508519625e-05\n",
      "epoch: 8.54834116474441\n",
      "\n",
      "\n",
      "Step: 240160\n",
      "loss: 0.0016\n",
      "grad_norm: 0.03420330956578255\n",
      "learning_rate: 1.5031562959798755e-05\n",
      "epoch: 8.549053111206037\n",
      "\n",
      "\n",
      "Step: 240180\n",
      "loss: 0.0009\n",
      "grad_norm: 0.013509809039533138\n",
      "learning_rate: 1.5007831411077885e-05\n",
      "epoch: 8.549765057667663\n",
      "\n",
      "\n",
      "Step: 240200\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00032101577380672097\n",
      "learning_rate: 1.4984099862357015e-05\n",
      "epoch: 8.55047700412929\n",
      "\n",
      "\n",
      "Step: 240220\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02967604622244835\n",
      "learning_rate: 1.4960368313636145e-05\n",
      "epoch: 8.551188950590916\n",
      "\n",
      "\n",
      "Step: 240240\n",
      "loss: 0.001\n",
      "grad_norm: 0.01107204519212246\n",
      "learning_rate: 1.4936636764915279e-05\n",
      "epoch: 8.551900897052542\n",
      "\n",
      "\n",
      "Step: 240260\n",
      "loss: 0.0015\n",
      "grad_norm: 0.02459992840886116\n",
      "learning_rate: 1.4912905216194409e-05\n",
      "epoch: 8.552612843514169\n",
      "\n",
      "\n",
      "Step: 240280\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04401634633541107\n",
      "learning_rate: 1.4889173667473539e-05\n",
      "epoch: 8.553324789975793\n",
      "\n",
      "\n",
      "Step: 240300\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008996142074465752\n",
      "learning_rate: 1.4865442118752669e-05\n",
      "epoch: 8.55403673643742\n",
      "\n",
      "\n",
      "Step: 240320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04979891702532768\n",
      "learning_rate: 1.4841710570031799e-05\n",
      "epoch: 8.554748682899046\n",
      "\n",
      "\n",
      "Step: 240340\n",
      "loss: 0.0016\n",
      "grad_norm: 0.05411127954721451\n",
      "learning_rate: 1.4817979021310929e-05\n",
      "epoch: 8.555460629360672\n",
      "\n",
      "\n",
      "Step: 240360\n",
      "loss: 0.001\n",
      "grad_norm: 0.0455329492688179\n",
      "learning_rate: 1.4794247472590061e-05\n",
      "epoch: 8.556172575822298\n",
      "\n",
      "\n",
      "Step: 240380\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0017539852997288108\n",
      "learning_rate: 1.4770515923869191e-05\n",
      "epoch: 8.556884522283925\n",
      "\n",
      "\n",
      "Step: 240400\n",
      "loss: 0.0009\n",
      "grad_norm: 0.024043992161750793\n",
      "learning_rate: 1.4746784375148321e-05\n",
      "epoch: 8.557596468745551\n",
      "\n",
      "\n",
      "Step: 240420\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02031683176755905\n",
      "learning_rate: 1.4723052826427451e-05\n",
      "epoch: 8.558308415207176\n",
      "\n",
      "\n",
      "Step: 240440\n",
      "loss: 0.001\n",
      "grad_norm: 0.02376682683825493\n",
      "learning_rate: 1.4699321277706581e-05\n",
      "epoch: 8.559020361668802\n",
      "\n",
      "\n",
      "Step: 240460\n",
      "loss: 0.0009\n",
      "grad_norm: 0.005444388370960951\n",
      "learning_rate: 1.4675589728985711e-05\n",
      "epoch: 8.559732308130428\n",
      "\n",
      "\n",
      "Step: 240480\n",
      "loss: 0.0009\n",
      "grad_norm: 0.04581037536263466\n",
      "learning_rate: 1.4651858180264843e-05\n",
      "epoch: 8.560444254592054\n",
      "\n",
      "\n",
      "Step: 240500\n",
      "loss: 0.0011\n",
      "grad_norm: 0.013421772047877312\n",
      "learning_rate: 1.4628126631543973e-05\n",
      "epoch: 8.56115620105368\n",
      "\n",
      "\n",
      "Step: 240520\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0023671602830290794\n",
      "learning_rate: 1.4604395082823103e-05\n",
      "epoch: 8.561868147515307\n",
      "\n",
      "\n",
      "Step: 240540\n",
      "loss: 0.001\n",
      "grad_norm: 0.0327814444899559\n",
      "learning_rate: 1.4580663534102234e-05\n",
      "epoch: 8.562580093976933\n",
      "\n",
      "\n",
      "Step: 240560\n",
      "loss: 0.0008\n",
      "grad_norm: 0.002477676374837756\n",
      "learning_rate: 1.4556931985381364e-05\n",
      "epoch: 8.56329204043856\n",
      "\n",
      "\n",
      "Step: 240580\n",
      "loss: 0.001\n",
      "grad_norm: 0.035214368253946304\n",
      "learning_rate: 1.4533200436660494e-05\n",
      "epoch: 8.564003986900186\n",
      "\n",
      "\n",
      "Step: 240600\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04117989167571068\n",
      "learning_rate: 1.4509468887939627e-05\n",
      "epoch: 8.56471593336181\n",
      "\n",
      "\n",
      "Step: 240620\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01949724182486534\n",
      "learning_rate: 1.4485737339218757e-05\n",
      "epoch: 8.565427879823437\n",
      "\n",
      "\n",
      "Step: 240640\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008529046550393105\n",
      "learning_rate: 1.4462005790497887e-05\n",
      "epoch: 8.566139826285063\n",
      "\n",
      "\n",
      "Step: 240660\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0021786768920719624\n",
      "learning_rate: 1.4438274241777018e-05\n",
      "epoch: 8.56685177274669\n",
      "\n",
      "\n",
      "Step: 240680\n",
      "loss: 0.001\n",
      "grad_norm: 0.016489354893565178\n",
      "learning_rate: 1.4414542693056148e-05\n",
      "epoch: 8.567563719208316\n",
      "\n",
      "\n",
      "Step: 240700\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01728554628789425\n",
      "learning_rate: 1.4390811144335278e-05\n",
      "epoch: 8.568275665669942\n",
      "\n",
      "\n",
      "Step: 240720\n",
      "loss: 0.0013\n",
      "grad_norm: 0.07288529723882675\n",
      "learning_rate: 1.436707959561441e-05\n",
      "epoch: 8.568987612131568\n",
      "\n",
      "\n",
      "Step: 240740\n",
      "loss: 0.0008\n",
      "grad_norm: 0.009617028757929802\n",
      "learning_rate: 1.434334804689354e-05\n",
      "epoch: 8.569699558593193\n",
      "\n",
      "\n",
      "Step: 240760\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02871844917535782\n",
      "learning_rate: 1.431961649817267e-05\n",
      "epoch: 8.57041150505482\n",
      "\n",
      "\n",
      "Step: 240780\n",
      "loss: 0.0009\n",
      "grad_norm: 0.005044520832598209\n",
      "learning_rate: 1.42958849494518e-05\n",
      "epoch: 8.571123451516446\n",
      "\n",
      "\n",
      "Step: 240800\n",
      "loss: 0.0009\n",
      "grad_norm: 0.019768398255109787\n",
      "learning_rate: 1.427215340073093e-05\n",
      "epoch: 8.571835397978072\n",
      "\n",
      "\n",
      "Step: 240820\n",
      "loss: 0.0009\n",
      "grad_norm: 0.003293110989034176\n",
      "learning_rate: 1.4248421852010062e-05\n",
      "epoch: 8.572547344439698\n",
      "\n",
      "\n",
      "Step: 240840\n",
      "loss: 0.0012\n",
      "grad_norm: 0.015062066726386547\n",
      "learning_rate: 1.4224690303289192e-05\n",
      "epoch: 8.573259290901325\n",
      "\n",
      "\n",
      "Step: 240860\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03429145738482475\n",
      "learning_rate: 1.4200958754568322e-05\n",
      "epoch: 8.57397123736295\n",
      "\n",
      "\n",
      "Step: 240880\n",
      "loss: 0.0013\n",
      "grad_norm: 0.009128334000706673\n",
      "learning_rate: 1.4177227205847452e-05\n",
      "epoch: 8.574683183824577\n",
      "\n",
      "\n",
      "Step: 240900\n",
      "loss: 0.0019\n",
      "grad_norm: 0.022495323792099953\n",
      "learning_rate: 1.4153495657126582e-05\n",
      "epoch: 8.575395130286202\n",
      "\n",
      "\n",
      "Step: 240920\n",
      "loss: 0.0013\n",
      "grad_norm: 0.022200077772140503\n",
      "learning_rate: 1.4129764108405712e-05\n",
      "epoch: 8.576107076747828\n",
      "\n",
      "\n",
      "Step: 240940\n",
      "loss: 0.0009\n",
      "grad_norm: 0.024394506588578224\n",
      "learning_rate: 1.4106032559684844e-05\n",
      "epoch: 8.576819023209454\n",
      "\n",
      "\n",
      "Step: 240960\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03818199411034584\n",
      "learning_rate: 1.4082301010963974e-05\n",
      "epoch: 8.57753096967108\n",
      "\n",
      "\n",
      "Step: 240980\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03163576126098633\n",
      "learning_rate: 1.4058569462243104e-05\n",
      "epoch: 8.578242916132707\n",
      "\n",
      "\n",
      "Step: 241000\n",
      "loss: 0.001\n",
      "grad_norm: 0.0462246835231781\n",
      "learning_rate: 1.4034837913522234e-05\n",
      "epoch: 8.578954862594333\n",
      "\n",
      "\n",
      "Step: 241020\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0056969765573740005\n",
      "learning_rate: 1.4011106364801365e-05\n",
      "epoch: 8.57966680905596\n",
      "\n",
      "\n",
      "Step: 241040\n",
      "loss: 0.0017\n",
      "grad_norm: 0.022168589755892754\n",
      "learning_rate: 1.3987374816080495e-05\n",
      "epoch: 8.580378755517586\n",
      "\n",
      "\n",
      "Step: 241060\n",
      "loss: 0.001\n",
      "grad_norm: 0.023635054007172585\n",
      "learning_rate: 1.3963643267359628e-05\n",
      "epoch: 8.58109070197921\n",
      "\n",
      "\n",
      "Step: 241080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02261565439403057\n",
      "learning_rate: 1.3939911718638758e-05\n",
      "epoch: 8.581802648440837\n",
      "\n",
      "\n",
      "Step: 241100\n",
      "loss: 0.002\n",
      "grad_norm: 0.05165712907910347\n",
      "learning_rate: 1.3916180169917888e-05\n",
      "epoch: 8.582514594902463\n",
      "\n",
      "\n",
      "Step: 241120\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01815573126077652\n",
      "learning_rate: 1.3892448621197018e-05\n",
      "epoch: 8.58322654136409\n",
      "\n",
      "\n",
      "Step: 241140\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03737803176045418\n",
      "learning_rate: 1.3868717072476149e-05\n",
      "epoch: 8.583938487825716\n",
      "\n",
      "\n",
      "Step: 241160\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0097481869161129\n",
      "learning_rate: 1.3844985523755279e-05\n",
      "epoch: 8.584650434287342\n",
      "\n",
      "\n",
      "Step: 241180\n",
      "loss: 0.0014\n",
      "grad_norm: 0.04365324229001999\n",
      "learning_rate: 1.382125397503441e-05\n",
      "epoch: 8.585362380748968\n",
      "\n",
      "\n",
      "Step: 241200\n",
      "loss: 0.0017\n",
      "grad_norm: 0.014749452471733093\n",
      "learning_rate: 1.379752242631354e-05\n",
      "epoch: 8.586074327210593\n",
      "\n",
      "\n",
      "Step: 241220\n",
      "loss: 0.0012\n",
      "grad_norm: 0.008439799770712852\n",
      "learning_rate: 1.377379087759267e-05\n",
      "epoch: 8.58678627367222\n",
      "\n",
      "\n",
      "Step: 241240\n",
      "loss: 0.0013\n",
      "grad_norm: 0.014559040777385235\n",
      "learning_rate: 1.37500593288718e-05\n",
      "epoch: 8.587498220133845\n",
      "\n",
      "\n",
      "Step: 241260\n",
      "loss: 0.0009\n",
      "grad_norm: 0.022126324474811554\n",
      "learning_rate: 1.3726327780150931e-05\n",
      "epoch: 8.588210166595472\n",
      "\n",
      "\n",
      "Step: 241280\n",
      "loss: 0.0009\n",
      "grad_norm: 0.008437654934823513\n",
      "learning_rate: 1.3702596231430061e-05\n",
      "epoch: 8.588922113057098\n",
      "\n",
      "\n",
      "Step: 241300\n",
      "loss: 0.0012\n",
      "grad_norm: 0.063536137342453\n",
      "learning_rate: 1.3678864682709193e-05\n",
      "epoch: 8.589634059518724\n",
      "\n",
      "\n",
      "Step: 241320\n",
      "loss: 0.0008\n",
      "grad_norm: 0.004031405318528414\n",
      "learning_rate: 1.3655133133988323e-05\n",
      "epoch: 8.59034600598035\n",
      "\n",
      "\n",
      "Step: 241340\n",
      "loss: 0.0008\n",
      "grad_norm: 0.017498837783932686\n",
      "learning_rate: 1.3631401585267453e-05\n",
      "epoch: 8.591057952441977\n",
      "\n",
      "\n",
      "Step: 241360\n",
      "loss: 0.0011\n",
      "grad_norm: 0.015769772231578827\n",
      "learning_rate: 1.3607670036546583e-05\n",
      "epoch: 8.591769898903603\n",
      "\n",
      "\n",
      "Step: 241380\n",
      "loss: 0.0012\n",
      "grad_norm: 0.012954196892678738\n",
      "learning_rate: 1.3583938487825713e-05\n",
      "epoch: 8.592481845365228\n",
      "\n",
      "\n",
      "Step: 241400\n",
      "loss: 0.001\n",
      "grad_norm: 0.018149560317397118\n",
      "learning_rate: 1.3560206939104847e-05\n",
      "epoch: 8.593193791826854\n",
      "\n",
      "\n",
      "Step: 241420\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0002633141411934048\n",
      "learning_rate: 1.3536475390383977e-05\n",
      "epoch: 8.59390573828848\n",
      "\n",
      "\n",
      "Step: 241440\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04143744334578514\n",
      "learning_rate: 1.3512743841663107e-05\n",
      "epoch: 8.594617684750107\n",
      "\n",
      "\n",
      "Step: 241460\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03810795024037361\n",
      "learning_rate: 1.3489012292942237e-05\n",
      "epoch: 8.595329631211733\n",
      "\n",
      "\n",
      "Step: 241480\n",
      "loss: 0.0015\n",
      "grad_norm: 0.006411997135728598\n",
      "learning_rate: 1.3465280744221367e-05\n",
      "epoch: 8.59604157767336\n",
      "\n",
      "\n",
      "Step: 241500\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011094341054558754\n",
      "learning_rate: 1.3441549195500497e-05\n",
      "epoch: 8.596753524134986\n",
      "\n",
      "\n",
      "Step: 241520\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00997734721750021\n",
      "learning_rate: 1.3417817646779629e-05\n",
      "epoch: 8.59746547059661\n",
      "\n",
      "\n",
      "Step: 241540\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0031261714175343513\n",
      "learning_rate: 1.3394086098058759e-05\n",
      "epoch: 8.598177417058237\n",
      "\n",
      "\n",
      "Step: 241560\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02343432791531086\n",
      "learning_rate: 1.337035454933789e-05\n",
      "epoch: 8.598889363519863\n",
      "\n",
      "\n",
      "Step: 241580\n",
      "loss: 0.0013\n",
      "grad_norm: 0.010113058611750603\n",
      "learning_rate: 1.334662300061702e-05\n",
      "epoch: 8.59960130998149\n",
      "\n",
      "\n",
      "Step: 241600\n",
      "loss: 0.0016\n",
      "grad_norm: 0.02675212360918522\n",
      "learning_rate: 1.332289145189615e-05\n",
      "epoch: 8.600313256443116\n",
      "\n",
      "\n",
      "Step: 241620\n",
      "loss: 0.0011\n",
      "grad_norm: 0.014575155451893806\n",
      "learning_rate: 1.329915990317528e-05\n",
      "epoch: 8.601025202904742\n",
      "\n",
      "\n",
      "Step: 241640\n",
      "loss: 0.0007\n",
      "grad_norm: 0.03754231706261635\n",
      "learning_rate: 1.3275428354454411e-05\n",
      "epoch: 8.601737149366368\n",
      "\n",
      "\n",
      "Step: 241660\n",
      "loss: 0.0009\n",
      "grad_norm: 0.005555540323257446\n",
      "learning_rate: 1.3251696805733541e-05\n",
      "epoch: 8.602449095827994\n",
      "\n",
      "\n",
      "Step: 241680\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0012332869227975607\n",
      "learning_rate: 1.3227965257012671e-05\n",
      "epoch: 8.60316104228962\n",
      "\n",
      "\n",
      "Step: 241700\n",
      "loss: 0.0009\n",
      "grad_norm: 0.014186128042638302\n",
      "learning_rate: 1.3204233708291802e-05\n",
      "epoch: 8.603872988751245\n",
      "\n",
      "\n",
      "Step: 241720\n",
      "loss: 0.0012\n",
      "grad_norm: 0.011682161130011082\n",
      "learning_rate: 1.3180502159570932e-05\n",
      "epoch: 8.604584935212872\n",
      "\n",
      "\n",
      "Step: 241740\n",
      "loss: 0.0017\n",
      "grad_norm: 0.07578445225954056\n",
      "learning_rate: 1.3156770610850062e-05\n",
      "epoch: 8.605296881674498\n",
      "\n",
      "\n",
      "Step: 241760\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008936915546655655\n",
      "learning_rate: 1.3133039062129194e-05\n",
      "epoch: 8.606008828136124\n",
      "\n",
      "\n",
      "Step: 241780\n",
      "loss: 0.0011\n",
      "grad_norm: 0.024677814915776253\n",
      "learning_rate: 1.3109307513408324e-05\n",
      "epoch: 8.60672077459775\n",
      "\n",
      "\n",
      "Step: 241800\n",
      "loss: 0.0014\n",
      "grad_norm: 0.023756766691803932\n",
      "learning_rate: 1.3085575964687454e-05\n",
      "epoch: 8.607432721059377\n",
      "\n",
      "\n",
      "Step: 241820\n",
      "loss: 0.0016\n",
      "grad_norm: 0.010874681174755096\n",
      "learning_rate: 1.3061844415966584e-05\n",
      "epoch: 8.608144667521003\n",
      "\n",
      "\n",
      "Step: 241840\n",
      "loss: 0.0013\n",
      "grad_norm: 0.06512054800987244\n",
      "learning_rate: 1.3038112867245714e-05\n",
      "epoch: 8.608856613982628\n",
      "\n",
      "\n",
      "Step: 241860\n",
      "loss: 0.0008\n",
      "grad_norm: 0.018763961270451546\n",
      "learning_rate: 1.3014381318524844e-05\n",
      "epoch: 8.609568560444254\n",
      "\n",
      "\n",
      "Step: 241880\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0018407026072964072\n",
      "learning_rate: 1.2990649769803978e-05\n",
      "epoch: 8.61028050690588\n",
      "\n",
      "\n",
      "Step: 241900\n",
      "loss: 0.0014\n",
      "grad_norm: 0.04413255304098129\n",
      "learning_rate: 1.2966918221083108e-05\n",
      "epoch: 8.610992453367507\n",
      "\n",
      "\n",
      "Step: 241920\n",
      "loss: 0.0011\n",
      "grad_norm: 0.021349875256419182\n",
      "learning_rate: 1.2943186672362238e-05\n",
      "epoch: 8.611704399829133\n",
      "\n",
      "\n",
      "Step: 241940\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03356914594769478\n",
      "learning_rate: 1.2919455123641368e-05\n",
      "epoch: 8.61241634629076\n",
      "\n",
      "\n",
      "Step: 241960\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02006477676331997\n",
      "learning_rate: 1.2895723574920498e-05\n",
      "epoch: 8.613128292752386\n",
      "\n",
      "\n",
      "Step: 241980\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0051602572202682495\n",
      "learning_rate: 1.287199202619963e-05\n",
      "epoch: 8.613840239214012\n",
      "\n",
      "\n",
      "Step: 242000\n",
      "loss: 0.0014\n",
      "grad_norm: 0.019171060994267464\n",
      "learning_rate: 1.284826047747876e-05\n",
      "epoch: 8.614552185675636\n",
      "\n",
      "\n",
      "Step: 242020\n",
      "loss: 0.0017\n",
      "grad_norm: 0.019965367391705513\n",
      "learning_rate: 1.282452892875789e-05\n",
      "epoch: 8.615264132137263\n",
      "\n",
      "\n",
      "Step: 242040\n",
      "loss: 0.0008\n",
      "grad_norm: 0.02365264669060707\n",
      "learning_rate: 1.280079738003702e-05\n",
      "epoch: 8.615976078598889\n",
      "\n",
      "\n",
      "Step: 242060\n",
      "loss: 0.0008\n",
      "grad_norm: 0.02085689641535282\n",
      "learning_rate: 1.277706583131615e-05\n",
      "epoch: 8.616688025060515\n",
      "\n",
      "\n",
      "Step: 242080\n",
      "loss: 0.0013\n",
      "grad_norm: 0.013479463756084442\n",
      "learning_rate: 1.275333428259528e-05\n",
      "epoch: 8.617399971522142\n",
      "\n",
      "\n",
      "Step: 242100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.045910511165857315\n",
      "learning_rate: 1.2729602733874412e-05\n",
      "epoch: 8.618111917983768\n",
      "\n",
      "\n",
      "Step: 242120\n",
      "loss: 0.0013\n",
      "grad_norm: 0.043134406208992004\n",
      "learning_rate: 1.2705871185153542e-05\n",
      "epoch: 8.618823864445394\n",
      "\n",
      "\n",
      "Step: 242140\n",
      "loss: 0.0008\n",
      "grad_norm: 0.002227646764367819\n",
      "learning_rate: 1.2682139636432672e-05\n",
      "epoch: 8.61953581090702\n",
      "\n",
      "\n",
      "Step: 242160\n",
      "loss: 0.0012\n",
      "grad_norm: 0.021295173093676567\n",
      "learning_rate: 1.2658408087711802e-05\n",
      "epoch: 8.620247757368645\n",
      "\n",
      "\n",
      "Step: 242180\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02440286986529827\n",
      "learning_rate: 1.2634676538990933e-05\n",
      "epoch: 8.620959703830271\n",
      "\n",
      "\n",
      "Step: 242200\n",
      "loss: 0.0012\n",
      "grad_norm: 0.014104598201811314\n",
      "learning_rate: 1.2610944990270063e-05\n",
      "epoch: 8.621671650291898\n",
      "\n",
      "\n",
      "Step: 242220\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009843355976045132\n",
      "learning_rate: 1.2587213441549194e-05\n",
      "epoch: 8.622383596753524\n",
      "\n",
      "\n",
      "Step: 242240\n",
      "loss: 0.0011\n",
      "grad_norm: 0.003319450654089451\n",
      "learning_rate: 1.2563481892828325e-05\n",
      "epoch: 8.62309554321515\n",
      "\n",
      "\n",
      "Step: 242260\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004237116780132055\n",
      "learning_rate: 1.2539750344107456e-05\n",
      "epoch: 8.623807489676777\n",
      "\n",
      "\n",
      "Step: 242280\n",
      "loss: 0.0015\n",
      "grad_norm: 0.028514564037322998\n",
      "learning_rate: 1.2516018795386586e-05\n",
      "epoch: 8.624519436138403\n",
      "\n",
      "\n",
      "Step: 242300\n",
      "loss: 0.0009\n",
      "grad_norm: 0.023528100922703743\n",
      "learning_rate: 1.2492287246665717e-05\n",
      "epoch: 8.62523138260003\n",
      "\n",
      "\n",
      "Step: 242320\n",
      "loss: 0.0013\n",
      "grad_norm: 0.06185328587889671\n",
      "learning_rate: 1.2468555697944847e-05\n",
      "epoch: 8.625943329061654\n",
      "\n",
      "\n",
      "Step: 242340\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04220318794250488\n",
      "learning_rate: 1.2444824149223978e-05\n",
      "epoch: 8.62665527552328\n",
      "\n",
      "\n",
      "Step: 242360\n",
      "loss: 0.001\n",
      "grad_norm: 0.006571796257048845\n",
      "learning_rate: 1.2421092600503109e-05\n",
      "epoch: 8.627367221984906\n",
      "\n",
      "\n",
      "Step: 242380\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0417761355638504\n",
      "learning_rate: 1.2397361051782239e-05\n",
      "epoch: 8.628079168446533\n",
      "\n",
      "\n",
      "Step: 242400\n",
      "loss: 0.0015\n",
      "grad_norm: 0.024866411462426186\n",
      "learning_rate: 1.2373629503061369e-05\n",
      "epoch: 8.628791114908159\n",
      "\n",
      "\n",
      "Step: 242420\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0190756656229496\n",
      "learning_rate: 1.2349897954340499e-05\n",
      "epoch: 8.629503061369785\n",
      "\n",
      "\n",
      "Step: 242440\n",
      "loss: 0.0013\n",
      "grad_norm: 0.046560343354940414\n",
      "learning_rate: 1.2326166405619629e-05\n",
      "epoch: 8.630215007831412\n",
      "\n",
      "\n",
      "Step: 242460\n",
      "loss: 0.0008\n",
      "grad_norm: 0.026307160034775734\n",
      "learning_rate: 1.230243485689876e-05\n",
      "epoch: 8.630926954293038\n",
      "\n",
      "\n",
      "Step: 242480\n",
      "loss: 0.0009\n",
      "grad_norm: 0.001855274080298841\n",
      "learning_rate: 1.2278703308177891e-05\n",
      "epoch: 8.631638900754663\n",
      "\n",
      "\n",
      "Step: 242500\n",
      "loss: 0.001\n",
      "grad_norm: 0.007155443541705608\n",
      "learning_rate: 1.2254971759457021e-05\n",
      "epoch: 8.632350847216289\n",
      "\n",
      "\n",
      "Step: 242520\n",
      "loss: 0.0016\n",
      "grad_norm: 0.04456104710698128\n",
      "learning_rate: 1.2231240210736151e-05\n",
      "epoch: 8.633062793677915\n",
      "\n",
      "\n",
      "Step: 242540\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008404992520809174\n",
      "learning_rate: 1.2207508662015281e-05\n",
      "epoch: 8.633774740139541\n",
      "\n",
      "\n",
      "Step: 242560\n",
      "loss: 0.0011\n",
      "grad_norm: 0.020260749384760857\n",
      "learning_rate: 1.2183777113294413e-05\n",
      "epoch: 8.634486686601168\n",
      "\n",
      "\n",
      "Step: 242580\n",
      "loss: 0.0012\n",
      "grad_norm: 0.007758468389511108\n",
      "learning_rate: 1.2160045564573543e-05\n",
      "epoch: 8.635198633062794\n",
      "\n",
      "\n",
      "Step: 242600\n",
      "loss: 0.0013\n",
      "grad_norm: 0.008929871954023838\n",
      "learning_rate: 1.2136314015852673e-05\n",
      "epoch: 8.63591057952442\n",
      "\n",
      "\n",
      "Step: 242620\n",
      "loss: 0.0012\n",
      "grad_norm: 0.05041518062353134\n",
      "learning_rate: 1.2112582467131803e-05\n",
      "epoch: 8.636622525986045\n",
      "\n",
      "\n",
      "Step: 242640\n",
      "loss: 0.0011\n",
      "grad_norm: 0.002369152382016182\n",
      "learning_rate: 1.2088850918410933e-05\n",
      "epoch: 8.637334472447671\n",
      "\n",
      "\n",
      "Step: 242660\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0008493455243296921\n",
      "learning_rate: 1.2065119369690064e-05\n",
      "epoch: 8.638046418909298\n",
      "\n",
      "\n",
      "Step: 242680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01206399779766798\n",
      "learning_rate: 1.2041387820969197e-05\n",
      "epoch: 8.638758365370924\n",
      "\n",
      "\n",
      "Step: 242700\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01683994196355343\n",
      "learning_rate: 1.2017656272248327e-05\n",
      "epoch: 8.63947031183255\n",
      "\n",
      "\n",
      "Step: 242720\n",
      "loss: 0.0012\n",
      "grad_norm: 0.005918164271861315\n",
      "learning_rate: 1.1993924723527457e-05\n",
      "epoch: 8.640182258294177\n",
      "\n",
      "\n",
      "Step: 242740\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03623194992542267\n",
      "learning_rate: 1.1970193174806587e-05\n",
      "epoch: 8.640894204755803\n",
      "\n",
      "\n",
      "Step: 242760\n",
      "loss: 0.001\n",
      "grad_norm: 0.019650548696517944\n",
      "learning_rate: 1.1946461626085717e-05\n",
      "epoch: 8.64160615121743\n",
      "\n",
      "\n",
      "Step: 242780\n",
      "loss: 0.001\n",
      "grad_norm: 0.02250051125884056\n",
      "learning_rate: 1.1922730077364848e-05\n",
      "epoch: 8.642318097679055\n",
      "\n",
      "\n",
      "Step: 242800\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02196173183619976\n",
      "learning_rate: 1.189899852864398e-05\n",
      "epoch: 8.64303004414068\n",
      "\n",
      "\n",
      "Step: 242820\n",
      "loss: 0.0011\n",
      "grad_norm: 0.06794026494026184\n",
      "learning_rate: 1.187526697992311e-05\n",
      "epoch: 8.643741990602306\n",
      "\n",
      "\n",
      "Step: 242840\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03704962506890297\n",
      "learning_rate: 1.185153543120224e-05\n",
      "epoch: 8.644453937063933\n",
      "\n",
      "\n",
      "Step: 242860\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02932725101709366\n",
      "learning_rate: 1.182780388248137e-05\n",
      "epoch: 8.645165883525559\n",
      "\n",
      "\n",
      "Step: 242880\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01932467706501484\n",
      "learning_rate: 1.18040723337605e-05\n",
      "epoch: 8.645877829987185\n",
      "\n",
      "\n",
      "Step: 242900\n",
      "loss: 0.001\n",
      "grad_norm: 0.0201557707041502\n",
      "learning_rate: 1.178034078503963e-05\n",
      "epoch: 8.646589776448812\n",
      "\n",
      "\n",
      "Step: 242920\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0384366437792778\n",
      "learning_rate: 1.1756609236318762e-05\n",
      "epoch: 8.647301722910438\n",
      "\n",
      "\n",
      "Step: 242940\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03321171924471855\n",
      "learning_rate: 1.1732877687597892e-05\n",
      "epoch: 8.648013669372062\n",
      "\n",
      "\n",
      "Step: 242960\n",
      "loss: 0.0011\n",
      "grad_norm: 0.014110208489000797\n",
      "learning_rate: 1.1709146138877022e-05\n",
      "epoch: 8.648725615833689\n",
      "\n",
      "\n",
      "Step: 242980\n",
      "loss: 0.0012\n",
      "grad_norm: 0.009545836597681046\n",
      "learning_rate: 1.1685414590156152e-05\n",
      "epoch: 8.649437562295315\n",
      "\n",
      "\n",
      "Step: 243000\n",
      "loss: 0.0014\n",
      "grad_norm: 0.008233385160565376\n",
      "learning_rate: 1.1661683041435282e-05\n",
      "epoch: 8.650149508756941\n",
      "\n",
      "\n",
      "Step: 243020\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03670111298561096\n",
      "learning_rate: 1.1637951492714412e-05\n",
      "epoch: 8.650861455218568\n",
      "\n",
      "\n",
      "Step: 243040\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008138395845890045\n",
      "learning_rate: 1.1614219943993544e-05\n",
      "epoch: 8.651573401680194\n",
      "\n",
      "\n",
      "Step: 243060\n",
      "loss: 0.001\n",
      "grad_norm: 0.02444535121321678\n",
      "learning_rate: 1.1590488395272674e-05\n",
      "epoch: 8.65228534814182\n",
      "\n",
      "\n",
      "Step: 243080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0028868766967207193\n",
      "learning_rate: 1.1566756846551804e-05\n",
      "epoch: 8.652997294603447\n",
      "\n",
      "\n",
      "Step: 243100\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01726618781685829\n",
      "learning_rate: 1.1543025297830934e-05\n",
      "epoch: 8.653709241065071\n",
      "\n",
      "\n",
      "Step: 243120\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03448690474033356\n",
      "learning_rate: 1.1519293749110066e-05\n",
      "epoch: 8.654421187526697\n",
      "\n",
      "\n",
      "Step: 243140\n",
      "loss: 0.001\n",
      "grad_norm: 0.009244067594408989\n",
      "learning_rate: 1.1495562200389198e-05\n",
      "epoch: 8.655133133988324\n",
      "\n",
      "\n",
      "Step: 243160\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03626463934779167\n",
      "learning_rate: 1.1471830651668328e-05\n",
      "epoch: 8.65584508044995\n",
      "\n",
      "\n",
      "Step: 243180\n",
      "loss: 0.001\n",
      "grad_norm: 0.019703174009919167\n",
      "learning_rate: 1.1448099102947458e-05\n",
      "epoch: 8.656557026911576\n",
      "\n",
      "\n",
      "Step: 243200\n",
      "loss: 0.0014\n",
      "grad_norm: 0.012239218689501286\n",
      "learning_rate: 1.1424367554226588e-05\n",
      "epoch: 8.657268973373203\n",
      "\n",
      "\n",
      "Step: 243220\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03995520994067192\n",
      "learning_rate: 1.1400636005505718e-05\n",
      "epoch: 8.657980919834829\n",
      "\n",
      "\n",
      "Step: 243240\n",
      "loss: 0.0016\n",
      "grad_norm: 0.013198183849453926\n",
      "learning_rate: 1.1376904456784848e-05\n",
      "epoch: 8.658692866296455\n",
      "\n",
      "\n",
      "Step: 243260\n",
      "loss: 0.0012\n",
      "grad_norm: 0.012472719885408878\n",
      "learning_rate: 1.135317290806398e-05\n",
      "epoch: 8.65940481275808\n",
      "\n",
      "\n",
      "Step: 243280\n",
      "loss: 0.0013\n",
      "grad_norm: 0.023395081982016563\n",
      "learning_rate: 1.132944135934311e-05\n",
      "epoch: 8.660116759219706\n",
      "\n",
      "\n",
      "Step: 243300\n",
      "loss: 0.001\n",
      "grad_norm: 0.010952139273285866\n",
      "learning_rate: 1.130570981062224e-05\n",
      "epoch: 8.660828705681332\n",
      "\n",
      "\n",
      "Step: 243320\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04846017062664032\n",
      "learning_rate: 1.128197826190137e-05\n",
      "epoch: 8.661540652142959\n",
      "\n",
      "\n",
      "Step: 243340\n",
      "loss: 0.0011\n",
      "grad_norm: 0.06091056764125824\n",
      "learning_rate: 1.12582467131805e-05\n",
      "epoch: 8.662252598604585\n",
      "\n",
      "\n",
      "Step: 243360\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03010118193924427\n",
      "learning_rate: 1.123451516445963e-05\n",
      "epoch: 8.662964545066211\n",
      "\n",
      "\n",
      "Step: 243380\n",
      "loss: 0.0014\n",
      "grad_norm: 0.01837906986474991\n",
      "learning_rate: 1.1210783615738762e-05\n",
      "epoch: 8.663676491527838\n",
      "\n",
      "\n",
      "Step: 243400\n",
      "loss: 0.0009\n",
      "grad_norm: 0.013901873491704464\n",
      "learning_rate: 1.1187052067017893e-05\n",
      "epoch: 8.664388437989464\n",
      "\n",
      "\n",
      "Step: 243420\n",
      "loss: 0.0005\n",
      "grad_norm: 0.025254851207137108\n",
      "learning_rate: 1.1163320518297023e-05\n",
      "epoch: 8.665100384451089\n",
      "\n",
      "\n",
      "Step: 243440\n",
      "loss: 0.0014\n",
      "grad_norm: 0.008074994198977947\n",
      "learning_rate: 1.1139588969576153e-05\n",
      "epoch: 8.665812330912715\n",
      "\n",
      "\n",
      "Step: 243460\n",
      "loss: 0.0012\n",
      "grad_norm: 0.011463150382041931\n",
      "learning_rate: 1.1115857420855283e-05\n",
      "epoch: 8.666524277374341\n",
      "\n",
      "\n",
      "Step: 243480\n",
      "loss: 0.0013\n",
      "grad_norm: 0.032556042075157166\n",
      "learning_rate: 1.1092125872134413e-05\n",
      "epoch: 8.667236223835967\n",
      "\n",
      "\n",
      "Step: 243500\n",
      "loss: 0.001\n",
      "grad_norm: 0.01830378733575344\n",
      "learning_rate: 1.1068394323413547e-05\n",
      "epoch: 8.667948170297594\n",
      "\n",
      "\n",
      "Step: 243520\n",
      "loss: 0.0011\n",
      "grad_norm: 0.017123671248555183\n",
      "learning_rate: 1.1044662774692677e-05\n",
      "epoch: 8.66866011675922\n",
      "\n",
      "\n",
      "Step: 243540\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010977479629218578\n",
      "learning_rate: 1.1020931225971807e-05\n",
      "epoch: 8.669372063220846\n",
      "\n",
      "\n",
      "Step: 243560\n",
      "loss: 0.0013\n",
      "grad_norm: 0.026353998109698296\n",
      "learning_rate: 1.0997199677250937e-05\n",
      "epoch: 8.670084009682473\n",
      "\n",
      "\n",
      "Step: 243580\n",
      "loss: 0.0011\n",
      "grad_norm: 0.015549573116004467\n",
      "learning_rate: 1.0973468128530067e-05\n",
      "epoch: 8.670795956144097\n",
      "\n",
      "\n",
      "Step: 243600\n",
      "loss: 0.0014\n",
      "grad_norm: 0.012456700205802917\n",
      "learning_rate: 1.0949736579809197e-05\n",
      "epoch: 8.671507902605724\n",
      "\n",
      "\n",
      "Step: 243620\n",
      "loss: 0.0007\n",
      "grad_norm: 0.004254561848938465\n",
      "learning_rate: 1.0926005031088329e-05\n",
      "epoch: 8.67221984906735\n",
      "\n",
      "\n",
      "Step: 243640\n",
      "loss: 0.001\n",
      "grad_norm: 0.01371315773576498\n",
      "learning_rate: 1.0902273482367459e-05\n",
      "epoch: 8.672931795528976\n",
      "\n",
      "\n",
      "Step: 243660\n",
      "loss: 0.0009\n",
      "grad_norm: 0.024460285902023315\n",
      "learning_rate: 1.0878541933646589e-05\n",
      "epoch: 8.673643741990603\n",
      "\n",
      "\n",
      "Step: 243680\n",
      "loss: 0.0012\n",
      "grad_norm: 0.032742083072662354\n",
      "learning_rate: 1.0854810384925719e-05\n",
      "epoch: 8.674355688452229\n",
      "\n",
      "\n",
      "Step: 243700\n",
      "loss: 0.001\n",
      "grad_norm: 0.007517004385590553\n",
      "learning_rate: 1.083107883620485e-05\n",
      "epoch: 8.675067634913855\n",
      "\n",
      "\n",
      "Step: 243720\n",
      "loss: 0.0008\n",
      "grad_norm: 0.039239998906850815\n",
      "learning_rate: 1.0807347287483981e-05\n",
      "epoch: 8.67577958137548\n",
      "\n",
      "\n",
      "Step: 243740\n",
      "loss: 0.0007\n",
      "grad_norm: 0.003356663743034005\n",
      "learning_rate: 1.0783615738763111e-05\n",
      "epoch: 8.676491527837106\n",
      "\n",
      "\n",
      "Step: 243760\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012554427608847618\n",
      "learning_rate: 1.0759884190042241e-05\n",
      "epoch: 8.677203474298732\n",
      "\n",
      "\n",
      "Step: 243780\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0067660450004041195\n",
      "learning_rate: 1.0736152641321371e-05\n",
      "epoch: 8.677915420760359\n",
      "\n",
      "\n",
      "Step: 243800\n",
      "loss: 0.0011\n",
      "grad_norm: 0.018682807683944702\n",
      "learning_rate: 1.0712421092600501e-05\n",
      "epoch: 8.678627367221985\n",
      "\n",
      "\n",
      "Step: 243820\n",
      "loss: 0.0006\n",
      "grad_norm: 0.015820849686861038\n",
      "learning_rate: 1.0688689543879632e-05\n",
      "epoch: 8.679339313683611\n",
      "\n",
      "\n",
      "Step: 243840\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0065468791872262955\n",
      "learning_rate: 1.0664957995158763e-05\n",
      "epoch: 8.680051260145238\n",
      "\n",
      "\n",
      "Step: 243860\n",
      "loss: 0.0008\n",
      "grad_norm: 0.040506187826395035\n",
      "learning_rate: 1.0641226446437893e-05\n",
      "epoch: 8.680763206606864\n",
      "\n",
      "\n",
      "Step: 243880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0271704513579607\n",
      "learning_rate: 1.0617494897717024e-05\n",
      "epoch: 8.68147515306849\n",
      "\n",
      "\n",
      "Step: 243900\n",
      "loss: 0.0011\n",
      "grad_norm: 0.033422235399484634\n",
      "learning_rate: 1.0593763348996154e-05\n",
      "epoch: 8.682187099530115\n",
      "\n",
      "\n",
      "Step: 243920\n",
      "loss: 0.0014\n",
      "grad_norm: 0.027172638103365898\n",
      "learning_rate: 1.0570031800275284e-05\n",
      "epoch: 8.682899045991741\n",
      "\n",
      "\n",
      "Step: 243940\n",
      "loss: 0.001\n",
      "grad_norm: 0.09631510078907013\n",
      "learning_rate: 1.0546300251554414e-05\n",
      "epoch: 8.683610992453367\n",
      "\n",
      "\n",
      "Step: 243960\n",
      "loss: 0.0016\n",
      "grad_norm: 0.012222935445606709\n",
      "learning_rate: 1.0522568702833547e-05\n",
      "epoch: 8.684322938914994\n",
      "\n",
      "\n",
      "Step: 243980\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0094724390655756\n",
      "learning_rate: 1.0498837154112677e-05\n",
      "epoch: 8.68503488537662\n",
      "\n",
      "\n",
      "Step: 244000\n",
      "loss: 0.0013\n",
      "grad_norm: 0.006313767284154892\n",
      "learning_rate: 1.0475105605391808e-05\n",
      "epoch: 8.685746831838246\n",
      "\n",
      "\n",
      "Step: 244020\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00882072001695633\n",
      "learning_rate: 1.0451374056670938e-05\n",
      "epoch: 8.686458778299873\n",
      "\n",
      "\n",
      "Step: 244040\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0022552339360117912\n",
      "learning_rate: 1.0427642507950068e-05\n",
      "epoch: 8.687170724761497\n",
      "\n",
      "\n",
      "Step: 244060\n",
      "loss: 0.0014\n",
      "grad_norm: 0.009644366800785065\n",
      "learning_rate: 1.0403910959229198e-05\n",
      "epoch: 8.687882671223123\n",
      "\n",
      "\n",
      "Step: 244080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03891802206635475\n",
      "learning_rate: 1.038017941050833e-05\n",
      "epoch: 8.68859461768475\n",
      "\n",
      "\n",
      "Step: 244100\n",
      "loss: 0.0008\n",
      "grad_norm: 0.000425261736381799\n",
      "learning_rate: 1.035644786178746e-05\n",
      "epoch: 8.689306564146376\n",
      "\n",
      "\n",
      "Step: 244120\n",
      "loss: 0.0007\n",
      "grad_norm: 0.02307122014462948\n",
      "learning_rate: 1.033271631306659e-05\n",
      "epoch: 8.690018510608002\n",
      "\n",
      "\n",
      "Step: 244140\n",
      "loss: 0.001\n",
      "grad_norm: 0.01633187010884285\n",
      "learning_rate: 1.030898476434572e-05\n",
      "epoch: 8.690730457069629\n",
      "\n",
      "\n",
      "Step: 244160\n",
      "loss: 0.0009\n",
      "grad_norm: 0.033395152539014816\n",
      "learning_rate: 1.028525321562485e-05\n",
      "epoch: 8.691442403531255\n",
      "\n",
      "\n",
      "Step: 244180\n",
      "loss: 0.001\n",
      "grad_norm: 0.0036020255647599697\n",
      "learning_rate: 1.026152166690398e-05\n",
      "epoch: 8.692154349992881\n",
      "\n",
      "\n",
      "Step: 244200\n",
      "loss: 0.001\n",
      "grad_norm: 0.006782745011150837\n",
      "learning_rate: 1.0237790118183112e-05\n",
      "epoch: 8.692866296454506\n",
      "\n",
      "\n",
      "Step: 244220\n",
      "loss: 0.0014\n",
      "grad_norm: 0.023594945669174194\n",
      "learning_rate: 1.0214058569462242e-05\n",
      "epoch: 8.693578242916132\n",
      "\n",
      "\n",
      "Step: 244240\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04253638908267021\n",
      "learning_rate: 1.0190327020741372e-05\n",
      "epoch: 8.694290189377758\n",
      "\n",
      "\n",
      "Step: 244260\n",
      "loss: 0.0011\n",
      "grad_norm: 0.013822000473737717\n",
      "learning_rate: 1.0166595472020502e-05\n",
      "epoch: 8.695002135839385\n",
      "\n",
      "\n",
      "Step: 244280\n",
      "loss: 0.0012\n",
      "grad_norm: 0.025735944509506226\n",
      "learning_rate: 1.0142863923299632e-05\n",
      "epoch: 8.695714082301011\n",
      "\n",
      "\n",
      "Step: 244300\n",
      "loss: 0.0009\n",
      "grad_norm: 0.006376159377396107\n",
      "learning_rate: 1.0119132374578766e-05\n",
      "epoch: 8.696426028762637\n",
      "\n",
      "\n",
      "Step: 244320\n",
      "loss: 0.001\n",
      "grad_norm: 0.005325367208570242\n",
      "learning_rate: 1.0095400825857896e-05\n",
      "epoch: 8.697137975224264\n",
      "\n",
      "\n",
      "Step: 244340\n",
      "loss: 0.0008\n",
      "grad_norm: 0.013324360363185406\n",
      "learning_rate: 1.0071669277137026e-05\n",
      "epoch: 8.69784992168589\n",
      "\n",
      "\n",
      "Step: 244360\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04406784474849701\n",
      "learning_rate: 1.0047937728416156e-05\n",
      "epoch: 8.698561868147515\n",
      "\n",
      "\n",
      "Step: 244380\n",
      "loss: 0.0008\n",
      "grad_norm: 0.04669394716620445\n",
      "learning_rate: 1.0024206179695286e-05\n",
      "epoch: 8.69927381460914\n",
      "\n",
      "\n",
      "Step: 244400\n",
      "loss: 0.0015\n",
      "grad_norm: 0.019847530871629715\n",
      "learning_rate: 1.0000474630974416e-05\n",
      "epoch: 8.699985761070767\n",
      "\n",
      "\n",
      "Step: 244420\n",
      "loss: 0.0016\n",
      "grad_norm: 0.021686717867851257\n",
      "learning_rate: 9.976743082253548e-06\n",
      "epoch: 8.700697707532393\n",
      "\n",
      "\n",
      "Step: 244440\n",
      "loss: 0.0014\n",
      "grad_norm: 0.13348835706710815\n",
      "learning_rate: 9.953011533532678e-06\n",
      "epoch: 8.70140965399402\n",
      "\n",
      "\n",
      "Step: 244460\n",
      "loss: 0.0013\n",
      "grad_norm: 0.00801108218729496\n",
      "learning_rate: 9.929279984811808e-06\n",
      "epoch: 8.702121600455646\n",
      "\n",
      "\n",
      "Step: 244480\n",
      "loss: 0.0016\n",
      "grad_norm: 0.01739206723868847\n",
      "learning_rate: 9.905548436090939e-06\n",
      "epoch: 8.702833546917272\n",
      "\n",
      "\n",
      "Step: 244500\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03212914988398552\n",
      "learning_rate: 9.881816887370069e-06\n",
      "epoch: 8.703545493378899\n",
      "\n",
      "\n",
      "Step: 244520\n",
      "loss: 0.0012\n",
      "grad_norm: 0.00993670430034399\n",
      "learning_rate: 9.858085338649199e-06\n",
      "epoch: 8.704257439840523\n",
      "\n",
      "\n",
      "Step: 244540\n",
      "loss: 0.0016\n",
      "grad_norm: 0.007683662232011557\n",
      "learning_rate: 9.83435378992833e-06\n",
      "epoch: 8.70496938630215\n",
      "\n",
      "\n",
      "Step: 244560\n",
      "loss: 0.0015\n",
      "grad_norm: 0.03598158061504364\n",
      "learning_rate: 9.81062224120746e-06\n",
      "epoch: 8.705681332763776\n",
      "\n",
      "\n",
      "Step: 244580\n",
      "loss: 0.0015\n",
      "grad_norm: 0.012686425819993019\n",
      "learning_rate: 9.78689069248659e-06\n",
      "epoch: 8.706393279225402\n",
      "\n",
      "\n",
      "Step: 244600\n",
      "loss: 0.0013\n",
      "grad_norm: 0.040153682231903076\n",
      "learning_rate: 9.76315914376572e-06\n",
      "epoch: 8.707105225687028\n",
      "\n",
      "\n",
      "Step: 244620\n",
      "loss: 0.0007\n",
      "grad_norm: 0.02245095744729042\n",
      "learning_rate: 9.739427595044851e-06\n",
      "epoch: 8.707817172148655\n",
      "\n",
      "\n",
      "Step: 244640\n",
      "loss: 0.0012\n",
      "grad_norm: 0.013868935406208038\n",
      "learning_rate: 9.715696046323981e-06\n",
      "epoch: 8.708529118610281\n",
      "\n",
      "\n",
      "Step: 244660\n",
      "loss: 0.0013\n",
      "grad_norm: 0.05036628618836403\n",
      "learning_rate: 9.691964497603113e-06\n",
      "epoch: 8.709241065071907\n",
      "\n",
      "\n",
      "Step: 244680\n",
      "loss: 0.001\n",
      "grad_norm: 0.020340217277407646\n",
      "learning_rate: 9.668232948882243e-06\n",
      "epoch: 8.709953011533532\n",
      "\n",
      "\n",
      "Step: 244700\n",
      "loss: 0.0012\n",
      "grad_norm: 0.016720101237297058\n",
      "learning_rate: 9.644501400161373e-06\n",
      "epoch: 8.710664957995158\n",
      "\n",
      "\n",
      "Step: 244720\n",
      "loss: 0.0013\n",
      "grad_norm: 0.011464336887001991\n",
      "learning_rate: 9.620769851440503e-06\n",
      "epoch: 8.711376904456785\n",
      "\n",
      "\n",
      "Step: 244740\n",
      "loss: 0.0016\n",
      "grad_norm: 0.0416276641190052\n",
      "learning_rate: 9.597038302719633e-06\n",
      "epoch: 8.712088850918411\n",
      "\n",
      "\n",
      "Step: 244760\n",
      "loss: 0.0014\n",
      "grad_norm: 0.005656713154166937\n",
      "learning_rate: 9.573306753998763e-06\n",
      "epoch: 8.712800797380037\n",
      "\n",
      "\n",
      "Step: 244780\n",
      "loss: 0.001\n",
      "grad_norm: 0.017945611849427223\n",
      "learning_rate: 9.549575205277897e-06\n",
      "epoch: 8.713512743841664\n",
      "\n",
      "\n",
      "Step: 244800\n",
      "loss: 0.0012\n",
      "grad_norm: 0.012900291010737419\n",
      "learning_rate: 9.525843656557027e-06\n",
      "epoch: 8.71422469030329\n",
      "\n",
      "\n",
      "Step: 244820\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03145322948694229\n",
      "learning_rate: 9.502112107836157e-06\n",
      "epoch: 8.714936636764914\n",
      "\n",
      "\n",
      "Step: 244840\n",
      "loss: 0.0011\n",
      "grad_norm: 0.014987700618803501\n",
      "learning_rate: 9.478380559115287e-06\n",
      "epoch: 8.71564858322654\n",
      "\n",
      "\n",
      "Step: 244860\n",
      "loss: 0.0014\n",
      "grad_norm: 0.04028528183698654\n",
      "learning_rate: 9.454649010394417e-06\n",
      "epoch: 8.716360529688167\n",
      "\n",
      "\n",
      "Step: 244880\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0377124585211277\n",
      "learning_rate: 9.430917461673549e-06\n",
      "epoch: 8.717072476149793\n",
      "\n",
      "\n",
      "Step: 244900\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04429899901151657\n",
      "learning_rate: 9.40718591295268e-06\n",
      "epoch: 8.71778442261142\n",
      "\n",
      "\n",
      "Step: 244920\n",
      "loss: 0.0009\n",
      "grad_norm: 0.007926041260361671\n",
      "learning_rate: 9.38345436423181e-06\n",
      "epoch: 8.718496369073046\n",
      "\n",
      "\n",
      "Step: 244940\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004409056156873703\n",
      "learning_rate: 9.35972281551094e-06\n",
      "epoch: 8.719208315534672\n",
      "\n",
      "\n",
      "Step: 244960\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04133889079093933\n",
      "learning_rate: 9.33599126679007e-06\n",
      "epoch: 8.719920261996299\n",
      "\n",
      "\n",
      "Step: 244980\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01090944092720747\n",
      "learning_rate: 9.312259718069201e-06\n",
      "epoch: 8.720632208457925\n",
      "\n",
      "\n",
      "Step: 245000\n",
      "loss: 0.0013\n",
      "grad_norm: 0.003130742348730564\n",
      "learning_rate: 9.288528169348331e-06\n",
      "epoch: 8.72134415491955\n",
      "\n",
      "\n",
      "Step: 245020\n",
      "loss: 0.001\n",
      "grad_norm: 0.03409124165773392\n",
      "learning_rate: 9.264796620627461e-06\n",
      "epoch: 8.722056101381176\n",
      "\n",
      "\n",
      "Step: 245040\n",
      "loss: 0.0008\n",
      "grad_norm: 0.00871104933321476\n",
      "learning_rate: 9.241065071906592e-06\n",
      "epoch: 8.722768047842802\n",
      "\n",
      "\n",
      "Step: 245060\n",
      "loss: 0.001\n",
      "grad_norm: 0.0015497492859140038\n",
      "learning_rate: 9.217333523185722e-06\n",
      "epoch: 8.723479994304428\n",
      "\n",
      "\n",
      "Step: 245080\n",
      "loss: 0.0007\n",
      "grad_norm: 0.003679974703118205\n",
      "learning_rate: 9.193601974464852e-06\n",
      "epoch: 8.724191940766055\n",
      "\n",
      "\n",
      "Step: 245100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.012065321207046509\n",
      "learning_rate: 9.169870425743984e-06\n",
      "epoch: 8.724903887227681\n",
      "\n",
      "\n",
      "Step: 245120\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0163233894854784\n",
      "learning_rate: 9.146138877023114e-06\n",
      "epoch: 8.725615833689307\n",
      "\n",
      "\n",
      "Step: 245140\n",
      "loss: 0.0008\n",
      "grad_norm: 0.013131449930369854\n",
      "learning_rate: 9.122407328302244e-06\n",
      "epoch: 8.726327780150932\n",
      "\n",
      "\n",
      "Step: 245160\n",
      "loss: 0.001\n",
      "grad_norm: 0.07079263031482697\n",
      "learning_rate: 9.098675779581376e-06\n",
      "epoch: 8.727039726612558\n",
      "\n",
      "\n",
      "Step: 245180\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0035469536669552326\n",
      "learning_rate: 9.074944230860506e-06\n",
      "epoch: 8.727751673074184\n",
      "\n",
      "\n",
      "Step: 245200\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010752958245575428\n",
      "learning_rate: 9.051212682139636e-06\n",
      "epoch: 8.72846361953581\n",
      "\n",
      "\n",
      "Step: 245220\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0011379814241081476\n",
      "learning_rate: 9.027481133418766e-06\n",
      "epoch: 8.729175565997437\n",
      "\n",
      "\n",
      "Step: 245240\n",
      "loss: 0.0007\n",
      "grad_norm: 0.017986973747611046\n",
      "learning_rate: 9.003749584697896e-06\n",
      "epoch: 8.729887512459063\n",
      "\n",
      "\n",
      "Step: 245260\n",
      "loss: 0.0012\n",
      "grad_norm: 0.014525359496474266\n",
      "learning_rate: 8.980018035977028e-06\n",
      "epoch: 8.73059945892069\n",
      "\n",
      "\n",
      "Step: 245280\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03057139925658703\n",
      "learning_rate: 8.956286487256158e-06\n",
      "epoch: 8.731311405382316\n",
      "\n",
      "\n",
      "Step: 245300\n",
      "loss: 0.0013\n",
      "grad_norm: 0.005861623212695122\n",
      "learning_rate: 8.932554938535288e-06\n",
      "epoch: 8.73202335184394\n",
      "\n",
      "\n",
      "Step: 245320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02844133973121643\n",
      "learning_rate: 8.908823389814418e-06\n",
      "epoch: 8.732735298305567\n",
      "\n",
      "\n",
      "Step: 245340\n",
      "loss: 0.0017\n",
      "grad_norm: 0.01595013216137886\n",
      "learning_rate: 8.885091841093548e-06\n",
      "epoch: 8.733447244767193\n",
      "\n",
      "\n",
      "Step: 245360\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01966843754053116\n",
      "learning_rate: 8.861360292372678e-06\n",
      "epoch: 8.73415919122882\n",
      "\n",
      "\n",
      "Step: 245380\n",
      "loss: 0.0017\n",
      "grad_norm: 0.03721662610769272\n",
      "learning_rate: 8.83762874365181e-06\n",
      "epoch: 8.734871137690446\n",
      "\n",
      "\n",
      "Step: 245400\n",
      "loss: 0.0011\n",
      "grad_norm: 0.017001517117023468\n",
      "learning_rate: 8.81389719493094e-06\n",
      "epoch: 8.735583084152072\n",
      "\n",
      "\n",
      "Step: 245420\n",
      "loss: 0.0013\n",
      "grad_norm: 0.008141715079545975\n",
      "learning_rate: 8.79016564621007e-06\n",
      "epoch: 8.736295030613698\n",
      "\n",
      "\n",
      "Step: 245440\n",
      "loss: 0.0015\n",
      "grad_norm: 0.01360999420285225\n",
      "learning_rate: 8.766434097489202e-06\n",
      "epoch: 8.737006977075325\n",
      "\n",
      "\n",
      "Step: 245460\n",
      "loss: 0.0011\n",
      "grad_norm: 0.016351420432329178\n",
      "learning_rate: 8.742702548768332e-06\n",
      "epoch: 8.73771892353695\n",
      "\n",
      "\n",
      "Step: 245480\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01848079450428486\n",
      "learning_rate: 8.718971000047462e-06\n",
      "epoch: 8.738430869998576\n",
      "\n",
      "\n",
      "Step: 245500\n",
      "loss: 0.001\n",
      "grad_norm: 0.04728463664650917\n",
      "learning_rate: 8.695239451326592e-06\n",
      "epoch: 8.739142816460202\n",
      "\n",
      "\n",
      "Step: 245520\n",
      "loss: 0.0015\n",
      "grad_norm: 0.05796361714601517\n",
      "learning_rate: 8.671507902605723e-06\n",
      "epoch: 8.739854762921828\n",
      "\n",
      "\n",
      "Step: 245540\n",
      "loss: 0.0008\n",
      "grad_norm: 0.030848488211631775\n",
      "learning_rate: 8.647776353884853e-06\n",
      "epoch: 8.740566709383454\n",
      "\n",
      "\n",
      "Step: 245560\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04431711509823799\n",
      "learning_rate: 8.624044805163984e-06\n",
      "epoch: 8.74127865584508\n",
      "\n",
      "\n",
      "Step: 245580\n",
      "loss: 0.0016\n",
      "grad_norm: 0.005207943730056286\n",
      "learning_rate: 8.600313256443115e-06\n",
      "epoch: 8.741990602306707\n",
      "\n",
      "\n",
      "Step: 245600\n",
      "loss: 0.001\n",
      "grad_norm: 0.009135568514466286\n",
      "learning_rate: 8.576581707722245e-06\n",
      "epoch: 8.742702548768333\n",
      "\n",
      "\n",
      "Step: 245620\n",
      "loss: 0.0009\n",
      "grad_norm: 0.020371126011013985\n",
      "learning_rate: 8.552850159001376e-06\n",
      "epoch: 8.743414495229958\n",
      "\n",
      "\n",
      "Step: 245640\n",
      "loss: 0.0013\n",
      "grad_norm: 0.013309475965797901\n",
      "learning_rate: 8.529118610280507e-06\n",
      "epoch: 8.744126441691584\n",
      "\n",
      "\n",
      "Step: 245660\n",
      "loss: 0.0009\n",
      "grad_norm: 0.030450455844402313\n",
      "learning_rate: 8.505387061559637e-06\n",
      "epoch: 8.74483838815321\n",
      "\n",
      "\n",
      "Step: 245680\n",
      "loss: 0.0014\n",
      "grad_norm: 0.022516394034028053\n",
      "learning_rate: 8.481655512838767e-06\n",
      "epoch: 8.745550334614837\n",
      "\n",
      "\n",
      "Step: 245700\n",
      "loss: 0.0013\n",
      "grad_norm: 0.00914593692868948\n",
      "learning_rate: 8.457923964117897e-06\n",
      "epoch: 8.746262281076463\n",
      "\n",
      "\n",
      "Step: 245720\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03732668235898018\n",
      "learning_rate: 8.434192415397027e-06\n",
      "epoch: 8.74697422753809\n",
      "\n",
      "\n",
      "Step: 245740\n",
      "loss: 0.001\n",
      "grad_norm: 0.02345425821840763\n",
      "learning_rate: 8.410460866676159e-06\n",
      "epoch: 8.747686173999716\n",
      "\n",
      "\n",
      "Step: 245760\n",
      "loss: 0.001\n",
      "grad_norm: 0.015872497111558914\n",
      "learning_rate: 8.386729317955289e-06\n",
      "epoch: 8.748398120461342\n",
      "\n",
      "\n",
      "Step: 245780\n",
      "loss: 0.0017\n",
      "grad_norm: 0.04139654338359833\n",
      "learning_rate: 8.36299776923442e-06\n",
      "epoch: 8.749110066922967\n",
      "\n",
      "\n",
      "Step: 245800\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0024845723528414965\n",
      "learning_rate: 8.33926622051355e-06\n",
      "epoch: 8.749822013384593\n",
      "\n",
      "\n",
      "Step: 245820\n",
      "loss: 0.001\n",
      "grad_norm: 0.010082134045660496\n",
      "learning_rate: 8.315534671792681e-06\n",
      "epoch: 8.75053395984622\n",
      "\n",
      "\n",
      "Step: 245840\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03388116508722305\n",
      "learning_rate: 8.291803123071811e-06\n",
      "epoch: 8.751245906307846\n",
      "\n",
      "\n",
      "Step: 245860\n",
      "loss: 0.001\n",
      "grad_norm: 0.0026105339638888836\n",
      "learning_rate: 8.268071574350941e-06\n",
      "epoch: 8.751957852769472\n",
      "\n",
      "\n",
      "Step: 245880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012330995872616768\n",
      "learning_rate: 8.244340025630071e-06\n",
      "epoch: 8.752669799231098\n",
      "\n",
      "\n",
      "Step: 245900\n",
      "loss: 0.0011\n",
      "grad_norm: 0.006928750313818455\n",
      "learning_rate: 8.220608476909203e-06\n",
      "epoch: 8.753381745692725\n",
      "\n",
      "\n",
      "Step: 245920\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02913704514503479\n",
      "learning_rate: 8.196876928188333e-06\n",
      "epoch: 8.754093692154349\n",
      "\n",
      "\n",
      "Step: 245940\n",
      "loss: 0.0014\n",
      "grad_norm: 0.02332797646522522\n",
      "learning_rate: 8.173145379467463e-06\n",
      "epoch: 8.754805638615975\n",
      "\n",
      "\n",
      "Step: 245960\n",
      "loss: 0.0008\n",
      "grad_norm: 0.028463061898946762\n",
      "learning_rate: 8.149413830746595e-06\n",
      "epoch: 8.755517585077602\n",
      "\n",
      "\n",
      "Step: 245980\n",
      "loss: 0.001\n",
      "grad_norm: 0.04655256122350693\n",
      "learning_rate: 8.125682282025725e-06\n",
      "epoch: 8.756229531539228\n",
      "\n",
      "\n",
      "Step: 246000\n",
      "loss: 0.0012\n",
      "grad_norm: 0.028809338808059692\n",
      "learning_rate: 8.101950733304855e-06\n",
      "epoch: 8.756941478000854\n",
      "\n",
      "\n",
      "Step: 246020\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012924732640385628\n",
      "learning_rate: 8.078219184583985e-06\n",
      "epoch: 8.75765342446248\n",
      "\n",
      "\n",
      "Step: 246040\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0580357201397419\n",
      "learning_rate: 8.054487635863115e-06\n",
      "epoch: 8.758365370924107\n",
      "\n",
      "\n",
      "Step: 246060\n",
      "loss: 0.001\n",
      "grad_norm: 0.0027551886159926653\n",
      "learning_rate: 8.030756087142246e-06\n",
      "epoch: 8.759077317385733\n",
      "\n",
      "\n",
      "Step: 246080\n",
      "loss: 0.0012\n",
      "grad_norm: 0.029113860800862312\n",
      "learning_rate: 8.007024538421377e-06\n",
      "epoch: 8.75978926384736\n",
      "\n",
      "\n",
      "Step: 246100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.018632812425494194\n",
      "learning_rate: 7.983292989700507e-06\n",
      "epoch: 8.760501210308984\n",
      "\n",
      "\n",
      "Step: 246120\n",
      "loss: 0.001\n",
      "grad_norm: 0.050338804721832275\n",
      "learning_rate: 7.959561440979638e-06\n",
      "epoch: 8.76121315677061\n",
      "\n",
      "\n",
      "Step: 246140\n",
      "loss: 0.0014\n",
      "grad_norm: 0.019662631675601006\n",
      "learning_rate: 7.935829892258768e-06\n",
      "epoch: 8.761925103232237\n",
      "\n",
      "\n",
      "Step: 246160\n",
      "loss: 0.001\n",
      "grad_norm: 0.004990757443010807\n",
      "learning_rate: 7.912098343537898e-06\n",
      "epoch: 8.762637049693863\n",
      "\n",
      "\n",
      "Step: 246180\n",
      "loss: 0.0013\n",
      "grad_norm: 0.011848789639770985\n",
      "learning_rate: 7.888366794817028e-06\n",
      "epoch: 8.76334899615549\n",
      "\n",
      "\n",
      "Step: 246200\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01521665696054697\n",
      "learning_rate: 7.86463524609616e-06\n",
      "epoch: 8.764060942617116\n",
      "\n",
      "\n",
      "Step: 246220\n",
      "loss: 0.0007\n",
      "grad_norm: 0.001727018621750176\n",
      "learning_rate: 7.84090369737529e-06\n",
      "epoch: 8.764772889078742\n",
      "\n",
      "\n",
      "Step: 246240\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03397688642144203\n",
      "learning_rate: 7.81717214865442e-06\n",
      "epoch: 8.765484835540367\n",
      "\n",
      "\n",
      "Step: 246260\n",
      "loss: 0.0011\n",
      "grad_norm: 0.039363764226436615\n",
      "learning_rate: 7.793440599933552e-06\n",
      "epoch: 8.766196782001993\n",
      "\n",
      "\n",
      "Step: 246280\n",
      "loss: 0.0009\n",
      "grad_norm: 0.026385637000203133\n",
      "learning_rate: 7.769709051212682e-06\n",
      "epoch: 8.76690872846362\n",
      "\n",
      "\n",
      "Step: 246300\n",
      "loss: 0.0011\n",
      "grad_norm: 0.039542995393276215\n",
      "learning_rate: 7.745977502491812e-06\n",
      "epoch: 8.767620674925245\n",
      "\n",
      "\n",
      "Step: 246320\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0188821479678154\n",
      "learning_rate: 7.722245953770942e-06\n",
      "epoch: 8.768332621386872\n",
      "\n",
      "\n",
      "Step: 246340\n",
      "loss: 0.0013\n",
      "grad_norm: 0.025430088862776756\n",
      "learning_rate: 7.698514405050072e-06\n",
      "epoch: 8.769044567848498\n",
      "\n",
      "\n",
      "Step: 246360\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004752288106828928\n",
      "learning_rate: 7.674782856329204e-06\n",
      "epoch: 8.769756514310124\n",
      "\n",
      "\n",
      "Step: 246380\n",
      "loss: 0.0007\n",
      "grad_norm: 0.025559324771165848\n",
      "learning_rate: 7.651051307608334e-06\n",
      "epoch: 8.77046846077175\n",
      "\n",
      "\n",
      "Step: 246400\n",
      "loss: 0.0017\n",
      "grad_norm: 0.016603125259280205\n",
      "learning_rate: 7.627319758887464e-06\n",
      "epoch: 8.771180407233375\n",
      "\n",
      "\n",
      "Step: 246420\n",
      "loss: 0.0013\n",
      "grad_norm: 0.018036404624581337\n",
      "learning_rate: 7.603588210166595e-06\n",
      "epoch: 8.771892353695002\n",
      "\n",
      "\n",
      "Step: 246440\n",
      "loss: 0.001\n",
      "grad_norm: 0.02380244806408882\n",
      "learning_rate: 7.579856661445725e-06\n",
      "epoch: 8.772604300156628\n",
      "\n",
      "\n",
      "Step: 246460\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0283809807151556\n",
      "learning_rate: 7.556125112724855e-06\n",
      "epoch: 8.773316246618254\n",
      "\n",
      "\n",
      "Step: 246480\n",
      "loss: 0.0012\n",
      "grad_norm: 0.006234052125364542\n",
      "learning_rate: 7.532393564003987e-06\n",
      "epoch: 8.77402819307988\n",
      "\n",
      "\n",
      "Step: 246500\n",
      "loss: 0.0015\n",
      "grad_norm: 0.039617788046598434\n",
      "learning_rate: 7.508662015283117e-06\n",
      "epoch: 8.774740139541507\n",
      "\n",
      "\n",
      "Step: 246520\n",
      "loss: 0.0016\n",
      "grad_norm: 0.021283181384205818\n",
      "learning_rate: 7.484930466562247e-06\n",
      "epoch: 8.775452086003133\n",
      "\n",
      "\n",
      "Step: 246540\n",
      "loss: 0.0011\n",
      "grad_norm: 0.018686512485146523\n",
      "learning_rate: 7.461198917841378e-06\n",
      "epoch: 8.77616403246476\n",
      "\n",
      "\n",
      "Step: 246560\n",
      "loss: 0.0008\n",
      "grad_norm: 0.009876334108412266\n",
      "learning_rate: 7.437467369120508e-06\n",
      "epoch: 8.776875978926384\n",
      "\n",
      "\n",
      "Step: 246580\n",
      "loss: 0.001\n",
      "grad_norm: 0.03731411695480347\n",
      "learning_rate: 7.413735820399638e-06\n",
      "epoch: 8.77758792538801\n",
      "\n",
      "\n",
      "Step: 246600\n",
      "loss: 0.0009\n",
      "grad_norm: 0.027515782043337822\n",
      "learning_rate: 7.390004271678769e-06\n",
      "epoch: 8.778299871849637\n",
      "\n",
      "\n",
      "Step: 246620\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0224295761436224\n",
      "learning_rate: 7.3662727229578994e-06\n",
      "epoch: 8.779011818311263\n",
      "\n",
      "\n",
      "Step: 246640\n",
      "loss: 0.001\n",
      "grad_norm: 0.015660932287573814\n",
      "learning_rate: 7.3425411742370295e-06\n",
      "epoch: 8.77972376477289\n",
      "\n",
      "\n",
      "Step: 246660\n",
      "loss: 0.001\n",
      "grad_norm: 0.01563175395131111\n",
      "learning_rate: 7.3188096255161605e-06\n",
      "epoch: 8.780435711234515\n",
      "\n",
      "\n",
      "Step: 246680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.006837308406829834\n",
      "learning_rate: 7.295078076795291e-06\n",
      "epoch: 8.781147657696142\n",
      "\n",
      "\n",
      "Step: 246700\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03698321804404259\n",
      "learning_rate: 7.271346528074421e-06\n",
      "epoch: 8.781859604157768\n",
      "\n",
      "\n",
      "Step: 246720\n",
      "loss: 0.0014\n",
      "grad_norm: 0.013184229843318462\n",
      "learning_rate: 7.2476149793535525e-06\n",
      "epoch: 8.782571550619393\n",
      "\n",
      "\n",
      "Step: 246740\n",
      "loss: 0.0011\n",
      "grad_norm: 0.028283510357141495\n",
      "learning_rate: 7.223883430632683e-06\n",
      "epoch: 8.783283497081019\n",
      "\n",
      "\n",
      "Step: 246760\n",
      "loss: 0.0012\n",
      "grad_norm: 0.007727104239165783\n",
      "learning_rate: 7.200151881911813e-06\n",
      "epoch: 8.783995443542645\n",
      "\n",
      "\n",
      "Step: 246780\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02842344157397747\n",
      "learning_rate: 7.176420333190944e-06\n",
      "epoch: 8.784707390004272\n",
      "\n",
      "\n",
      "Step: 246800\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04299023374915123\n",
      "learning_rate: 7.152688784470074e-06\n",
      "epoch: 8.785419336465898\n",
      "\n",
      "\n",
      "Step: 246820\n",
      "loss: 0.0016\n",
      "grad_norm: 0.01866893284022808\n",
      "learning_rate: 7.128957235749204e-06\n",
      "epoch: 8.786131282927524\n",
      "\n",
      "\n",
      "Step: 246840\n",
      "loss: 0.0013\n",
      "grad_norm: 0.03537512198090553\n",
      "learning_rate: 7.105225687028335e-06\n",
      "epoch: 8.78684322938915\n",
      "\n",
      "\n",
      "Step: 246860\n",
      "loss: 0.0013\n",
      "grad_norm: 0.004731911234557629\n",
      "learning_rate: 7.081494138307465e-06\n",
      "epoch: 8.787555175850777\n",
      "\n",
      "\n",
      "Step: 246880\n",
      "loss: 0.0004\n",
      "grad_norm: 0.009621385484933853\n",
      "learning_rate: 7.057762589586595e-06\n",
      "epoch: 8.788267122312401\n",
      "\n",
      "\n",
      "Step: 246900\n",
      "loss: 0.0016\n",
      "grad_norm: 0.052565474063158035\n",
      "learning_rate: 7.034031040865727e-06\n",
      "epoch: 8.788979068774028\n",
      "\n",
      "\n",
      "Step: 246920\n",
      "loss: 0.0011\n",
      "grad_norm: 0.004595967009663582\n",
      "learning_rate: 7.010299492144857e-06\n",
      "epoch: 8.789691015235654\n",
      "\n",
      "\n",
      "Step: 246940\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03164885938167572\n",
      "learning_rate: 6.986567943423988e-06\n",
      "epoch: 8.79040296169728\n",
      "\n",
      "\n",
      "Step: 246960\n",
      "loss: 0.0011\n",
      "grad_norm: 0.012751161120831966\n",
      "learning_rate: 6.962836394703118e-06\n",
      "epoch: 8.791114908158907\n",
      "\n",
      "\n",
      "Step: 246980\n",
      "loss: 0.0013\n",
      "grad_norm: 0.038337282836437225\n",
      "learning_rate: 6.939104845982248e-06\n",
      "epoch: 8.791826854620533\n",
      "\n",
      "\n",
      "Step: 247000\n",
      "loss: 0.001\n",
      "grad_norm: 0.024977419525384903\n",
      "learning_rate: 6.915373297261379e-06\n",
      "epoch: 8.79253880108216\n",
      "\n",
      "\n",
      "Step: 247020\n",
      "loss: 0.0019\n",
      "grad_norm: 0.011657935567200184\n",
      "learning_rate: 6.891641748540509e-06\n",
      "epoch: 8.793250747543784\n",
      "\n",
      "\n",
      "Step: 247040\n",
      "loss: 0.0007\n",
      "grad_norm: 0.016590727493166924\n",
      "learning_rate: 6.867910199819639e-06\n",
      "epoch: 8.79396269400541\n",
      "\n",
      "\n",
      "Step: 247060\n",
      "loss: 0.001\n",
      "grad_norm: 0.035265352576971054\n",
      "learning_rate: 6.84417865109877e-06\n",
      "epoch: 8.794674640467036\n",
      "\n",
      "\n",
      "Step: 247080\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010520984418690205\n",
      "learning_rate: 6.8204471023779e-06\n",
      "epoch: 8.795386586928663\n",
      "\n",
      "\n",
      "Step: 247100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.003984639421105385\n",
      "learning_rate: 6.79671555365703e-06\n",
      "epoch: 8.796098533390289\n",
      "\n",
      "\n",
      "Step: 247120\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03167497739195824\n",
      "learning_rate: 6.772984004936162e-06\n",
      "epoch: 8.796810479851915\n",
      "\n",
      "\n",
      "Step: 247140\n",
      "loss: 0.0007\n",
      "grad_norm: 0.02088284306228161\n",
      "learning_rate: 6.749252456215292e-06\n",
      "epoch: 8.797522426313542\n",
      "\n",
      "\n",
      "Step: 247160\n",
      "loss: 0.001\n",
      "grad_norm: 0.03796546906232834\n",
      "learning_rate: 6.725520907494422e-06\n",
      "epoch: 8.798234372775168\n",
      "\n",
      "\n",
      "Step: 247180\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0396050401031971\n",
      "learning_rate: 6.701789358773553e-06\n",
      "epoch: 8.798946319236794\n",
      "\n",
      "\n",
      "Step: 247200\n",
      "loss: 0.0008\n",
      "grad_norm: 0.021490786224603653\n",
      "learning_rate: 6.6780578100526835e-06\n",
      "epoch: 8.799658265698419\n",
      "\n",
      "\n",
      "Step: 247220\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02425430901348591\n",
      "learning_rate: 6.6543262613318136e-06\n",
      "epoch: 8.800370212160045\n",
      "\n",
      "\n",
      "Step: 247240\n",
      "loss: 0.0014\n",
      "grad_norm: 0.05107461288571358\n",
      "learning_rate: 6.6305947126109445e-06\n",
      "epoch: 8.801082158621671\n",
      "\n",
      "\n",
      "Step: 247260\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01847584918141365\n",
      "learning_rate: 6.606863163890075e-06\n",
      "epoch: 8.801794105083298\n",
      "\n",
      "\n",
      "Step: 247280\n",
      "loss: 0.0009\n",
      "grad_norm: 0.03120746649801731\n",
      "learning_rate: 6.583131615169205e-06\n",
      "epoch: 8.802506051544924\n",
      "\n",
      "\n",
      "Step: 247300\n",
      "loss: 0.0015\n",
      "grad_norm: 0.003984171897172928\n",
      "learning_rate: 6.5594000664483365e-06\n",
      "epoch: 8.80321799800655\n",
      "\n",
      "\n",
      "Step: 247320\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02298884280025959\n",
      "learning_rate: 6.535668517727467e-06\n",
      "epoch: 8.803929944468177\n",
      "\n",
      "\n",
      "Step: 247340\n",
      "loss: 0.0008\n",
      "grad_norm: 0.008856618776917458\n",
      "learning_rate: 6.511936969006597e-06\n",
      "epoch: 8.804641890929801\n",
      "\n",
      "\n",
      "Step: 247360\n",
      "loss: 0.0014\n",
      "grad_norm: 0.006468653213232756\n",
      "learning_rate: 6.488205420285728e-06\n",
      "epoch: 8.805353837391428\n",
      "\n",
      "\n",
      "Step: 247380\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0019821145106106997\n",
      "learning_rate: 6.464473871564858e-06\n",
      "epoch: 8.806065783853054\n",
      "\n",
      "\n",
      "Step: 247400\n",
      "loss: 0.001\n",
      "grad_norm: 0.047373104840517044\n",
      "learning_rate: 6.440742322843988e-06\n",
      "epoch: 8.80677773031468\n",
      "\n",
      "\n",
      "Step: 247420\n",
      "loss: 0.0011\n",
      "grad_norm: 0.004730286542326212\n",
      "learning_rate: 6.417010774123119e-06\n",
      "epoch: 8.807489676776306\n",
      "\n",
      "\n",
      "Step: 247440\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004960007965564728\n",
      "learning_rate: 6.393279225402249e-06\n",
      "epoch: 8.808201623237933\n",
      "\n",
      "\n",
      "Step: 247460\n",
      "loss: 0.0014\n",
      "grad_norm: 0.013096227310597897\n",
      "learning_rate: 6.369547676681379e-06\n",
      "epoch: 8.808913569699559\n",
      "\n",
      "\n",
      "Step: 247480\n",
      "loss: 0.0009\n",
      "grad_norm: 0.0014271296095103025\n",
      "learning_rate: 6.34581612796051e-06\n",
      "epoch: 8.809625516161185\n",
      "\n",
      "\n",
      "Step: 247500\n",
      "loss: 0.0016\n",
      "grad_norm: 0.005454686935991049\n",
      "learning_rate: 6.32208457923964e-06\n",
      "epoch: 8.810337462622812\n",
      "\n",
      "\n",
      "Step: 247520\n",
      "loss: 0.0009\n",
      "grad_norm: 0.04044576734304428\n",
      "learning_rate: 6.298353030518772e-06\n",
      "epoch: 8.811049409084436\n",
      "\n",
      "\n",
      "Step: 247540\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03423624113202095\n",
      "learning_rate: 6.274621481797902e-06\n",
      "epoch: 8.811761355546063\n",
      "\n",
      "\n",
      "Step: 247560\n",
      "loss: 0.0015\n",
      "grad_norm: 0.010303482413291931\n",
      "learning_rate: 6.250889933077032e-06\n",
      "epoch: 8.812473302007689\n",
      "\n",
      "\n",
      "Step: 247580\n",
      "loss: 0.0011\n",
      "grad_norm: 0.006780880503356457\n",
      "learning_rate: 6.227158384356163e-06\n",
      "epoch: 8.813185248469315\n",
      "\n",
      "\n",
      "Step: 247600\n",
      "loss: 0.0014\n",
      "grad_norm: 0.00517997657880187\n",
      "learning_rate: 6.203426835635293e-06\n",
      "epoch: 8.813897194930941\n",
      "\n",
      "\n",
      "Step: 247620\n",
      "loss: 0.0014\n",
      "grad_norm: 0.006987219210714102\n",
      "learning_rate: 6.179695286914423e-06\n",
      "epoch: 8.814609141392568\n",
      "\n",
      "\n",
      "Step: 247640\n",
      "loss: 0.0013\n",
      "grad_norm: 0.034918174147605896\n",
      "learning_rate: 6.155963738193554e-06\n",
      "epoch: 8.815321087854194\n",
      "\n",
      "\n",
      "Step: 247660\n",
      "loss: 0.0009\n",
      "grad_norm: 0.015470018610358238\n",
      "learning_rate: 6.132232189472684e-06\n",
      "epoch: 8.816033034315819\n",
      "\n",
      "\n",
      "Step: 247680\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0029386403039097786\n",
      "learning_rate: 6.108500640751814e-06\n",
      "epoch: 8.816744980777445\n",
      "\n",
      "\n",
      "Step: 247700\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0009766111616045237\n",
      "learning_rate: 6.084769092030946e-06\n",
      "epoch: 8.817456927239071\n",
      "\n",
      "\n",
      "Step: 247720\n",
      "loss: 0.0016\n",
      "grad_norm: 0.05143871530890465\n",
      "learning_rate: 6.061037543310076e-06\n",
      "epoch: 8.818168873700698\n",
      "\n",
      "\n",
      "Step: 247740\n",
      "loss: 0.0013\n",
      "grad_norm: 0.01373025681823492\n",
      "learning_rate: 6.037305994589206e-06\n",
      "epoch: 8.818880820162324\n",
      "\n",
      "\n",
      "Step: 247760\n",
      "loss: 0.0006\n",
      "grad_norm: 0.015565667301416397\n",
      "learning_rate: 6.013574445868337e-06\n",
      "epoch: 8.81959276662395\n",
      "\n",
      "\n",
      "Step: 247780\n",
      "loss: 0.001\n",
      "grad_norm: 0.0428287610411644\n",
      "learning_rate: 5.9898428971474675e-06\n",
      "epoch: 8.820304713085577\n",
      "\n",
      "\n",
      "Step: 247800\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009585005231201649\n",
      "learning_rate: 5.9661113484265976e-06\n",
      "epoch: 8.821016659547203\n",
      "\n",
      "\n",
      "Step: 247820\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03770173341035843\n",
      "learning_rate: 5.9423797997057285e-06\n",
      "epoch: 8.821728606008827\n",
      "\n",
      "\n",
      "Step: 247840\n",
      "loss: 0.0009\n",
      "grad_norm: 0.023151814937591553\n",
      "learning_rate: 5.918648250984859e-06\n",
      "epoch: 8.822440552470454\n",
      "\n",
      "\n",
      "Step: 247860\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0034619756042957306\n",
      "learning_rate: 5.894916702263989e-06\n",
      "epoch: 8.82315249893208\n",
      "\n",
      "\n",
      "Step: 247880\n",
      "loss: 0.0015\n",
      "grad_norm: 0.042476244270801544\n",
      "learning_rate: 5.87118515354312e-06\n",
      "epoch: 8.823864445393706\n",
      "\n",
      "\n",
      "Step: 247900\n",
      "loss: 0.0019\n",
      "grad_norm: 0.01177580188959837\n",
      "learning_rate: 5.84745360482225e-06\n",
      "epoch: 8.824576391855333\n",
      "\n",
      "\n",
      "Step: 247920\n",
      "loss: 0.001\n",
      "grad_norm: 0.027482500299811363\n",
      "learning_rate: 5.82372205610138e-06\n",
      "epoch: 8.825288338316959\n",
      "\n",
      "\n",
      "Step: 247940\n",
      "loss: 0.0014\n",
      "grad_norm: 0.032801829278469086\n",
      "learning_rate: 5.799990507380512e-06\n",
      "epoch: 8.826000284778585\n",
      "\n",
      "\n",
      "Step: 247960\n",
      "loss: 0.0012\n",
      "grad_norm: 0.010964011773467064\n",
      "learning_rate: 5.776258958659642e-06\n",
      "epoch: 8.826712231240212\n",
      "\n",
      "\n",
      "Step: 247980\n",
      "loss: 0.0009\n",
      "grad_norm: 0.002609945833683014\n",
      "learning_rate: 5.752527409938772e-06\n",
      "epoch: 8.827424177701836\n",
      "\n",
      "\n",
      "Step: 248000\n",
      "loss: 0.0009\n",
      "grad_norm: 0.027884960174560547\n",
      "learning_rate: 5.728795861217903e-06\n",
      "epoch: 8.828136124163462\n",
      "\n",
      "\n",
      "Step: 248020\n",
      "loss: 0.0012\n",
      "grad_norm: 0.014354248531162739\n",
      "learning_rate: 5.705064312497033e-06\n",
      "epoch: 8.828848070625089\n",
      "\n",
      "\n",
      "Step: 248040\n",
      "loss: 0.0008\n",
      "grad_norm: 0.03922952711582184\n",
      "learning_rate: 5.681332763776163e-06\n",
      "epoch: 8.829560017086715\n",
      "\n",
      "\n",
      "Step: 248060\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0009801830165088177\n",
      "learning_rate: 5.657601215055294e-06\n",
      "epoch: 8.830271963548341\n",
      "\n",
      "\n",
      "Step: 248080\n",
      "loss: 0.0008\n",
      "grad_norm: 0.034644968807697296\n",
      "learning_rate: 5.633869666334424e-06\n",
      "epoch: 8.830983910009968\n",
      "\n",
      "\n",
      "Step: 248100\n",
      "loss: 0.001\n",
      "grad_norm: 0.01102660782635212\n",
      "learning_rate: 5.610138117613556e-06\n",
      "epoch: 8.831695856471594\n",
      "\n",
      "\n",
      "Step: 248120\n",
      "loss: 0.0012\n",
      "grad_norm: 0.031186452135443687\n",
      "learning_rate: 5.586406568892686e-06\n",
      "epoch: 8.83240780293322\n",
      "\n",
      "\n",
      "Step: 248140\n",
      "loss: 0.001\n",
      "grad_norm: 0.08133888244628906\n",
      "learning_rate: 5.562675020171816e-06\n",
      "epoch: 8.833119749394845\n",
      "\n",
      "\n",
      "Step: 248160\n",
      "loss: 0.0006\n",
      "grad_norm: 0.030195707455277443\n",
      "learning_rate: 5.538943471450947e-06\n",
      "epoch: 8.833831695856471\n",
      "\n",
      "\n",
      "Step: 248180\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0310920849442482\n",
      "learning_rate: 5.515211922730077e-06\n",
      "epoch: 8.834543642318097\n",
      "\n",
      "\n",
      "Step: 248200\n",
      "loss: 0.0006\n",
      "grad_norm: 0.003508250694721937\n",
      "learning_rate: 5.491480374009207e-06\n",
      "epoch: 8.835255588779724\n",
      "\n",
      "\n",
      "Step: 248220\n",
      "loss: 0.0014\n",
      "grad_norm: 0.020659485831856728\n",
      "learning_rate: 5.467748825288338e-06\n",
      "epoch: 8.83596753524135\n",
      "\n",
      "\n",
      "Step: 248240\n",
      "loss: 0.0015\n",
      "grad_norm: 0.014875972643494606\n",
      "learning_rate: 5.444017276567468e-06\n",
      "epoch: 8.836679481702976\n",
      "\n",
      "\n",
      "Step: 248260\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02044806070625782\n",
      "learning_rate: 5.4202857278465984e-06\n",
      "epoch: 8.837391428164603\n",
      "\n",
      "\n",
      "Step: 248280\n",
      "loss: 0.0005\n",
      "grad_norm: 0.009510187432169914\n",
      "learning_rate: 5.396554179125729e-06\n",
      "epoch: 8.838103374626229\n",
      "\n",
      "\n",
      "Step: 248300\n",
      "loss: 0.0008\n",
      "grad_norm: 0.055036384612321854\n",
      "learning_rate: 5.3728226304048595e-06\n",
      "epoch: 8.838815321087854\n",
      "\n",
      "\n",
      "Step: 248320\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02440846525132656\n",
      "learning_rate: 5.34909108168399e-06\n",
      "epoch: 8.83952726754948\n",
      "\n",
      "\n",
      "Step: 248340\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0010299617424607277\n",
      "learning_rate: 5.325359532963121e-06\n",
      "epoch: 8.840239214011106\n",
      "\n",
      "\n",
      "Step: 248360\n",
      "loss: 0.0009\n",
      "grad_norm: 0.003457438200712204\n",
      "learning_rate: 5.3016279842422515e-06\n",
      "epoch: 8.840951160472732\n",
      "\n",
      "\n",
      "Step: 248380\n",
      "loss: 0.001\n",
      "grad_norm: 0.0036669354885816574\n",
      "learning_rate: 5.277896435521382e-06\n",
      "epoch: 8.841663106934359\n",
      "\n",
      "\n",
      "Step: 248400\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0026250919327139854\n",
      "learning_rate: 5.2541648868005125e-06\n",
      "epoch: 8.842375053395985\n",
      "\n",
      "\n",
      "Step: 248420\n",
      "loss: 0.0008\n",
      "grad_norm: 0.008657564409077168\n",
      "learning_rate: 5.230433338079643e-06\n",
      "epoch: 8.843086999857611\n",
      "\n",
      "\n",
      "Step: 248440\n",
      "loss: 0.0013\n",
      "grad_norm: 0.024522513151168823\n",
      "learning_rate: 5.206701789358773e-06\n",
      "epoch: 8.843798946319236\n",
      "\n",
      "\n",
      "Step: 248460\n",
      "loss: 0.0021\n",
      "grad_norm: 0.028946207836270332\n",
      "learning_rate: 5.182970240637904e-06\n",
      "epoch: 8.844510892780862\n",
      "\n",
      "\n",
      "Step: 248480\n",
      "loss: 0.0007\n",
      "grad_norm: 0.014286444522440434\n",
      "learning_rate: 5.159238691917034e-06\n",
      "epoch: 8.845222839242489\n",
      "\n",
      "\n",
      "Step: 248500\n",
      "loss: 0.001\n",
      "grad_norm: 0.006440314929932356\n",
      "learning_rate: 5.135507143196164e-06\n",
      "epoch: 8.845934785704115\n",
      "\n",
      "\n",
      "Step: 248520\n",
      "loss: 0.0006\n",
      "grad_norm: 0.010080434381961823\n",
      "learning_rate: 5.111775594475295e-06\n",
      "epoch: 8.846646732165741\n",
      "\n",
      "\n",
      "Step: 248540\n",
      "loss: 0.0011\n",
      "grad_norm: 0.020816873759031296\n",
      "learning_rate: 5.088044045754426e-06\n",
      "epoch: 8.847358678627367\n",
      "\n",
      "\n",
      "Step: 248560\n",
      "loss: 0.0011\n",
      "grad_norm: 0.036262720823287964\n",
      "learning_rate: 5.064312497033556e-06\n",
      "epoch: 8.848070625088994\n",
      "\n",
      "\n",
      "Step: 248580\n",
      "loss: 0.0014\n",
      "grad_norm: 0.027123909443616867\n",
      "learning_rate: 5.040580948312687e-06\n",
      "epoch: 8.84878257155062\n",
      "\n",
      "\n",
      "Step: 248600\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0013098698109388351\n",
      "learning_rate: 5.016849399591817e-06\n",
      "epoch: 8.849494518012246\n",
      "\n",
      "\n",
      "Step: 248620\n",
      "loss: 0.0011\n",
      "grad_norm: 0.024160385131835938\n",
      "learning_rate: 4.993117850870947e-06\n",
      "epoch: 8.850206464473871\n",
      "\n",
      "\n",
      "Step: 248640\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02431740239262581\n",
      "learning_rate: 4.969386302150078e-06\n",
      "epoch: 8.850918410935497\n",
      "\n",
      "\n",
      "Step: 248660\n",
      "loss: 0.0006\n",
      "grad_norm: 0.016209902241826057\n",
      "learning_rate: 4.945654753429208e-06\n",
      "epoch: 8.851630357397124\n",
      "\n",
      "\n",
      "Step: 248680\n",
      "loss: 0.0014\n",
      "grad_norm: 0.009385021403431892\n",
      "learning_rate: 4.921923204708339e-06\n",
      "epoch: 8.85234230385875\n",
      "\n",
      "\n",
      "Step: 248700\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011529571376740932\n",
      "learning_rate: 4.898191655987469e-06\n",
      "epoch: 8.853054250320376\n",
      "\n",
      "\n",
      "Step: 248720\n",
      "loss: 0.0007\n",
      "grad_norm: 0.014695531688630581\n",
      "learning_rate: 4.874460107266599e-06\n",
      "epoch: 8.853766196782002\n",
      "\n",
      "\n",
      "Step: 248740\n",
      "loss: 0.0008\n",
      "grad_norm: 0.00040884915506467223\n",
      "learning_rate: 4.850728558545731e-06\n",
      "epoch: 8.854478143243629\n",
      "\n",
      "\n",
      "Step: 248760\n",
      "loss: 0.0019\n",
      "grad_norm: 0.045013267546892166\n",
      "learning_rate: 4.826997009824861e-06\n",
      "epoch: 8.855190089705253\n",
      "\n",
      "\n",
      "Step: 248780\n",
      "loss: 0.0015\n",
      "grad_norm: 0.05550684779882431\n",
      "learning_rate: 4.803265461103991e-06\n",
      "epoch: 8.85590203616688\n",
      "\n",
      "\n",
      "Step: 248800\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0035622636787593365\n",
      "learning_rate: 4.779533912383122e-06\n",
      "epoch: 8.856613982628506\n",
      "\n",
      "\n",
      "Step: 248820\n",
      "loss: 0.0008\n",
      "grad_norm: 0.027528658509254456\n",
      "learning_rate: 4.755802363662252e-06\n",
      "epoch: 8.857325929090132\n",
      "\n",
      "\n",
      "Step: 248840\n",
      "loss: 0.001\n",
      "grad_norm: 0.03922408074140549\n",
      "learning_rate: 4.7320708149413824e-06\n",
      "epoch: 8.858037875551759\n",
      "\n",
      "\n",
      "Step: 248860\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03672220557928085\n",
      "learning_rate: 4.708339266220513e-06\n",
      "epoch: 8.858749822013385\n",
      "\n",
      "\n",
      "Step: 248880\n",
      "loss: 0.001\n",
      "grad_norm: 0.018862588331103325\n",
      "learning_rate: 4.6846077174996435e-06\n",
      "epoch: 8.859461768475011\n",
      "\n",
      "\n",
      "Step: 248900\n",
      "loss: 0.0008\n",
      "grad_norm: 0.04370580241084099\n",
      "learning_rate: 4.6608761687787745e-06\n",
      "epoch: 8.860173714936638\n",
      "\n",
      "\n",
      "Step: 248920\n",
      "loss: 0.001\n",
      "grad_norm: 0.010082104243338108\n",
      "learning_rate: 4.6371446200579046e-06\n",
      "epoch: 8.860885661398262\n",
      "\n",
      "\n",
      "Step: 248940\n",
      "loss: 0.0019\n",
      "grad_norm: 0.06179305538535118\n",
      "learning_rate: 4.613413071337035e-06\n",
      "epoch: 8.861597607859888\n",
      "\n",
      "\n",
      "Step: 248960\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01080107968300581\n",
      "learning_rate: 4.589681522616166e-06\n",
      "epoch: 8.862309554321515\n",
      "\n",
      "\n",
      "Step: 248980\n",
      "loss: 0.0014\n",
      "grad_norm: 0.0017212157836183906\n",
      "learning_rate: 4.565949973895296e-06\n",
      "epoch: 8.863021500783141\n",
      "\n",
      "\n",
      "Step: 249000\n",
      "loss: 0.0011\n",
      "grad_norm: 0.005723578855395317\n",
      "learning_rate: 4.542218425174427e-06\n",
      "epoch: 8.863733447244767\n",
      "\n",
      "\n",
      "Step: 249020\n",
      "loss: 0.0011\n",
      "grad_norm: 0.005859996657818556\n",
      "learning_rate: 4.518486876453557e-06\n",
      "epoch: 8.864445393706394\n",
      "\n",
      "\n",
      "Step: 249040\n",
      "loss: 0.0013\n",
      "grad_norm: 0.011305917985737324\n",
      "learning_rate: 4.494755327732687e-06\n",
      "epoch: 8.86515734016802\n",
      "\n",
      "\n",
      "Step: 249060\n",
      "loss: 0.0009\n",
      "grad_norm: 0.020139161497354507\n",
      "learning_rate: 4.471023779011818e-06\n",
      "epoch: 8.865869286629646\n",
      "\n",
      "\n",
      "Step: 249080\n",
      "loss: 0.001\n",
      "grad_norm: 0.00571767333894968\n",
      "learning_rate: 4.447292230290949e-06\n",
      "epoch: 8.86658123309127\n",
      "\n",
      "\n",
      "Step: 249100\n",
      "loss: 0.0014\n",
      "grad_norm: 0.006942988373339176\n",
      "learning_rate: 4.423560681570079e-06\n",
      "epoch: 8.867293179552897\n",
      "\n",
      "\n",
      "Step: 249120\n",
      "loss: 0.0012\n",
      "grad_norm: 0.000901833176612854\n",
      "learning_rate: 4.399829132849209e-06\n",
      "epoch: 8.868005126014523\n",
      "\n",
      "\n",
      "Step: 249140\n",
      "loss: 0.001\n",
      "grad_norm: 0.03403340280056\n",
      "learning_rate: 4.37609758412834e-06\n",
      "epoch: 8.86871707247615\n",
      "\n",
      "\n",
      "Step: 249160\n",
      "loss: 0.001\n",
      "grad_norm: 0.02463606558740139\n",
      "learning_rate: 4.352366035407471e-06\n",
      "epoch: 8.869429018937776\n",
      "\n",
      "\n",
      "Step: 249180\n",
      "loss: 0.001\n",
      "grad_norm: 0.044119060039520264\n",
      "learning_rate: 4.328634486686601e-06\n",
      "epoch: 8.870140965399402\n",
      "\n",
      "\n",
      "Step: 249200\n",
      "loss: 0.0014\n",
      "grad_norm: 0.057889796793460846\n",
      "learning_rate: 4.304902937965731e-06\n",
      "epoch: 8.870852911861029\n",
      "\n",
      "\n",
      "Step: 249220\n",
      "loss: 0.0015\n",
      "grad_norm: 0.03823857009410858\n",
      "learning_rate: 4.281171389244862e-06\n",
      "epoch: 8.871564858322655\n",
      "\n",
      "\n",
      "Step: 249240\n",
      "loss: 0.0008\n",
      "grad_norm: 0.02772938460111618\n",
      "learning_rate: 4.257439840523992e-06\n",
      "epoch: 8.87227680478428\n",
      "\n",
      "\n",
      "Step: 249260\n",
      "loss: 0.0012\n",
      "grad_norm: 0.013054246082901955\n",
      "learning_rate: 4.233708291803123e-06\n",
      "epoch: 8.872988751245906\n",
      "\n",
      "\n",
      "Step: 249280\n",
      "loss: 0.0011\n",
      "grad_norm: 0.034773971885442734\n",
      "learning_rate: 4.209976743082253e-06\n",
      "epoch: 8.873700697707532\n",
      "\n",
      "\n",
      "Step: 249300\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03575314208865166\n",
      "learning_rate: 4.186245194361383e-06\n",
      "epoch: 8.874412644169158\n",
      "\n",
      "\n",
      "Step: 249320\n",
      "loss: 0.0013\n",
      "grad_norm: 0.008017342537641525\n",
      "learning_rate: 4.162513645640514e-06\n",
      "epoch: 8.875124590630785\n",
      "\n",
      "\n",
      "Step: 249340\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01845872402191162\n",
      "learning_rate: 4.138782096919644e-06\n",
      "epoch: 8.875836537092411\n",
      "\n",
      "\n",
      "Step: 249360\n",
      "loss: 0.0017\n",
      "grad_norm: 0.017474524676799774\n",
      "learning_rate: 4.1150505481987745e-06\n",
      "epoch: 8.876548483554037\n",
      "\n",
      "\n",
      "Step: 249380\n",
      "loss: 0.0012\n",
      "grad_norm: 0.056423984467983246\n",
      "learning_rate: 4.091318999477905e-06\n",
      "epoch: 8.877260430015664\n",
      "\n",
      "\n",
      "Step: 249400\n",
      "loss: 0.0009\n",
      "grad_norm: 0.014475181698799133\n",
      "learning_rate: 4.067587450757036e-06\n",
      "epoch: 8.877972376477288\n",
      "\n",
      "\n",
      "Step: 249420\n",
      "loss: 0.0009\n",
      "grad_norm: 0.04724109172821045\n",
      "learning_rate: 4.0438559020361665e-06\n",
      "epoch: 8.878684322938915\n",
      "\n",
      "\n",
      "Step: 249440\n",
      "loss: 0.0016\n",
      "grad_norm: 0.02316134423017502\n",
      "learning_rate: 4.0201243533152966e-06\n",
      "epoch: 8.87939626940054\n",
      "\n",
      "\n",
      "Step: 249460\n",
      "loss: 0.0011\n",
      "grad_norm: 0.06836700439453125\n",
      "learning_rate: 3.9963928045944275e-06\n",
      "epoch: 8.880108215862167\n",
      "\n",
      "\n",
      "Step: 249480\n",
      "loss: 0.0008\n",
      "grad_norm: 0.03179136663675308\n",
      "learning_rate: 3.9726612558735585e-06\n",
      "epoch: 8.880820162323793\n",
      "\n",
      "\n",
      "Step: 249500\n",
      "loss: 0.0007\n",
      "grad_norm: 0.02070579119026661\n",
      "learning_rate: 3.9489297071526886e-06\n",
      "epoch: 8.88153210878542\n",
      "\n",
      "\n",
      "Step: 249520\n",
      "loss: 0.0007\n",
      "grad_norm: 0.01421430055052042\n",
      "learning_rate: 3.925198158431819e-06\n",
      "epoch: 8.882244055247046\n",
      "\n",
      "\n",
      "Step: 249540\n",
      "loss: 0.0012\n",
      "grad_norm: 0.016769513487815857\n",
      "learning_rate: 3.90146660971095e-06\n",
      "epoch: 8.88295600170867\n",
      "\n",
      "\n",
      "Step: 249560\n",
      "loss: 0.001\n",
      "grad_norm: 0.005107277538627386\n",
      "learning_rate: 3.87773506099008e-06\n",
      "epoch: 8.883667948170297\n",
      "\n",
      "\n",
      "Step: 249580\n",
      "loss: 0.001\n",
      "grad_norm: 0.030779996886849403\n",
      "learning_rate: 3.854003512269211e-06\n",
      "epoch: 8.884379894631923\n",
      "\n",
      "\n",
      "Step: 249600\n",
      "loss: 0.0012\n",
      "grad_norm: 0.015911152586340904\n",
      "learning_rate: 3.830271963548341e-06\n",
      "epoch: 8.88509184109355\n",
      "\n",
      "\n",
      "Step: 249620\n",
      "loss: 0.0015\n",
      "grad_norm: 0.022905606776475906\n",
      "learning_rate: 3.806540414827471e-06\n",
      "epoch: 8.885803787555176\n",
      "\n",
      "\n",
      "Step: 249640\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04538089036941528\n",
      "learning_rate: 3.782808866106602e-06\n",
      "epoch: 8.886515734016802\n",
      "\n",
      "\n",
      "Step: 249660\n",
      "loss: 0.0006\n",
      "grad_norm: 0.029566368088126183\n",
      "learning_rate: 3.7590773173857324e-06\n",
      "epoch: 8.887227680478428\n",
      "\n",
      "\n",
      "Step: 249680\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009521232917904854\n",
      "learning_rate: 3.7353457686648625e-06\n",
      "epoch: 8.887939626940055\n",
      "\n",
      "\n",
      "Step: 249700\n",
      "loss: 0.001\n",
      "grad_norm: 0.02248750627040863\n",
      "learning_rate: 3.711614219943993e-06\n",
      "epoch: 8.888651573401681\n",
      "\n",
      "\n",
      "Step: 249720\n",
      "loss: 0.001\n",
      "grad_norm: 0.03340056911110878\n",
      "learning_rate: 3.687882671223124e-06\n",
      "epoch: 8.889363519863306\n",
      "\n",
      "\n",
      "Step: 249740\n",
      "loss: 0.0008\n",
      "grad_norm: 0.033934336155653\n",
      "learning_rate: 3.6641511225022545e-06\n",
      "epoch: 8.890075466324932\n",
      "\n",
      "\n",
      "Step: 249760\n",
      "loss: 0.0014\n",
      "grad_norm: 0.009230382740497589\n",
      "learning_rate: 3.6404195737813846e-06\n",
      "epoch: 8.890787412786558\n",
      "\n",
      "\n",
      "Step: 249780\n",
      "loss: 0.0008\n",
      "grad_norm: 0.03772684186697006\n",
      "learning_rate: 3.616688025060515e-06\n",
      "epoch: 8.891499359248185\n",
      "\n",
      "\n",
      "Step: 249800\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01745021343231201\n",
      "learning_rate: 3.5929564763396456e-06\n",
      "epoch: 8.89221130570981\n",
      "\n",
      "\n",
      "Step: 249820\n",
      "loss: 0.0012\n",
      "grad_norm: 0.024870527908205986\n",
      "learning_rate: 3.5692249276187757e-06\n",
      "epoch: 8.892923252171437\n",
      "\n",
      "\n",
      "Step: 249840\n",
      "loss: 0.0014\n",
      "grad_norm: 0.01686364784836769\n",
      "learning_rate: 3.5454933788979067e-06\n",
      "epoch: 8.893635198633064\n",
      "\n",
      "\n",
      "Step: 249860\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0069060963578522205\n",
      "learning_rate: 3.5217618301770372e-06\n",
      "epoch: 8.894347145094688\n",
      "\n",
      "\n",
      "Step: 249880\n",
      "loss: 0.0006\n",
      "grad_norm: 0.021913083270192146\n",
      "learning_rate: 3.4980302814561673e-06\n",
      "epoch: 8.895059091556314\n",
      "\n",
      "\n",
      "Step: 249900\n",
      "loss: 0.0009\n",
      "grad_norm: 0.004877113271504641\n",
      "learning_rate: 3.474298732735298e-06\n",
      "epoch: 8.89577103801794\n",
      "\n",
      "\n",
      "Step: 249920\n",
      "loss: 0.001\n",
      "grad_norm: 0.0323590412735939\n",
      "learning_rate: 3.450567184014429e-06\n",
      "epoch: 8.896482984479567\n",
      "\n",
      "\n",
      "Step: 249940\n",
      "loss: 0.0011\n",
      "grad_norm: 0.026498379185795784\n",
      "learning_rate: 3.426835635293559e-06\n",
      "epoch: 8.897194930941193\n",
      "\n",
      "\n",
      "Step: 249960\n",
      "loss: 0.0012\n",
      "grad_norm: 0.04529149830341339\n",
      "learning_rate: 3.4031040865726894e-06\n",
      "epoch: 8.89790687740282\n",
      "\n",
      "\n",
      "Step: 249980\n",
      "loss: 0.0009\n",
      "grad_norm: 0.028471138328313828\n",
      "learning_rate: 3.37937253785182e-06\n",
      "epoch: 8.898618823864446\n",
      "\n",
      "\n",
      "Step: 250000\n",
      "loss: 0.0013\n",
      "grad_norm: 0.004594546742737293\n",
      "learning_rate: 3.3556409891309505e-06\n",
      "epoch: 8.899330770326072\n",
      "\n",
      "\n",
      "Step: 250020\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04625839740037918\n",
      "learning_rate: 3.3319094404100806e-06\n",
      "epoch: 8.900042716787697\n",
      "\n",
      "\n",
      "Step: 250040\n",
      "loss: 0.0012\n",
      "grad_norm: 0.021053485572338104\n",
      "learning_rate: 3.3081778916892115e-06\n",
      "epoch: 8.900754663249323\n",
      "\n",
      "\n",
      "Step: 250060\n",
      "loss: 0.0013\n",
      "grad_norm: 0.006866089534014463\n",
      "learning_rate: 3.284446342968342e-06\n",
      "epoch: 8.90146660971095\n",
      "\n",
      "\n",
      "Step: 250080\n",
      "loss: 0.0012\n",
      "grad_norm: 0.014493267983198166\n",
      "learning_rate: 3.260714794247472e-06\n",
      "epoch: 8.902178556172576\n",
      "\n",
      "\n",
      "Step: 250100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.00814273115247488\n",
      "learning_rate: 3.2369832455266027e-06\n",
      "epoch: 8.902890502634202\n",
      "\n",
      "\n",
      "Step: 250120\n",
      "loss: 0.0017\n",
      "grad_norm: 0.016232805326581\n",
      "learning_rate: 3.2132516968057336e-06\n",
      "epoch: 8.903602449095828\n",
      "\n",
      "\n",
      "Step: 250140\n",
      "loss: 0.0011\n",
      "grad_norm: 0.015300152823328972\n",
      "learning_rate: 3.1895201480848638e-06\n",
      "epoch: 8.904314395557455\n",
      "\n",
      "\n",
      "Step: 250160\n",
      "loss: 0.0014\n",
      "grad_norm: 0.015663597732782364\n",
      "learning_rate: 3.1657885993639943e-06\n",
      "epoch: 8.905026342019081\n",
      "\n",
      "\n",
      "Step: 250180\n",
      "loss: 0.0017\n",
      "grad_norm: 0.02042422629892826\n",
      "learning_rate: 3.142057050643125e-06\n",
      "epoch: 8.905738288480705\n",
      "\n",
      "\n",
      "Step: 250200\n",
      "loss: 0.0014\n",
      "grad_norm: 0.022509653121232986\n",
      "learning_rate: 3.118325501922255e-06\n",
      "epoch: 8.906450234942332\n",
      "\n",
      "\n",
      "Step: 250220\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03428727760910988\n",
      "learning_rate: 3.0945939532013854e-06\n",
      "epoch: 8.907162181403958\n",
      "\n",
      "\n",
      "Step: 250240\n",
      "loss: 0.0013\n",
      "grad_norm: 0.019722068682312965\n",
      "learning_rate: 3.0708624044805164e-06\n",
      "epoch: 8.907874127865584\n",
      "\n",
      "\n",
      "Step: 250260\n",
      "loss: 0.001\n",
      "grad_norm: 0.016051890328526497\n",
      "learning_rate: 3.0471308557596465e-06\n",
      "epoch: 8.90858607432721\n",
      "\n",
      "\n",
      "Step: 250280\n",
      "loss: 0.0014\n",
      "grad_norm: 0.04356551170349121\n",
      "learning_rate: 3.023399307038777e-06\n",
      "epoch: 8.909298020788837\n",
      "\n",
      "\n",
      "Step: 250300\n",
      "loss: 0.0007\n",
      "grad_norm: 0.002794078318402171\n",
      "learning_rate: 2.9996677583179075e-06\n",
      "epoch: 8.910009967250463\n",
      "\n",
      "\n",
      "Step: 250320\n",
      "loss: 0.0013\n",
      "grad_norm: 0.03920877352356911\n",
      "learning_rate: 2.975936209597038e-06\n",
      "epoch: 8.91072191371209\n",
      "\n",
      "\n",
      "Step: 250340\n",
      "loss: 0.0012\n",
      "grad_norm: 0.017364108934998512\n",
      "learning_rate: 2.9522046608761686e-06\n",
      "epoch: 8.911433860173714\n",
      "\n",
      "\n",
      "Step: 250360\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03859631344676018\n",
      "learning_rate: 2.928473112155299e-06\n",
      "epoch: 8.91214580663534\n",
      "\n",
      "\n",
      "Step: 250380\n",
      "loss: 0.001\n",
      "grad_norm: 0.012093271128833294\n",
      "learning_rate: 2.9047415634344297e-06\n",
      "epoch: 8.912857753096967\n",
      "\n",
      "\n",
      "Step: 250400\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03708920255303383\n",
      "learning_rate: 2.8810100147135598e-06\n",
      "epoch: 8.913569699558593\n",
      "\n",
      "\n",
      "Step: 250420\n",
      "loss: 0.0012\n",
      "grad_norm: 0.004434623755514622\n",
      "learning_rate: 2.8572784659926903e-06\n",
      "epoch: 8.91428164602022\n",
      "\n",
      "\n",
      "Step: 250440\n",
      "loss: 0.0009\n",
      "grad_norm: 0.021694576367735863\n",
      "learning_rate: 2.8335469172718212e-06\n",
      "epoch: 8.914993592481846\n",
      "\n",
      "\n",
      "Step: 250460\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0028139546047896147\n",
      "learning_rate: 2.8098153685509513e-06\n",
      "epoch: 8.915705538943472\n",
      "\n",
      "\n",
      "Step: 250480\n",
      "loss: 0.0007\n",
      "grad_norm: 0.009536290541291237\n",
      "learning_rate: 2.786083819830082e-06\n",
      "epoch: 8.916417485405098\n",
      "\n",
      "\n",
      "Step: 250500\n",
      "loss: 0.001\n",
      "grad_norm: 0.0137709965929389\n",
      "learning_rate: 2.7623522711092124e-06\n",
      "epoch: 8.917129431866723\n",
      "\n",
      "\n",
      "Step: 250520\n",
      "loss: 0.001\n",
      "grad_norm: 0.019480789080262184\n",
      "learning_rate: 2.7386207223883425e-06\n",
      "epoch: 8.91784137832835\n",
      "\n",
      "\n",
      "Step: 250540\n",
      "loss: 0.0013\n",
      "grad_norm: 0.06576843559741974\n",
      "learning_rate: 2.7148891736674734e-06\n",
      "epoch: 8.918553324789976\n",
      "\n",
      "\n",
      "Step: 250560\n",
      "loss: 0.0013\n",
      "grad_norm: 0.02085038274526596\n",
      "learning_rate: 2.691157624946604e-06\n",
      "epoch: 8.919265271251602\n",
      "\n",
      "\n",
      "Step: 250580\n",
      "loss: 0.001\n",
      "grad_norm: 0.03740569204092026\n",
      "learning_rate: 2.6674260762257345e-06\n",
      "epoch: 8.919977217713228\n",
      "\n",
      "\n",
      "Step: 250600\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04317654296755791\n",
      "learning_rate: 2.6436945275048646e-06\n",
      "epoch: 8.920689164174854\n",
      "\n",
      "\n",
      "Step: 250620\n",
      "loss: 0.0014\n",
      "grad_norm: 0.04044803977012634\n",
      "learning_rate: 2.619962978783995e-06\n",
      "epoch: 8.92140111063648\n",
      "\n",
      "\n",
      "Step: 250640\n",
      "loss: 0.001\n",
      "grad_norm: 0.020291084423661232\n",
      "learning_rate: 2.596231430063126e-06\n",
      "epoch: 8.922113057098105\n",
      "\n",
      "\n",
      "Step: 250660\n",
      "loss: 0.001\n",
      "grad_norm: 0.009987725876271725\n",
      "learning_rate: 2.572499881342256e-06\n",
      "epoch: 8.922825003559732\n",
      "\n",
      "\n",
      "Step: 250680\n",
      "loss: 0.0013\n",
      "grad_norm: 0.0019636352080851793\n",
      "learning_rate: 2.5487683326213867e-06\n",
      "epoch: 8.923536950021358\n",
      "\n",
      "\n",
      "Step: 250700\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02414996363222599\n",
      "learning_rate: 2.5250367839005172e-06\n",
      "epoch: 8.924248896482984\n",
      "\n",
      "\n",
      "Step: 250720\n",
      "loss: 0.0009\n",
      "grad_norm: 0.034465283155441284\n",
      "learning_rate: 2.5013052351796473e-06\n",
      "epoch: 8.92496084294461\n",
      "\n",
      "\n",
      "Step: 250740\n",
      "loss: 0.0009\n",
      "grad_norm: 0.003692016238346696\n",
      "learning_rate: 2.477573686458778e-06\n",
      "epoch: 8.925672789406237\n",
      "\n",
      "\n",
      "Step: 250760\n",
      "loss: 0.0009\n",
      "grad_norm: 0.016209706664085388\n",
      "learning_rate: 2.453842137737909e-06\n",
      "epoch: 8.926384735867863\n",
      "\n",
      "\n",
      "Step: 250780\n",
      "loss: 0.0012\n",
      "grad_norm: 0.01917368359863758\n",
      "learning_rate: 2.430110589017039e-06\n",
      "epoch: 8.92709668232949\n",
      "\n",
      "\n",
      "Step: 250800\n",
      "loss: 0.0012\n",
      "grad_norm: 0.010950494557619095\n",
      "learning_rate: 2.4063790402961695e-06\n",
      "epoch: 8.927808628791116\n",
      "\n",
      "\n",
      "Step: 250820\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01473889872431755\n",
      "learning_rate: 2.3826474915753e-06\n",
      "epoch: 8.92852057525274\n",
      "\n",
      "\n",
      "Step: 250840\n",
      "loss: 0.001\n",
      "grad_norm: 0.014954731799662113\n",
      "learning_rate: 2.35891594285443e-06\n",
      "epoch: 8.929232521714367\n",
      "\n",
      "\n",
      "Step: 250860\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009237425401806831\n",
      "learning_rate: 2.335184394133561e-06\n",
      "epoch: 8.929944468175993\n",
      "\n",
      "\n",
      "Step: 250880\n",
      "loss: 0.0011\n",
      "grad_norm: 0.04957554489374161\n",
      "learning_rate: 2.3114528454126916e-06\n",
      "epoch: 8.93065641463762\n",
      "\n",
      "\n",
      "Step: 250900\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0034170416183769703\n",
      "learning_rate: 2.287721296691822e-06\n",
      "epoch: 8.931368361099246\n",
      "\n",
      "\n",
      "Step: 250920\n",
      "loss: 0.0011\n",
      "grad_norm: 0.0010278710396960378\n",
      "learning_rate: 2.263989747970952e-06\n",
      "epoch: 8.932080307560872\n",
      "\n",
      "\n",
      "Step: 250940\n",
      "loss: 0.0015\n",
      "grad_norm: 0.004731297958642244\n",
      "learning_rate: 2.2402581992500827e-06\n",
      "epoch: 8.932792254022498\n",
      "\n",
      "\n",
      "Step: 250960\n",
      "loss: 0.0017\n",
      "grad_norm: 0.003562448313459754\n",
      "learning_rate: 2.2165266505292132e-06\n",
      "epoch: 8.933504200484123\n",
      "\n",
      "\n",
      "Step: 250980\n",
      "loss: 0.0008\n",
      "grad_norm: 0.004306385759264231\n",
      "learning_rate: 2.1927951018083438e-06\n",
      "epoch: 8.934216146945749\n",
      "\n",
      "\n",
      "Step: 251000\n",
      "loss: 0.0011\n",
      "grad_norm: 0.03740650787949562\n",
      "learning_rate: 2.1690635530874743e-06\n",
      "epoch: 8.934928093407375\n",
      "\n",
      "\n",
      "Step: 251020\n",
      "loss: 0.0008\n",
      "grad_norm: 0.017345992848277092\n",
      "learning_rate: 2.145332004366605e-06\n",
      "epoch: 8.935640039869002\n",
      "\n",
      "\n",
      "Step: 251040\n",
      "loss: 0.001\n",
      "grad_norm: 0.021168788895010948\n",
      "learning_rate: 2.1216004556457354e-06\n",
      "epoch: 8.936351986330628\n",
      "\n",
      "\n",
      "Step: 251060\n",
      "loss: 0.0008\n",
      "grad_norm: 0.036668986082077026\n",
      "learning_rate: 2.097868906924866e-06\n",
      "epoch: 8.937063932792254\n",
      "\n",
      "\n",
      "Step: 251080\n",
      "loss: 0.001\n",
      "grad_norm: 0.004350734408944845\n",
      "learning_rate: 2.074137358203996e-06\n",
      "epoch: 8.93777587925388\n",
      "\n",
      "\n",
      "Step: 251100\n",
      "loss: 0.0009\n",
      "grad_norm: 0.027606744319200516\n",
      "learning_rate: 2.0504058094831265e-06\n",
      "epoch: 8.938487825715507\n",
      "\n",
      "\n",
      "Step: 251120\n",
      "loss: 0.0009\n",
      "grad_norm: 0.007941843941807747\n",
      "learning_rate: 2.026674260762257e-06\n",
      "epoch: 8.939199772177131\n",
      "\n",
      "\n",
      "Step: 251140\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0690479502081871\n",
      "learning_rate: 2.0029427120413876e-06\n",
      "epoch: 8.939911718638758\n",
      "\n",
      "\n",
      "Step: 251160\n",
      "loss: 0.0011\n",
      "grad_norm: 0.013438883237540722\n",
      "learning_rate: 1.979211163320518e-06\n",
      "epoch: 8.940623665100384\n",
      "\n",
      "\n",
      "Step: 251180\n",
      "loss: 0.0012\n",
      "grad_norm: 0.018939156085252762\n",
      "learning_rate: 1.9554796145996486e-06\n",
      "epoch: 8.94133561156201\n",
      "\n",
      "\n",
      "Step: 251200\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02341235801577568\n",
      "learning_rate: 1.931748065878779e-06\n",
      "epoch: 8.942047558023637\n",
      "\n",
      "\n",
      "Step: 251220\n",
      "loss: 0.0009\n",
      "grad_norm: 0.14983534812927246\n",
      "learning_rate: 1.9080165171579097e-06\n",
      "epoch: 8.942759504485263\n",
      "\n",
      "\n",
      "Step: 251240\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04782893508672714\n",
      "learning_rate: 1.8842849684370402e-06\n",
      "epoch: 8.94347145094689\n",
      "\n",
      "\n",
      "Step: 251260\n",
      "loss: 0.0008\n",
      "grad_norm: 0.025418825447559357\n",
      "learning_rate: 1.8605534197161705e-06\n",
      "epoch: 8.944183397408516\n",
      "\n",
      "\n",
      "Step: 251280\n",
      "loss: 0.0013\n",
      "grad_norm: 0.038196198642253876\n",
      "learning_rate: 1.8368218709953008e-06\n",
      "epoch: 8.94489534387014\n",
      "\n",
      "\n",
      "Step: 251300\n",
      "loss: 0.0013\n",
      "grad_norm: 0.04609856382012367\n",
      "learning_rate: 1.8130903222744316e-06\n",
      "epoch: 8.945607290331766\n",
      "\n",
      "\n",
      "Step: 251320\n",
      "loss: 0.0011\n",
      "grad_norm: 0.008136721327900887\n",
      "learning_rate: 1.7893587735535619e-06\n",
      "epoch: 8.946319236793393\n",
      "\n",
      "\n",
      "Step: 251340\n",
      "loss: 0.0011\n",
      "grad_norm: 0.024434728547930717\n",
      "learning_rate: 1.7656272248326924e-06\n",
      "epoch: 8.94703118325502\n",
      "\n",
      "\n",
      "Step: 251360\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02743612229824066\n",
      "learning_rate: 1.741895676111823e-06\n",
      "epoch: 8.947743129716645\n",
      "\n",
      "\n",
      "Step: 251380\n",
      "loss: 0.0007\n",
      "grad_norm: 0.027088647708296776\n",
      "learning_rate: 1.7181641273909533e-06\n",
      "epoch: 8.948455076178272\n",
      "\n",
      "\n",
      "Step: 251400\n",
      "loss: 0.0012\n",
      "grad_norm: 0.009851719252765179\n",
      "learning_rate: 1.694432578670084e-06\n",
      "epoch: 8.949167022639898\n",
      "\n",
      "\n",
      "Step: 251420\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03978412225842476\n",
      "learning_rate: 1.6707010299492143e-06\n",
      "epoch: 8.949878969101524\n",
      "\n",
      "\n",
      "Step: 251440\n",
      "loss: 0.0006\n",
      "grad_norm: 0.002874143188819289\n",
      "learning_rate: 1.6469694812283448e-06\n",
      "epoch: 8.950590915563149\n",
      "\n",
      "\n",
      "Step: 251460\n",
      "loss: 0.0015\n",
      "grad_norm: 0.02508980967104435\n",
      "learning_rate: 1.6232379325074754e-06\n",
      "epoch: 8.951302862024775\n",
      "\n",
      "\n",
      "Step: 251480\n",
      "loss: 0.0011\n",
      "grad_norm: 0.011060750111937523\n",
      "learning_rate: 1.5995063837866057e-06\n",
      "epoch: 8.952014808486402\n",
      "\n",
      "\n",
      "Step: 251500\n",
      "loss: 0.0012\n",
      "grad_norm: 0.0016653279308229685\n",
      "learning_rate: 1.5757748350657362e-06\n",
      "epoch: 8.952726754948028\n",
      "\n",
      "\n",
      "Step: 251520\n",
      "loss: 0.001\n",
      "grad_norm: 0.032170429825782776\n",
      "learning_rate: 1.5520432863448667e-06\n",
      "epoch: 8.953438701409654\n",
      "\n",
      "\n",
      "Step: 251540\n",
      "loss: 0.0012\n",
      "grad_norm: 0.022102875635027885\n",
      "learning_rate: 1.528311737623997e-06\n",
      "epoch: 8.95415064787128\n",
      "\n",
      "\n",
      "Step: 251560\n",
      "loss: 0.0008\n",
      "grad_norm: 0.04971616715192795\n",
      "learning_rate: 1.5045801889031278e-06\n",
      "epoch: 8.954862594332907\n",
      "\n",
      "\n",
      "Step: 251580\n",
      "loss: 0.001\n",
      "grad_norm: 0.020732050761580467\n",
      "learning_rate: 1.4808486401822581e-06\n",
      "epoch: 8.955574540794533\n",
      "\n",
      "\n",
      "Step: 251600\n",
      "loss: 0.001\n",
      "grad_norm: 0.00894172303378582\n",
      "learning_rate: 1.4571170914613886e-06\n",
      "epoch: 8.956286487256158\n",
      "\n",
      "\n",
      "Step: 251620\n",
      "loss: 0.0011\n",
      "grad_norm: 0.022240804508328438\n",
      "learning_rate: 1.4333855427405192e-06\n",
      "epoch: 8.956998433717784\n",
      "\n",
      "\n",
      "Step: 251640\n",
      "loss: 0.001\n",
      "grad_norm: 0.020120879635214806\n",
      "learning_rate: 1.4096539940196495e-06\n",
      "epoch: 8.95771038017941\n",
      "\n",
      "\n",
      "Step: 251660\n",
      "loss: 0.0015\n",
      "grad_norm: 0.012831512838602066\n",
      "learning_rate: 1.38592244529878e-06\n",
      "epoch: 8.958422326641037\n",
      "\n",
      "\n",
      "Step: 251680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.04902948811650276\n",
      "learning_rate: 1.3621908965779105e-06\n",
      "epoch: 8.959134273102663\n",
      "\n",
      "\n",
      "Step: 251700\n",
      "loss: 0.0007\n",
      "grad_norm: 0.007694513536989689\n",
      "learning_rate: 1.338459347857041e-06\n",
      "epoch: 8.95984621956429\n",
      "\n",
      "\n",
      "Step: 251720\n",
      "loss: 0.0011\n",
      "grad_norm: 0.015453096479177475\n",
      "learning_rate: 1.3147277991361716e-06\n",
      "epoch: 8.960558166025915\n",
      "\n",
      "\n",
      "Step: 251740\n",
      "loss: 0.001\n",
      "grad_norm: 0.028030427172780037\n",
      "learning_rate: 1.290996250415302e-06\n",
      "epoch: 8.96127011248754\n",
      "\n",
      "\n",
      "Step: 251760\n",
      "loss: 0.0011\n",
      "grad_norm: 0.009923810139298439\n",
      "learning_rate: 1.2672647016944324e-06\n",
      "epoch: 8.961982058949166\n",
      "\n",
      "\n",
      "Step: 251780\n",
      "loss: 0.0009\n",
      "grad_norm: 0.02047807164490223\n",
      "learning_rate: 1.243533152973563e-06\n",
      "epoch: 8.962694005410793\n",
      "\n",
      "\n",
      "Step: 251800\n",
      "loss: 0.0007\n",
      "grad_norm: 0.0019353468669578433\n",
      "learning_rate: 1.2198016042526935e-06\n",
      "epoch: 8.963405951872419\n",
      "\n",
      "\n",
      "Step: 251820\n",
      "loss: 0.001\n",
      "grad_norm: 0.021962685510516167\n",
      "learning_rate: 1.196070055531824e-06\n",
      "epoch: 8.964117898334045\n",
      "\n",
      "\n",
      "Step: 251840\n",
      "loss: 0.0014\n",
      "grad_norm: 0.030369024723768234\n",
      "learning_rate: 1.1723385068109543e-06\n",
      "epoch: 8.964829844795672\n",
      "\n",
      "\n",
      "Step: 251860\n",
      "loss: 0.0017\n",
      "grad_norm: 0.01137132290750742\n",
      "learning_rate: 1.1486069580900849e-06\n",
      "epoch: 8.965541791257298\n",
      "\n",
      "\n",
      "Step: 251880\n",
      "loss: 0.001\n",
      "grad_norm: 0.008380538783967495\n",
      "learning_rate: 1.1248754093692154e-06\n",
      "epoch: 8.966253737718924\n",
      "\n",
      "\n",
      "Step: 251900\n",
      "loss: 0.0008\n",
      "grad_norm: 0.043261293321847916\n",
      "learning_rate: 1.101143860648346e-06\n",
      "epoch: 8.96696568418055\n",
      "\n",
      "\n",
      "Step: 251920\n",
      "loss: 0.001\n",
      "grad_norm: 0.045728787779808044\n",
      "learning_rate: 1.0774123119274762e-06\n",
      "epoch: 8.967677630642175\n",
      "\n",
      "\n",
      "Step: 251940\n",
      "loss: 0.0011\n",
      "grad_norm: 0.00832323171198368\n",
      "learning_rate: 1.0536807632066068e-06\n",
      "epoch: 8.968389577103801\n",
      "\n",
      "\n",
      "Step: 251960\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03151523694396019\n",
      "learning_rate: 1.0299492144857373e-06\n",
      "epoch: 8.969101523565428\n",
      "\n",
      "\n",
      "Step: 251980\n",
      "loss: 0.0015\n",
      "grad_norm: 0.03544463589787483\n",
      "learning_rate: 1.0062176657648678e-06\n",
      "epoch: 8.969813470027054\n",
      "\n",
      "\n",
      "Step: 252000\n",
      "loss: 0.0009\n",
      "grad_norm: 0.010415478609502316\n",
      "learning_rate: 9.824861170439983e-07\n",
      "epoch: 8.97052541648868\n",
      "\n",
      "\n",
      "Step: 252020\n",
      "loss: 0.0005\n",
      "grad_norm: 0.01593090407550335\n",
      "learning_rate: 9.587545683231286e-07\n",
      "epoch: 8.971237362950307\n",
      "\n",
      "\n",
      "Step: 252040\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01381151843816042\n",
      "learning_rate: 9.350230196022591e-07\n",
      "epoch: 8.971949309411933\n",
      "\n",
      "\n",
      "Step: 252060\n",
      "loss: 0.0015\n",
      "grad_norm: 0.021821334958076477\n",
      "learning_rate: 9.112914708813896e-07\n",
      "epoch: 8.972661255873557\n",
      "\n",
      "\n",
      "Step: 252080\n",
      "loss: 0.001\n",
      "grad_norm: 0.003362215356901288\n",
      "learning_rate: 8.875599221605201e-07\n",
      "epoch: 8.973373202335184\n",
      "\n",
      "\n",
      "Step: 252100\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01856369897723198\n",
      "learning_rate: 8.638283734396507e-07\n",
      "epoch: 8.97408514879681\n",
      "\n",
      "\n",
      "Step: 252120\n",
      "loss: 0.001\n",
      "grad_norm: 0.03399399667978287\n",
      "learning_rate: 8.400968247187811e-07\n",
      "epoch: 8.974797095258436\n",
      "\n",
      "\n",
      "Step: 252140\n",
      "loss: 0.0012\n",
      "grad_norm: 0.003185800276696682\n",
      "learning_rate: 8.163652759979115e-07\n",
      "epoch: 8.975509041720063\n",
      "\n",
      "\n",
      "Step: 252160\n",
      "loss: 0.0011\n",
      "grad_norm: 0.00999600999057293\n",
      "learning_rate: 7.92633727277042e-07\n",
      "epoch: 8.976220988181689\n",
      "\n",
      "\n",
      "Step: 252180\n",
      "loss: 0.0009\n",
      "grad_norm: 0.00810911227017641\n",
      "learning_rate: 7.689021785561725e-07\n",
      "epoch: 8.976932934643315\n",
      "\n",
      "\n",
      "Step: 252200\n",
      "loss: 0.0011\n",
      "grad_norm: 0.051687564700841904\n",
      "learning_rate: 7.45170629835303e-07\n",
      "epoch: 8.977644881104942\n",
      "\n",
      "\n",
      "Step: 252220\n",
      "loss: 0.0011\n",
      "grad_norm: 0.01764342375099659\n",
      "learning_rate: 7.214390811144334e-07\n",
      "epoch: 8.978356827566566\n",
      "\n",
      "\n",
      "Step: 252240\n",
      "loss: 0.0016\n",
      "grad_norm: 0.06809757649898529\n",
      "learning_rate: 6.977075323935639e-07\n",
      "epoch: 8.979068774028192\n",
      "\n",
      "\n",
      "Step: 252260\n",
      "loss: 0.0012\n",
      "grad_norm: 0.02317056618630886\n",
      "learning_rate: 6.739759836726944e-07\n",
      "epoch: 8.979780720489819\n",
      "\n",
      "\n",
      "Step: 252280\n",
      "loss: 0.0008\n",
      "grad_norm: 0.0198418777436018\n",
      "learning_rate: 6.502444349518249e-07\n",
      "epoch: 8.980492666951445\n",
      "\n",
      "\n",
      "Step: 252300\n",
      "loss: 0.0009\n",
      "grad_norm: 0.029735278338193893\n",
      "learning_rate: 6.265128862309554e-07\n",
      "epoch: 8.981204613413071\n",
      "\n",
      "\n",
      "Step: 252320\n",
      "loss: 0.0006\n",
      "grad_norm: 0.02249169908463955\n",
      "learning_rate: 6.027813375100858e-07\n",
      "epoch: 8.981916559874698\n",
      "\n",
      "\n",
      "Step: 252340\n",
      "loss: 0.0014\n",
      "grad_norm: 0.009127134457230568\n",
      "learning_rate: 5.790497887892163e-07\n",
      "epoch: 8.982628506336324\n",
      "\n",
      "\n",
      "Step: 252360\n",
      "loss: 0.001\n",
      "grad_norm: 0.05622079595923424\n",
      "learning_rate: 5.553182400683468e-07\n",
      "epoch: 8.98334045279795\n",
      "\n",
      "\n",
      "Step: 252380\n",
      "loss: 0.001\n",
      "grad_norm: 0.021106693893671036\n",
      "learning_rate: 5.315866913474773e-07\n",
      "epoch: 8.984052399259575\n",
      "\n",
      "\n",
      "Step: 252400\n",
      "loss: 0.0011\n",
      "grad_norm: 0.06065674498677254\n",
      "learning_rate: 5.078551426266078e-07\n",
      "epoch: 8.984764345721201\n",
      "\n",
      "\n",
      "Step: 252420\n",
      "loss: 0.0012\n",
      "grad_norm: 0.029257722198963165\n",
      "learning_rate: 4.841235939057382e-07\n",
      "epoch: 8.985476292182828\n",
      "\n",
      "\n",
      "Step: 252440\n",
      "loss: 0.001\n",
      "grad_norm: 0.006724431179463863\n",
      "learning_rate: 4.603920451848687e-07\n",
      "epoch: 8.986188238644454\n",
      "\n",
      "\n",
      "Step: 252460\n",
      "loss: 0.0014\n",
      "grad_norm: 0.03919639810919762\n",
      "learning_rate: 4.3666049646399924e-07\n",
      "epoch: 8.98690018510608\n",
      "\n",
      "\n",
      "Step: 252480\n",
      "loss: 0.001\n",
      "grad_norm: 0.028045719489455223\n",
      "learning_rate: 4.1292894774312966e-07\n",
      "epoch: 8.987612131567706\n",
      "\n",
      "\n",
      "Step: 252500\n",
      "loss: 0.0011\n",
      "grad_norm: 0.019934941083192825\n",
      "learning_rate: 3.891973990222602e-07\n",
      "epoch: 8.988324078029333\n",
      "\n",
      "\n",
      "Step: 252520\n",
      "loss: 0.001\n",
      "grad_norm: 0.023884328082203865\n",
      "learning_rate: 3.654658503013906e-07\n",
      "epoch: 8.989036024490959\n",
      "\n",
      "\n",
      "Step: 252540\n",
      "loss: 0.0015\n",
      "grad_norm: 0.01242743618786335\n",
      "learning_rate: 3.4173430158052114e-07\n",
      "epoch: 8.989747970952584\n",
      "\n",
      "\n",
      "Step: 252560\n",
      "loss: 0.0009\n",
      "grad_norm: 0.015110593289136887\n",
      "learning_rate: 3.180027528596516e-07\n",
      "epoch: 8.99045991741421\n",
      "\n",
      "\n",
      "Step: 252580\n",
      "loss: 0.0009\n",
      "grad_norm: 0.01049113180488348\n",
      "learning_rate: 2.942712041387821e-07\n",
      "epoch: 8.991171863875836\n",
      "\n",
      "\n",
      "Step: 252600\n",
      "loss: 0.0012\n",
      "grad_norm: 0.022952698171138763\n",
      "learning_rate: 2.7053965541791256e-07\n",
      "epoch: 8.991883810337463\n",
      "\n",
      "\n",
      "Step: 252620\n",
      "loss: 0.0014\n",
      "grad_norm: 0.04659629985690117\n",
      "learning_rate: 2.4680810669704304e-07\n",
      "epoch: 8.992595756799089\n",
      "\n",
      "\n",
      "Step: 252640\n",
      "loss: 0.0011\n",
      "grad_norm: 0.08040379732847214\n",
      "learning_rate: 2.2307655797617348e-07\n",
      "epoch: 8.993307703260715\n",
      "\n",
      "\n",
      "Step: 252660\n",
      "loss: 0.0012\n",
      "grad_norm: 0.03114752471446991\n",
      "learning_rate: 1.9934500925530398e-07\n",
      "epoch: 8.994019649722341\n",
      "\n",
      "\n",
      "Step: 252680\n",
      "loss: 0.0009\n",
      "grad_norm: 0.019390694797039032\n",
      "learning_rate: 1.7561346053443446e-07\n",
      "epoch: 8.994731596183968\n",
      "\n",
      "\n",
      "Step: 252700\n",
      "loss: 0.0008\n",
      "grad_norm: 0.02016865648329258\n",
      "learning_rate: 1.5188191181356493e-07\n",
      "epoch: 8.995443542645592\n",
      "\n",
      "\n",
      "Step: 252720\n",
      "loss: 0.0012\n",
      "grad_norm: 0.024591630324721336\n",
      "learning_rate: 1.281503630926954e-07\n",
      "epoch: 8.996155489107219\n",
      "\n",
      "\n",
      "Step: 252740\n",
      "loss: 0.0009\n",
      "grad_norm: 0.011539684608578682\n",
      "learning_rate: 1.044188143718259e-07\n",
      "epoch: 8.996867435568845\n",
      "\n",
      "\n",
      "Step: 252760\n",
      "loss: 0.0012\n",
      "grad_norm: 0.019129522144794464\n",
      "learning_rate: 8.068726565095637e-08\n",
      "epoch: 8.997579382030471\n",
      "\n",
      "\n",
      "Step: 252780\n",
      "loss: 0.0011\n",
      "grad_norm: 0.02080303058028221\n",
      "learning_rate: 5.695571693008685e-08\n",
      "epoch: 8.998291328492098\n",
      "\n",
      "\n",
      "Step: 252800\n",
      "loss: 0.0015\n",
      "grad_norm: 0.0017063259147107601\n",
      "learning_rate: 3.322416820921733e-08\n",
      "epoch: 8.999003274953724\n",
      "\n",
      "\n",
      "Step: 252820\n",
      "loss: 0.001\n",
      "grad_norm: 0.025090089067816734\n",
      "learning_rate: 9.492619488347808e-09\n",
      "epoch: 8.99971522141535\n",
      "\n",
      "\n",
      "Step: 252828\n",
      "eval_loss: 0.010128169320523739\n",
      "eval_runtime: 703.3719\n",
      "eval_samples_per_second: 159.756\n",
      "eval_steps_per_second: 9.985\n",
      "epoch: 9.0\n",
      "\n",
      "\n",
      "Step: 252828\n",
      "train_runtime: 11075.9946\n",
      "train_samples_per_second: 365.225\n",
      "train_steps_per_second: 22.827\n",
      "total_flos: 2.18952856829952e+18\n",
      "train_loss: 0.0001263577391465667\n",
      "epoch: 9.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=252828, training_loss=0.0001263577391465667, metrics={'train_runtime': 11075.9946, 'train_samples_per_second': 365.225, 'train_steps_per_second': 22.827, 'total_flos': 2.18952856829952e+18, 'train_loss': 0.0001263577391465667, 'epoch': 9.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[CustomCallback()]\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=\"/home/dysl-ai/Desktop/indoml_datathon/final_final_results/checkpoint-224736\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7023' max='7023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7023/7023 11:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 196644\n",
      "eval_loss: 0.009085921570658684\n",
      "eval_runtime: 703.1566\n",
      "eval_samples_per_second: 159.805\n",
      "eval_steps_per_second: 9.988\n",
      "epoch: 7.0\n",
      "\n",
      "\n",
      "Validation Loss: 0.009085921570658684\n"
     ]
    }
   ],
   "source": [
    "val_results = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
    "print(f\"Validation Loss: {val_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_t5_large_4/tokenizer_config.json',\n",
       " './fine_tuned_t5_large_4/special_tokens_map.json',\n",
       " './fine_tuned_t5_large_4/spiece.model',\n",
       " './fine_tuned_t5_large_4/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./fine_tuned_t5_large_4')\n",
    "tokenizer.save_pretrained('./fine_tuned_t5_large_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data1(data):\n",
    "    # Create the input_text column\n",
    "    data['input_text'] = data.apply(lambda row: f\"description: {row['description']} retailer: {row['retailer']} price: {row['price']}\", axis=1)\n",
    "\n",
    "    # Return the dictionary format with only input_text\n",
    "    return {\n",
    "        'input_text': data['input_text'].tolist()\n",
    "    }\n",
    "\n",
    "# Process the test data\n",
    "test_processed = preprocess_data1(test_data)\n",
    "\n",
    "# Convert the processed dictionary to a Hugging Face Dataset\n",
    "test_dataset = Dataset.from_dict(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('/home/dysl-ai/Desktop/indoml_datathon/fine_tuned_t5_large_4').to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('/home/dysl-ai/Desktop/indoml_datathon/fine_tuned_t5_large_4')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_data = test_dataset['input_text']\n",
    "#test_label = test_dataset['target_text']\n",
    "\n",
    "def generate_text(inputs):\n",
    "    inputs = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=352)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return generated_texts\n",
    "\n",
    "def extract_details(text):\n",
    "    pattern = r'supergroup: (.*?) group: (.*?) module: (.*?) brand: (.*)'\n",
    "    match = re.match(pattern, text)\n",
    "    if match:\n",
    "        return tuple(item if item is not None else 'na' for item in match.groups())\n",
    "    return 'na', 'na', 'na', 'na'\n",
    "\n",
    "def clean_repeated_patterns(text):\n",
    "    cleaned_data = text.split(' brand')[0]\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1443/1443 [14:43<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated info extracted.............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "generated_details = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing test data\"):\n",
    "    batch_inputs = test_data[i:i+batch_size]  # Get a batch of inputs\n",
    "\n",
    "    # Generate texts based on the batch inputs\n",
    "    generated_texts = generate_text(batch_inputs)\n",
    "\n",
    "    # Extract details from the generated texts and store them\n",
    "    for generated_text in generated_texts:\n",
    "        generated_details.append(extract_details(generated_text))\n",
    "\n",
    "print('Generated info extracted.............')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "categories = ['supergroup', 'group', 'module', 'brand']\n",
    "\n",
    "with open('attrebute_test_baseline_200dp.predict', 'w') as file:\n",
    "\n",
    "    for indoml_id, details in enumerate(generated_details):\n",
    "        result = {\"indoml_id\": indoml_id}\n",
    "        for category, value in zip(categories, details):\n",
    "            result[category] = value\n",
    "\n",
    "        file.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "file_to_zip = 'attrebute_test_baseline_200dp.predict'\n",
    "zip_file_name = 'codalab_new_final_3.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
    "     zipf.write(file_to_zip, arcname=file_to_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
